{"id":"meta_skill-08m","title":"[P5] GitHub Integration","description":"# GitHub Integration (Bundles)\n\n## Overview\n\nPublish bundles to GitHub releases and fetch them securely. This supports distribution to teams and machines.\n\n---\n\n## Tasks\n\n1. Implement GitHub release publishing.\n2. Download and verify release assets.\n3. Support token/SSH auth.\n4. Integrate with auto‑update checks.\n\n---\n\n## Testing Requirements\n\n- Integration tests with mock GitHub API or local fixture server.\n- Signature verification tests.\n\n---\n\n## Acceptance Criteria\n\n- Bundles can be published + installed from GitHub.\n- Signature verification enforced by default.\n\n---\n\n## Dependencies\n\n- `meta_skill-6fi` Bundle Format and Manifest\n- `meta_skill-nht` Auto‑Update System","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:04.116814134-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:07:41.376144271-05:00","labels":["bundles","github","phase-5"],"dependencies":[{"issue_id":"meta_skill-08m","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.375049919-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-0an","title":"[P3] Micro-Slicing Engine","description":"# Micro‑Slicing Engine\n\n## Overview\n\nSplit skills into token‑bounded slices (rules, commands, examples, pitfalls) with metadata needed for packing (coverage group, utility score, tags, dependencies). This is the foundation for efficient loading.\n\n---\n\n## Tasks\n\n1. Define `SkillSlice` and `SkillSliceIndex`.\n2. Implement slicing heuristics (by block + section).\n3. Compute token estimates per slice.\n4. Assign coverage groups + tags.\n5. Persist slices to SQLite.\n\n---\n\n## Testing Requirements\n\n- Unit tests: slicing determinism, token estimation.\n- Integration: SkillSpec → slices → packer compatibility.\n- Snapshot: slice ordering and metadata.\n\n---\n\n## Acceptance Criteria\n\n- Slices cover all content blocks.\n- Token estimates within ±10% of actual.\n- Coverage groups consistently assigned.\n\n---\n\n## Dependencies\n\n- `meta_skill-ik6` SkillSpec Data Model\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:13.214317137-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:57:45.182024908-05:00","labels":["indexing","phase-3","slicing"],"dependencies":[{"issue_id":"meta_skill-0an","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:24:25.846060335-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0an","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:58:12.247214857-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-0ki","title":"[P2] ms search Command","description":"## ms search Command (Complete CLI Specification)\n\nThe `ms search` command provides hybrid search over the skill registry, combining BM25 full-text and vector similarity via RRF fusion.\n\n### Basic Usage\n\n```bash\n# Simple search\nms search \"git workflow\"\n\n# With result limit\nms search \"git workflow\" --limit 10\n\n# Filter by tags\nms search \"error handling\" --tags rust,cli\n\n# Quality threshold\nms search \"testing\" --min-quality 0.7\n\n# Include deprecated skills\nms search \"legacy patterns\" --include-deprecated\n\n# Restrict to layer\nms search \"logging\" --layer project\n```\n\n### Advanced Options\n\n```bash\n# Alias resolution (exact match resolves to canonical id)\nms search \"old-skill-name\"  # Resolves via alias if exact match\n\n# Combined filters\nms search \"api design\" --tags rust --layer org --min-quality 0.8\n\n# Robot mode for automation\nms search \"testing\" --robot\n\n# Format as JSON\nms search \"testing\" --format json\n```\n\n### Command Specification\n\n```rust\n/// ms search command\n#[derive(Parser)]\npub struct SearchCmd {\n    /// Search query (supports boolean operators)\n    #[arg(required = true)]\n    pub query: String,\n    \n    /// Maximum number of results\n    #[arg(short, long, default_value = \"20\")]\n    pub limit: usize,\n    \n    /// Filter by tags (comma-separated)\n    #[arg(long, value_delimiter = ',')]\n    pub tags: Vec\u003cString\u003e,\n    \n    /// Minimum quality score (0.0-1.0)\n    #[arg(long)]\n    pub min_quality: Option\u003cf32\u003e,\n    \n    /// Include deprecated skills in results\n    #[arg(long)]\n    pub include_deprecated: bool,\n    \n    /// Restrict to specific layer\n    #[arg(long)]\n    pub layer: Option\u003cSkillLayer\u003e,\n    \n    /// Output format\n    #[arg(long, default_value = \"human\")]\n    pub format: OutputFormat,\n    \n    /// Robot mode (JSON to stdout)\n    #[arg(long)]\n    pub robot: bool,\n}\n```\n\n### Output Formats\n\n**Human (default):**\n```\nSearch results for \"git workflow\" (5 matches)\n\n1. git-commit-workflow (0.89)\n   Git commit best practices and workflow patterns\n   Tags: git, workflow, vcs\n   Layer: base\n\n2. git-branching-strategy (0.82)\n   Branching models for team collaboration\n   Tags: git, branching, team\n   Layer: org\n   \n...\n```\n\n**Robot/JSON:**\n```json\n{\n  \"status\": \"ok\",\n  \"timestamp\": \"2026-01-14T00:00:00Z\",\n  \"version\": \"0.1.0\",\n  \"data\": {\n    \"query\": \"git workflow\",\n    \"total_results\": 5,\n    \"results\": [\n      {\n        \"skill_id\": \"git-commit-workflow\",\n        \"name\": \"Git Commit Workflow\",\n        \"score\": 0.89,\n        \"description\": \"Git commit best practices...\",\n        \"tags\": [\"git\", \"workflow\", \"vcs\"],\n        \"layer\": \"base\",\n        \"deprecated\": false,\n        \"rrf_breakdown\": {\n          \"bm25_rank\": 1,\n          \"vector_rank\": 2,\n          \"rrf_score\": 0.89\n        }\n      }\n    ]\n  },\n  \"warnings\": []\n}\n```\n\n### Search Pipeline\n\n```\n┌─────────────┐\n│   Query     │\n└──────┬──────┘\n       │\n       ▼\n┌─────────────────────────────────────────┐\n│           Query Parser                  │\n│  - Tokenization                         │\n│  - Alias resolution (exact match)       │\n│  - Boolean operators                    │\n└──────┬──────────────────────────────────┘\n       │\n       ├──────────────────┬───────────────┐\n       ▼                  ▼               │\n┌─────────────┐   ┌─────────────┐         │\n│  Tantivy    │   │   Vector    │         │\n│  BM25       │   │   Search    │         │\n└──────┬──────┘   └──────┬──────┘         │\n       │                  │               │\n       └────────┬─────────┘               │\n                ▼                         │\n       ┌─────────────┐                    │\n       │  RRF Fusion │                    │\n       └──────┬──────┘                    │\n              │                           │\n              ▼                           │\n       ┌─────────────┐                    │\n       │   Filters   │ ◄──────────────────┘\n       │  - layer    │    (applied post-fusion)\n       │  - tags     │\n       │  - quality  │\n       │  - deprecated│\n       └──────┬──────┘\n              │\n              ▼\n       ┌─────────────┐\n       │   Results   │\n       └─────────────┘\n```\n\n### Alias \u0026 Deprecation Handling\n\n- **Alias resolution**: If the query exactly matches a skill alias, ms resolves to the canonical skill id before searching.\n- **Deprecated filtering**: Deprecated skills are filtered out by default. Use `--include-deprecated` to show them.\n\n### Exit Codes\n\n| Code | Meaning |\n|------|---------|\n| 0 | Success (results found or empty) |\n| 1 | Error (index not found, invalid query) |\n\n### Related Commands\n\n```bash\nms suggest           # Context-aware skill suggestions\nms show \u003cskill\u003e      # Show skill details\nms load \u003cskill\u003e      # Load skill content\nms alias resolve X   # Resolve an alias manually\n```","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:06.624504298-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:13:30.906008441-05:00","labels":["cli","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-93z","type":"blocks","created_at":"2026-01-13T22:23:13.620915337-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-5e6","type":"blocks","created_at":"2026-01-13T22:23:13.648348116-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-r6k","type":"blocks","created_at":"2026-01-13T22:23:13.674465537-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-ch6","type":"blocks","created_at":"2026-01-13T23:48:45.524055849-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T23:48:53.164321241-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-14h","title":"[P1] CLI Commands: init, index, list, show","description":"# Basic CLI Commands: init, index, list, show\n\n## Overview\n\nImplement the foundational CLI surface for ms. These commands are the first user touchpoints and must be deterministic, fast, and fully instrumented with robot‑mode JSON. They also establish core workflow expectations that all future commands follow.\n\n---\n\n## Commands \u0026 Behavior\n\n1. **`ms init`**\n   - Initialize `.ms/` and local config.\n   - Create SQLite DB and Git archive directories.\n   - Support `--global` to create `~/.config/ms/config.toml` only.\n\n2. **`ms index`**\n   - Scan configured skill paths (layered + non‑layered).\n   - Parse `SkillSpec`, compile `SKILL.md`, and update SQLite.\n   - Build search indices and skillpack cache.\n   - Support `--path`, `--all`, `--watch`, `--cass-incremental`.\n\n3. **`ms list`**\n   - List skills with filters: `--layer`, `--tag`, `--status`, `--deprecated`.\n   - Support `--robot` JSON output.\n\n4. **`ms show \u003cid\u003e`**\n   - Show skill metadata + dependency graph + provenance.\n   - Support `--robot` JSON output.\n\n---\n\n## Tasks\n\n- Implement clap subcommands and flag parsing.\n- Wire command handlers to core services (DB, Git, search, compiler).\n- Emit structured robot output schemas for each command.\n- Add human‑friendly formatting for list/show.\n\n---\n\n## Testing Requirements\n\n- Unit tests: argument parsing and validation errors.\n- Integration tests: `init → index → list → show` on temp repo.\n- Snapshot tests: human output formatting for `list` and `show`.\n- E2E scripts: basic workflow with logging enabled.\n\n---\n\n## Acceptance Criteria\n\n- Commands are stable and documented in `--help`.\n- Robot mode outputs valid JSON with status + data.\n- Index rebuilds skill registry deterministically.\n- List and show are fast (\u003c200ms for small repos).\n\n---\n\n## Dependencies\n\n- `meta_skill-ik6` SkillSpec Data Model\n- `meta_skill-qs1` SQLite Database Layer\n- `meta_skill-b98` Git Archive Layer\n- `meta_skill-vqr` Robot Mode Infrastructure\n- `meta_skill-igx` Global File Locking (for index)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:05.275101079-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:48:05.820057326-05:00","labels":["cli","commands","phase-1"],"dependencies":[{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:22:14.981654513-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:22:15.008840067-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:22:15.034452446-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-225","type":"blocks","created_at":"2026-01-13T22:54:04.187430476-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T23:48:15.889279857-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-igx","type":"blocks","created_at":"2026-01-13T23:48:23.552762346-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1bg","title":"[P6] Skill Versioning System","description":"# Skill Versioning System\n\n## Overview\n\nTrack skill versions, enable migrations, and provide backward compatibility for SkillSpec changes. Versioning should be explicit and deterministic.\n\n---\n\n## Tasks\n\n1. Add version metadata to SkillSpec and bundles.\n2. Implement migration registry for spec versions.\n3. Provide `ms migrate` command for upgrades.\n4. Expose version info in `ms show`.\n\n---\n\n## Testing Requirements\n\n- Unit tests for migrations.\n- Integration tests: old spec → new spec.\n- Snapshot tests for migrated output.\n\n---\n\n## Acceptance Criteria\n\n- Old skills load after migration.\n- Migration is deterministic and reversible where possible.\n\n---\n\n## Dependencies\n\n- `meta_skill-ik6` SkillSpec Data Model\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:24.898227549-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:11:48.160885847-05:00","labels":["migration","phase-6","versioning"],"dependencies":[{"issue_id":"meta_skill-1bg","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:28:37.089513634-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1bg","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:28:37.118898402-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1bg","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:12:17.984773167-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1jl","title":"[P3] Conditional Predicates","description":"# Conditional Predicates\n\n## Overview\n\nEnable slices to be conditionally included based on context signals (file presence, toolchain, environment, dependencies). This keeps packs relevant and lean.\n\n---\n\n## Tasks\n\n1. Define predicate language (FileExists, TechStack, EnvVar, DependsOn).\n2. Implement predicate evaluator with context snapshot.\n3. Integrate evaluator into packer + load.\n4. Provide explain output for predicate filtering.\n\n---\n\n## Testing Requirements\n\n- Unit tests for each predicate type.\n- Integration tests with fixture repos.\n- Snapshot tests for explain output.\n\n---\n\n## Acceptance Criteria\n\n- Predicates evaluate deterministically.\n- False positives/negatives minimized.\n- Explain output clearly states inclusion/exclusion.\n\n---\n\n## Dependencies\n\n- `meta_skill-ftj` Tech Stack Detection","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:24:15.035776247-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:00:11.547319697-05:00","labels":["filtering","phase-3","predicates"],"dependencies":[{"issue_id":"meta_skill-1jl","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T22:24:25.926673919-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1jl","depends_on_id":"meta_skill-ftj","type":"blocks","created_at":"2026-01-14T00:00:19.968533091-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1p7","title":"[P4] Provenance Graph","description":"# Provenance Graph\n\n## Overview\n\nMaintain a rule‑level provenance graph linking skills, rules, and evidence excerpts. This enables auditing, evidence jump‑to‑source, and quality scoring.\n\n---\n\n## Tasks\n\n1. Define EvidenceRef with session + message range.\n2. Persist evidence per rule in SQLite.\n3. Provide `ms evidence` CLI to jump to source.\n4. Expose provenance graph export (JSON/DOT).\n\n---\n\n## Testing Requirements\n\n- Unit tests for evidence serialization.\n- Integration tests: rule → evidence → cass expand.\n- Snapshot tests for provenance graph export.\n\n---\n\n## Acceptance Criteria\n\n- Every rule has traceable evidence.\n- Evidence fetch works with cass expand.\n- Graph export is deterministic.\n\n---\n\n## Dependencies\n\n- `meta_skill-hhu` CASS Client Integration\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:48.342543267-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:04:51.333381584-05:00","labels":["evidence","phase-4","provenance"],"dependencies":[{"issue_id":"meta_skill-1p7","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:13.048393324-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1p7","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-14T00:05:00.087284477-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1p7","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:05:08.271366944-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1w2","title":"[P6] Agent Mail Integration","description":"# Agent Mail Integration\n\nMulti-agent skill coordination via Agent Mail MCP.\n\n## Tasks\n1. Agent Mail client integration\n2. Skill announcement protocol\n3. Pattern sharing between agents\n4. Skill request/fulfillment\n5. Swarm coordination\n\n## Use Cases (from Section 20)\n1. Share discovered patterns in real-time\n2. Coordinate skill generation (avoid duplication)\n3. Request skills from specialized agents\n4. Notify when new skills ready\n\n## Message Types\n- skill_build_start: Agent starting skill generation\n- skill_build_complete: Skill ready for use\n- pattern_share: Sharing extracted patterns\n- skill_request: Requesting skill from others\n- skill_response: Responding to request\n\n## Integration Pattern\n```rust\nstruct AgentMailClient {\n    project_key: String,\n    agent_name: String,\n    mcp_endpoint: String,\n}\n\nimpl AgentMailClient {\n    async fn announce_build_start(\u0026self, topic: \u0026str);\n    async fn announce_build_complete(\u0026self, skill_id: \u0026str);\n    async fn share_patterns(\u0026self, patterns: \u0026[Pattern]);\n}\n```\n\n## Fallback\n- If Agent Mail unavailable, local-only operation\n- No blocking on network\n\n## Acceptance Criteria\n- Agents can coordinate skill building\n- Patterns shared successfully\n- Graceful fallback when offline","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-13T22:28:28.106184603-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:06.982463956-05:00","closed_at":"2026-01-13T23:42:06.982463956-05:00","close_reason":"Duplicate of meta_skill-tzu (Agent Mail Integration)","labels":["agent-mail","coordination","phase-6"],"dependencies":[{"issue_id":"meta_skill-1w2","depends_on_id":"meta_skill-ugf","type":"blocks","created_at":"2026-01-13T22:28:37.203040025-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1w2","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:28:37.230093279-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-225","title":"Skill Layering \u0026 Conflict Resolution","description":"## Overview\n\nImplement skill layering and conflict resolution. Skills can exist in multiple layers (base \u003c org \u003c project \u003c user), with higher layers overriding lower layers. This enables organizational customization while preserving upstream skills.\n\n## Background \u0026 Rationale\n\n### Why Layering\n\n1. **Separation of Concerns**: Base skills vs org customizations vs user preferences\n2. **Override Semantics**: Project-specific rules can override global defaults\n3. **No Duplication**: Overlays patch specific blocks without copying entire skills\n4. **Drift Prevention**: Base updates propagate automatically\n5. **Clear Provenance**: Each block tracks which layer modified it\n\n### Layer Order\n\n```\nbase \u003c org \u003c project \u003c user\n```\n\nHigher layers take precedence when conflicts occur.\n\n---\n\n## Key Data Structures (from Plan Section 3.5)\n\n### Layered Registry\n\n```rust\nuse std::collections::HashMap;\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, PartialOrd, Ord)]\npub enum SkillLayer {\n    Base,    // System-provided skills\n    Org,     // Organization-wide customizations\n    Project, // Project-specific skills\n    User,    // User preferences (highest precedence)\n}\n\npub struct LayeredRegistry {\n    /// Layers in precedence order (low to high)\n    pub layers: Vec\u003cSkillLayer\u003e,\n    /// Per-layer registries\n    pub registries: HashMap\u003cSkillLayer, SkillRegistry\u003e,\n}\n\nimpl LayeredRegistry {\n    pub fn new() -\u003e Self {\n        Self {\n            layers: vec![SkillLayer::Base, SkillLayer::Org, SkillLayer::Project, SkillLayer::User],\n            registries: HashMap::new(),\n        }\n    }\n    \n    /// Return the effective skill, resolving conflicts by layer\n    pub fn effective(\u0026self, skill_id: \u0026str) -\u003e Result\u003cResolvedSkill\u003e {\n        let mut candidates = Vec::new();\n        \n        for layer in \u0026self.layers {\n            if let Some(registry) = self.registries.get(layer) {\n                if let Ok(skill) = registry.get(skill_id) {\n                    candidates.push((*layer, skill));\n                }\n            }\n        }\n        \n        if candidates.is_empty() {\n            return Err(MsError::SkillNotFound(skill_id.to_string()));\n        }\n        \n        if candidates.len() == 1 {\n            let (layer, skill) = candidates.pop().unwrap();\n            return Ok(ResolvedSkill {\n                skill,\n                source_layer: layer,\n                conflicts: vec![],\n            });\n        }\n        \n        // Multiple candidates - resolve conflicts\n        resolve_conflicts(candidates, ConflictStrategy::PreferHigher)\n    }\n}\n```\n\n### Conflict Resolution\n\n```rust\n#[derive(Debug, Clone)]\npub struct ResolvedSkill {\n    pub skill: Skill,\n    pub source_layer: SkillLayer,\n    pub conflicts: Vec\u003cConflictDetail\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct ConflictDetail {\n    /// Which section had the conflict\n    pub section: String,\n    /// Layer that won\n    pub higher_layer: SkillLayer,\n    /// Layer that was overridden\n    pub lower_layer: SkillLayer,\n    /// How it was resolved\n    pub resolution: ConflictResolution,\n}\n\n#[derive(Debug, Clone)]\npub enum ConflictStrategy {\n    /// Always use higher layer\n    PreferHigher,\n    /// Always use lower layer\n    PreferLower,\n    /// Require user choice\n    Interactive,\n}\n\n/// How to merge non-identical sections before falling back to strategy\n#[derive(Debug, Clone)]\npub enum MergeStrategy {\n    /// Merge only when sections are non-overlapping\n    Auto,\n    /// Prefer higher-layer rules/pitfalls, lower-layer examples/references\n    PreferSections,\n}\n\n#[derive(Debug, Clone)]\npub enum ConflictResolution {\n    UseHigher,\n    UseLower,\n    Merge(String), // merged section content\n}\n```\n\n### Conflict Merger\n\n```rust\npub struct ConflictMerger;\n\nimpl ConflictMerger {\n    pub fn resolve(\n        \u0026self,\n        higher: \u0026SkillSpec,\n        lower: \u0026SkillSpec,\n        strategy: ConflictStrategy,\n        merge_strategy: MergeStrategy,\n    ) -\u003e Result\u003cResolvedSkill\u003e {\n        let diffs = section_diff(higher, lower);\n\n        // Auto-merge if changes are non-overlapping\n        if matches!(merge_strategy, MergeStrategy::Auto) \u0026\u0026 diffs.non_overlapping() {\n            return Ok(ResolvedSkill {\n                skill: merge_sections(higher, lower)?,\n                source_layer: SkillLayer::User, // Higher layer\n                conflicts: vec![],\n            });\n        }\n\n        // Prefer sections across layers to keep lower-layer examples\n        if matches!(merge_strategy, MergeStrategy::PreferSections) {\n            return Ok(ResolvedSkill {\n                skill: merge_by_section_preference(higher, lower)?,\n                source_layer: SkillLayer::User,\n                conflicts: diffs.to_conflicts(SkillLayer::User, SkillLayer::Project),\n            });\n        }\n\n        match strategy {\n            ConflictStrategy::PreferHigher =\u003e Ok(ResolvedSkill {\n                skill: higher.to_skill(),\n                source_layer: SkillLayer::User,\n                conflicts: diffs.to_conflicts(SkillLayer::User, SkillLayer::Project),\n            }),\n            ConflictStrategy::PreferLower =\u003e Ok(ResolvedSkill {\n                skill: lower.to_skill(),\n                source_layer: SkillLayer::Project,\n                conflicts: diffs.to_conflicts(SkillLayer::Project, SkillLayer::User),\n            }),\n            ConflictStrategy::Interactive =\u003e Err(anyhow!(\"Interactive resolution required\")),\n        }\n    }\n}\n\n/// Compute section-level differences\nfn section_diff(higher: \u0026SkillSpec, lower: \u0026SkillSpec) -\u003e SectionDiff {\n    // Compare each section and identify differences\n    unimplemented!()\n}\n\n/// Merge non-overlapping sections\nfn merge_sections(higher: \u0026SkillSpec, lower: \u0026SkillSpec) -\u003e Result\u003cSkill\u003e {\n    unimplemented!()\n}\n\n/// Merge with section-type preferences\nfn merge_by_section_preference(higher: \u0026SkillSpec, lower: \u0026SkillSpec) -\u003e Result\u003cSkill\u003e {\n    // Higher-layer: rules, pitfalls\n    // Lower-layer: examples, references\n    unimplemented!()\n}\n```\n\n---\n\n## Block-Level Overlays (from Plan Section 3.5)\n\nBeyond whole-skill overrides, higher layers can provide overlay files that patch specific block IDs without copying the entire skill.\n\n### Overlay Types\n\n```rust\n/// Overlay operations for surgical skill modifications\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillOverlay {\n    /// Target skill id\n    pub skill_id: String,\n\n    /// Layer that provides this overlay\n    pub layer: SkillLayer,\n\n    /// Ordered list of patch operations\n    pub operations: Vec\u003cOverlayOp\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum OverlayOp {\n    /// Replace a block's content entirely\n    ReplaceBlock { block_id: String, content: String },\n\n    /// Delete a block\n    DeleteBlock { block_id: String },\n\n    /// Insert a new block after an existing one\n    InsertAfter { after_block_id: String, new_block: SkillBlock },\n\n    /// Append items to a checklist block\n    AppendToChecklist { block_id: String, items: Vec\u003cString\u003e },\n\n    /// Prepend a critical rule (inserted at top of rules section)\n    PrependRule { rule: SkillBlock },\n\n    /// Override metadata fields\n    PatchMetadata { patches: HashMap\u003cString, serde_json::Value\u003e },\n}\n```\n\n### Overlay Application\n\n```rust\nimpl LayeredRegistry {\n    /// Apply overlays from higher layers to base skill\n    pub fn apply_overlays(\u0026self, skill: \u0026Skill, overlays: \u0026[SkillOverlay]) -\u003e Result\u003cSkill\u003e {\n        let mut spec = skill.to_spec()?;\n\n        for overlay in overlays {\n            for op in \u0026overlay.operations {\n                match op {\n                    OverlayOp::ReplaceBlock { block_id, content } =\u003e {\n                        spec.replace_block(block_id, content)?;\n                    }\n                    OverlayOp::DeleteBlock { block_id } =\u003e {\n                        spec.delete_block(block_id)?;\n                    }\n                    OverlayOp::InsertAfter { after_block_id, new_block } =\u003e {\n                        spec.insert_after(after_block_id, new_block.clone())?;\n                    }\n                    OverlayOp::AppendToChecklist { block_id, items } =\u003e {\n                        spec.append_checklist_items(block_id, items)?;\n                    }\n                    OverlayOp::PrependRule { rule } =\u003e {\n                        spec.prepend_rule(rule.clone())?;\n                    }\n                    OverlayOp::PatchMetadata { patches } =\u003e {\n                        spec.patch_metadata(patches)?;\n                    }\n                }\n            }\n        }\n\n        spec.compile()\n    }\n}\n```\n\n### Overlay File Format\n\nOverlays are stored in the layer's skill directory as `skill.overlay.json`:\n\n```json\n{\n  \"skill_id\": \"nextjs-patterns\",\n  \"operations\": [\n    {\n      \"type\": \"replace_block\",\n      \"block_id\": \"rule-3\",\n      \"content\": \"NEVER use transition-all; prefer specific properties per new policy.\"\n    },\n    {\n      \"type\": \"append_to_checklist\",\n      \"block_id\": \"checklist-pre-deploy\",\n      \"items\": [\"Run compliance audit: `audit-tool check`\"]\n    },\n    {\n      \"type\": \"prepend_rule\",\n      \"rule\": {\n        \"id\": \"org-rule-1\",\n        \"type\": \"rule\",\n        \"content\": \"All API routes must use the org auth middleware.\"\n      }\n    }\n  ]\n}\n```\n\n---\n\n## Benefits of Overlay System\n\n- **No duplication:** Org/user layers don't copy entire skills\n- **Drift prevention:** Base skill updates propagate automatically\n- **Surgical policy:** Add compliance rules without rewriting\n- **Clear provenance:** Each block records which layer modified it\n\n---\n\n## Tasks\n\n### Task 1: Layer Types\n- [ ] Create src/core/layers.rs module\n- [ ] Define SkillLayer enum with ordering\n- [ ] Implement Ord trait for layer comparison\n- [ ] Add layer_path() method for filesystem locations\n\n### Task 2: LayeredRegistry\n- [ ] Implement LayeredRegistry struct\n- [ ] Implement add_layer() method\n- [ ] Implement effective() with conflict resolution\n- [ ] Support layer iteration in precedence order\n\n### Task 3: Conflict Detection\n- [ ] Implement section_diff() function\n- [ ] Detect overlapping sections\n- [ ] Track which blocks differ\n- [ ] Generate ConflictDetail records\n\n### Task 4: Conflict Resolution\n- [ ] Implement ConflictMerger struct\n- [ ] Support PreferHigher strategy\n- [ ] Support PreferLower strategy\n- [ ] Support Auto merge for non-overlapping\n- [ ] Support PreferSections merge\n\n### Task 5: Overlay Types\n- [ ] Define SkillOverlay struct\n- [ ] Define OverlayOp enum with all variants\n- [ ] Implement Serialize/Deserialize\n- [ ] Validate overlay file format\n\n### Task 6: Overlay Application\n- [ ] Implement apply_overlays() method\n- [ ] Implement ReplaceBlock operation\n- [ ] Implement DeleteBlock operation\n- [ ] Implement InsertAfter operation\n- [ ] Implement AppendToChecklist operation\n- [ ] Implement PrependRule operation\n- [ ] Implement PatchMetadata operation\n\n### Task 7: Overlay Loading\n- [ ] Scan for skill.overlay.json files\n- [ ] Load overlays for each layer\n- [ ] Apply in layer order\n- [ ] Cache applied results\n\n### Task 8: CLI Integration\n- [ ] Add --layer flag to relevant commands\n- [ ] Implement `ms resolve` for interactive resolution\n- [ ] Show conflict details in robot mode\n- [ ] Implement `ms diff --layers` command\n\n---\n\n## Acceptance Criteria\n\n1. **Layer Ordering**: Higher layers override lower\n2. **Conflict Detection**: Overlapping sections identified\n3. **Merge Works**: Non-overlapping sections merged automatically\n4. **Overlays Apply**: Block-level patches work correctly\n5. **Provenance Tracked**: Each block knows its source layer\n6. **No Duplication**: Org layers don't copy base skills\n\n---\n\n## Testing Requirements\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_layer_ordering() {\n        assert!(SkillLayer::Base \u003c SkillLayer::Org);\n        assert!(SkillLayer::Org \u003c SkillLayer::Project);\n        assert!(SkillLayer::Project \u003c SkillLayer::User);\n    }\n\n    #[test]\n    fn test_effective_single_layer() {\n        let mut registry = LayeredRegistry::new();\n        registry.add_skill(SkillLayer::Base, skill(\"test\"));\n        \n        let resolved = registry.effective(\"test\").unwrap();\n        assert_eq!(resolved.source_layer, SkillLayer::Base);\n        assert!(resolved.conflicts.is_empty());\n    }\n\n    #[test]\n    fn test_effective_higher_wins() {\n        let mut registry = LayeredRegistry::new();\n        registry.add_skill(SkillLayer::Base, skill(\"test\"));\n        registry.add_skill(SkillLayer::User, skill_modified(\"test\"));\n        \n        let resolved = registry.effective(\"test\").unwrap();\n        assert_eq!(resolved.source_layer, SkillLayer::User);\n    }\n\n    #[test]\n    fn test_overlay_replace_block() {\n        let base = skill_with_blocks(\"test\", vec![block(\"rule-1\", \"Original\")]);\n        let overlay = SkillOverlay {\n            skill_id: \"test\".to_string(),\n            layer: SkillLayer::Org,\n            operations: vec![OverlayOp::ReplaceBlock {\n                block_id: \"rule-1\".to_string(),\n                content: \"Modified\".to_string(),\n            }],\n        };\n        \n        let registry = LayeredRegistry::new();\n        let result = registry.apply_overlays(\u0026base, \u0026[overlay]).unwrap();\n        \n        assert!(result.body.contains(\"Modified\"));\n        assert!(!result.body.contains(\"Original\"));\n    }\n\n    #[test]\n    fn test_overlay_prepend_rule() {\n        let base = skill_with_blocks(\"test\", vec![block(\"rule-1\", \"Existing\")]);\n        let overlay = SkillOverlay {\n            skill_id: \"test\".to_string(),\n            layer: SkillLayer::Org,\n            operations: vec![OverlayOp::PrependRule {\n                rule: SkillBlock::Rule {\n                    id: \"org-rule\".to_string(),\n                    text: \"Org policy\".to_string(),\n                },\n            }],\n        };\n        \n        let registry = LayeredRegistry::new();\n        let result = registry.apply_overlays(\u0026base, \u0026[overlay]).unwrap();\n        \n        // Org rule should come first\n        let org_pos = result.body.find(\"Org policy\").unwrap();\n        let existing_pos = result.body.find(\"Existing\").unwrap();\n        assert!(org_pos \u003c existing_pos);\n    }\n}\n```\n\n---\n\n## Logging Requirements\n\n- **DEBUG**: Layer iteration, overlay operations applied\n- **INFO**: Conflict resolution decisions, merge results\n- **WARN**: Unresolved conflicts, deprecated layer usage\n- **ERROR**: Invalid overlay format, apply failures\n\n---\n\n## References\n\n- **Plan Section 3.5**: Layering and Conflict Resolution\n- **Depends on**: meta_skill-ik6 (SkillSpec), meta_skill-qs1 (SQLite)\n- **Blocks**: meta_skill-cn4 (Block-Level Overlays), meta_skill-7va (ms load)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:52:47.294634944-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:03:56.049637652-05:00","labels":["conflicts","layers","phase-1"],"dependencies":[{"issue_id":"meta_skill-225","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:54:03.394720272-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-237","title":"[P4] Pattern Extraction Pipeline","description":"# Pattern Extraction Pipeline\n\n## Overview\n\nExtract high‑signal, reusable patterns from CASS sessions. This is the *front‑door* of the mining pipeline and directly determines skill quality. Extraction must be deterministic, safe, and provenance‑rich.\n\n---\n\n## Inputs / Outputs\n\n**Input:**\n- CASS sessions (messages + tool calls + results)\n- Session quality signals\n- Redaction + injection filters\n\n**Output:**\n- `ExtractedPattern` objects with evidence pointers, confidence, and taint metadata.\n\n---\n\n## Core Extraction Types\n\n1. **Command Recipes** (ordered command sequences)\n2. **Code Patterns** (reusable snippets + intent)\n3. **Workflow Patterns** (step‑by‑step procedures)\n4. **Constraints** (CRITICAL RULES / invariants)\n5. **Error Resolutions** (error → fix mapping)\n6. **Anti‑patterns** (negative examples; see meta_skill-tun)\n\n---\n\n## Tasks\n\n1. Session segmentation (recon → change → validation → wrap‑up).\n2. Pattern detectors per type (commands, code, workflow, constraints, errors).\n3. Normalize + de‑duplicate extracted patterns.\n4. Assign confidence score (frequency + outcomes).\n5. Attach provenance refs (session id + message ranges).\n6. Emit taint labels for safety filtering.\n\n---\n\n## Testing Requirements\n\n- Unit tests for each detector (positive + negative cases).\n- Integration test: full session → extracted patterns.\n- Determinism test: same session → same patterns.\n- Safety test: injected content is excluded.\n\n---\n\n## Acceptance Criteria\n\n- Extraction covers all core pattern types.\n- Patterns include provenance + confidence.\n- Unsafe/tainted content never emitted.\n\n---\n\n## Dependencies\n\n- `meta_skill-hhu` CASS Client Integration\n- `meta_skill-ans` Redaction Pipeline\n- `meta_skill-fma` Prompt Injection Defense\n- `meta_skill-llm` Session Quality Scoring","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:46.2225012-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:50:29.58294067-05:00","labels":["extraction","patterns","phase-4"],"dependencies":[{"issue_id":"meta_skill-237","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T22:26:12.937095954-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-237","depends_on_id":"meta_skill-fma","type":"blocks","created_at":"2026-01-13T23:51:36.59820069-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-237","depends_on_id":"meta_skill-ans","type":"blocks","created_at":"2026-01-13T23:51:45.818341561-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-237","depends_on_id":"meta_skill-llm","type":"blocks","created_at":"2026-01-13T23:52:20.027824131-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-27c","title":"UBS (Ultimate Bug Scanner) Integration","description":"## Section Reference\nIntegration with existing tooling - UBS (Ultimate Bug Scanner)\n\n## Overview\n\nIntegrate UBS from /data/projects/ultimate_bug_scanner as the static analysis layer for ms. Per AGENTS.md golden rule: \"ubs \u003cchanged-files\u003e before every commit. Exit 0 = safe. Exit \u003e0 = fix \u0026 re-run.\"\n\n## Why UBS Integration\n\n| UBS Feature | ms Application |\n|------------|----------------|\n| **Static analysis** | Validate extracted code patterns |\n| **Multi-language** | Go, Rust, TypeScript support |\n| **Exit codes** | Clear pass/fail for CI |\n| **Suggested fixes** | Include in skill pitfalls |\n\n## Integration Architecture\n\n```rust\n/// UBS client for static analysis\nstruct UbsClient {\n    /// Path to ubs binary\n    ubs_path: PathBuf,\n}\n\nimpl UbsClient {\n    /// Run UBS on files\n    async fn check(\u0026self, files: \u0026[PathBuf]) -\u003e Result\u003cUbsResult\u003e {\n        // Call: ubs file1.go file2.go\n    }\n    \n    /// Run UBS on staged git files\n    async fn check_staged(\u0026self) -\u003e Result\u003cUbsResult\u003e {\n        // Call: ubs $(git diff --name-only --cached)\n    }\n    \n    /// Check entire directory\n    async fn check_dir(\u0026self, dir: \u0026Path, only: Option\u003c\u0026str\u003e) -\u003e Result\u003cUbsResult\u003e {\n        // Call: ubs --only=\u003clang\u003e \u003cdir\u003e\n    }\n}\n\nstruct UbsResult {\n    exit_code: i32,\n    findings: Vec\u003cUbsFinding\u003e,\n    summary: String,\n}\n\nstruct UbsFinding {\n    category: String,\n    severity: UbsSeverity,\n    file: PathBuf,\n    line: u32,\n    column: u32,\n    message: String,\n    suggested_fix: Option\u003cString\u003e,\n}\n\nenum UbsSeverity {\n    Critical,  // nil deref, div by zero, race conditions\n    Important, // error handling, unchecked assertions\n    Contextual, // TODOs, unused vars\n}\n```\n\n## ms Integration Points\n\n### 1. Pre-Commit Hook for Skills\n\nValidate skill code snippets before publishing:\n\n```rust\nimpl SkillValidator {\n    async fn validate_code_snippets(\u0026self, skill: \u0026SkillSpec) -\u003e Result\u003cValidationResult\u003e {\n        // Extract code blocks from skill\n        let code_blocks = skill.extract_code_blocks();\n        \n        // Write to temp files\n        let temp_files = self.write_temp_files(\u0026code_blocks)?;\n        \n        // Run UBS\n        let ubs_result = self.ubs.check(\u0026temp_files).await?;\n        \n        if ubs_result.exit_code != 0 {\n            return Err(ValidationError::UbsFindings(ubs_result.findings));\n        }\n        \n        Ok(ValidationResult::Clean)\n    }\n}\n```\n\n### 2. Pattern Extraction Quality Gate\n\nDon't extract patterns from code with UBS findings:\n\n```rust\nimpl PatternExtractor {\n    async fn extract(\u0026self, session: \u0026Session) -\u003e Result\u003cVec\u003cPattern\u003e\u003e {\n        let code_blocks = session.extract_code_changes();\n        \n        // Run UBS on extracted code\n        let ubs_result = self.ubs.check_code(\u0026code_blocks).await?;\n        \n        // Skip patterns from code with critical findings\n        if ubs_result.has_critical_findings() {\n            log::warn!(\"Skipping patterns from code with UBS findings\");\n            return Ok(vec![]);\n        }\n        \n        // Continue extraction\n        self.extract_patterns(session)\n    }\n}\n```\n\n### 3. CI Integration\n\nRun UBS as part of skill validation in CI:\n\n```yaml\n# .github/workflows/skill-validation.yml\n- name: Validate skill code\n  run: |\n    for skill in skills/*.skill.yaml; do\n      ms validate --ubs \"$skill\"\n    done\n```\n\n## CLI Commands\n\n```bash\n# Validate skill with UBS\nms validate --ubs \u003cskill\u003e\n\n# Check extracted code quality\nms build --ubs-check\n\n# Pre-commit hook\nms pre-commit  # Runs UBS on changed files\n```\n\n## Tasks\n\n1. [ ] Implement UbsClient wrapper\n2. [ ] Add --ubs flag to validate command\n3. [ ] Integrate UBS in pattern extraction\n4. [ ] Add pre-commit hook command\n5. [ ] Document UBS installation requirements\n\n## Testing Requirements\n\n- UBS integration tests\n- Code block extraction accuracy\n- Pre-commit hook tests\n- CI integration tests\n\n## Acceptance Criteria\n\n- UBS detected and integrated\n- Skills validated for code quality\n- Patterns not extracted from bad code\n- Pre-commit hook functional\n- Graceful fallback when UBS unavailable\n\n## References\n\n- UBS repository: /data/projects/ultimate_bug_scanner\n- AGENTS.md UBS section","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T23:09:53.868716824-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:09:53.868716824-05:00","labels":["cross-cutting quality static-analysis"],"dependencies":[{"issue_id":"meta_skill-27c","depends_on_id":"meta_skill-9ok","type":"blocks","created_at":"2026-01-13T23:09:59.044933274-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-2kd","title":"E2E Test Scripts","description":"## Overview\n\nCreate comprehensive end-to-end test scripts that exercise full workflows with detailed logging. These tests simulate real user scenarios from start to finish, verifying the entire system works correctly as an integrated whole.\n\n## Requirements\n\n### 1. Test Scenarios\n\n#### Scenario 1: Fresh Install to Search Workflow\n```rust\n#[tokio::test]\nasync fn test_fresh_install_to_search() {\n    let fixture = E2EFixture::new(\"fresh_install_to_search\").await;\n    \n    // Step 1: Initialize fresh installation\n    fixture.log_step(\"Initialize fresh installation\");\n    let output = fixture.run_ms(\u0026[\"init\"]).await;\n    fixture.assert_success(\u0026output, \"init\");\n    fixture.checkpoint(\"post_init\");\n    \n    // Step 2: Create test skills\n    fixture.log_step(\"Create test skills\");\n    fixture.create_skill(\"rust-patterns\", r#\"\n---\nname: rust-patterns\ndescription: Common Rust design patterns and idioms\ntags: [rust, patterns, design]\n---\n# Rust Patterns\nCommon patterns for Rust development including error handling, \nbuilder pattern, type state, and newtype pattern.\n\"#);\n    fixture.checkpoint(\"skills_created\");\n    \n    // Step 3: Index skills\n    fixture.log_step(\"Index skills\");\n    let output = fixture.run_ms(\u0026[\"index\"]).await;\n    fixture.assert_success(\u0026output, \"index\");\n    fixture.checkpoint(\"post_index\");\n    \n    // Step 4: Search for skills\n    fixture.log_step(\"Search for skills\");\n    let output = fixture.run_ms(\u0026[\"search\", \"rust patterns\"]).await;\n    fixture.assert_success(\u0026output, \"search\");\n    fixture.assert_output_contains(\u0026output, \"rust-patterns\");\n    fixture.checkpoint(\"post_search\");\n    \n    // Step 5: Load skill and verify output\n    fixture.log_step(\"Load skill\");\n    let output = fixture.run_ms(\u0026[\"load\", \"rust-patterns\"]).await;\n    fixture.assert_success(\u0026output, \"load\");\n    fixture.assert_output_contains(\u0026output, \"Rust Patterns\");\n    fixture.checkpoint(\"post_load\");\n    \n    // Generate report\n    fixture.generate_report();\n}\n```\n\n#### Scenario 2: Skill Creation Workflow (CASS Integration)\n```rust\n#[tokio::test]\nasync fn test_skill_creation_workflow() {\n    let fixture = E2EFixture::with_mock_cass(\"skill_creation_workflow\").await;\n    \n    // Step 1: Start session for skill mining\n    fixture.log_step(\"Start CASS session\");\n    let session_id = fixture.create_mock_session(r#\"\n        User is implementing a complex async state machine in Rust\n        with proper error handling and cancellation support.\n    \"#).await;\n    fixture.checkpoint(\"session_created\");\n    \n    // Step 2: Mine skill from session\n    fixture.log_step(\"Mine skill from session\");\n    let output = fixture.run_ms(\u0026[\"build\", \"--from-session\", \u0026session_id]).await;\n    fixture.assert_success(\u0026output, \"build\");\n    fixture.checkpoint(\"post_build\");\n    \n    // Step 3: Validate mined skill\n    fixture.log_step(\"Validate skill\");\n    let output = fixture.run_ms(\u0026[\"validate\", \"async-state-machine\"]).await;\n    fixture.assert_success(\u0026output, \"validate\");\n    fixture.checkpoint(\"post_validate\");\n    \n    // Step 4: Publish skill locally\n    fixture.log_step(\"Publish skill\");\n    let output = fixture.run_ms(\u0026[\"publish\", \"async-state-machine\", \"--local\"]).await;\n    fixture.assert_success(\u0026output, \"publish\");\n    fixture.checkpoint(\"post_publish\");\n    \n    // Step 5: Verify skill is searchable\n    fixture.log_step(\"Verify searchable\");\n    let output = fixture.run_ms(\u0026[\"search\", \"async state machine\"]).await;\n    fixture.assert_success(\u0026output, \"search\");\n    fixture.assert_output_contains(\u0026output, \"async-state-machine\");\n    fixture.checkpoint(\"verification_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n#### Scenario 3: Bundle Workflow\n```rust\n#[tokio::test]\nasync fn test_bundle_workflow() {\n    let fixture = E2EFixture::new(\"bundle_workflow\").await;\n    \n    // Setup: Create skills to bundle\n    fixture.log_step(\"Setup test skills\");\n    for i in 1..=5 {\n        fixture.create_skill(\n            \u0026format!(\"bundle-skill-{}\", i),\n            \u0026format!(\"Test skill {} for bundle testing\", i)\n        );\n    }\n    fixture.run_ms(\u0026[\"index\"]).await;\n    fixture.checkpoint(\"skills_indexed\");\n    \n    // Step 1: Create bundle\n    fixture.log_step(\"Create bundle\");\n    let output = fixture.run_ms(\u0026[\n        \"bundle\", \"create\", \"test-bundle\",\n        \"--skills\", \"bundle-skill-1,bundle-skill-2,bundle-skill-3\",\n        \"--description\", \"Test bundle for E2E testing\"\n    ]).await;\n    fixture.assert_success(\u0026output, \"bundle create\");\n    fixture.checkpoint(\"bundle_created\");\n    \n    // Step 2: Publish bundle\n    fixture.log_step(\"Publish bundle\");\n    let output = fixture.run_ms(\u0026[\"bundle\", \"publish\", \"test-bundle\", \"--local\"]).await;\n    fixture.assert_success(\u0026output, \"bundle publish\");\n    fixture.checkpoint(\"bundle_published\");\n    \n    // Step 3: Simulate fresh system install\n    fixture.log_step(\"Simulate fresh install\");\n    let fresh_fixture = E2EFixture::new(\"fresh_install\").await;\n    fresh_fixture.run_ms(\u0026[\"init\"]).await;\n    \n    // Step 4: Install bundle on fresh system\n    fixture.log_step(\"Install bundle on fresh system\");\n    let bundle_path = fixture.get_bundle_path(\"test-bundle\");\n    let output = fresh_fixture.run_ms(\u0026[\"bundle\", \"install\", bundle_path.to_str().unwrap()]).await;\n    fresh_fixture.assert_success(\u0026output, \"bundle install\");\n    fixture.checkpoint(\"bundle_installed\");\n    \n    // Step 5: Verify skills available on fresh system\n    fixture.log_step(\"Verify skills on fresh system\");\n    let output = fresh_fixture.run_ms(\u0026[\"list\"]).await;\n    fresh_fixture.assert_output_contains(\u0026output, \"bundle-skill-1\");\n    fresh_fixture.assert_output_contains(\u0026output, \"bundle-skill-2\");\n    fixture.checkpoint(\"verification_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n#### Scenario 4: Multi-Machine Sync\n```rust\n#[tokio::test]\nasync fn test_multi_machine_sync() {\n    // Machine A setup\n    let machine_a = E2EFixture::new(\"machine_a\").await;\n    machine_a.run_ms(\u0026[\"init\"]).await;\n    machine_a.create_skill(\"sync-test-skill\", \"Skill for sync testing\");\n    machine_a.run_ms(\u0026[\"index\"]).await;\n    machine_a.checkpoint(\"machine_a_setup\");\n    \n    // Export from Machine A\n    machine_a.log_step(\"Export from Machine A\");\n    let output = machine_a.run_ms(\u0026[\"export\", \"--format\", \"portable\"]).await;\n    machine_a.assert_success(\u0026output, \"export\");\n    let export_path = machine_a.get_export_path();\n    machine_a.checkpoint(\"exported\");\n    \n    // Machine B setup\n    let machine_b = E2EFixture::new(\"machine_b\").await;\n    machine_b.run_ms(\u0026[\"init\"]).await;\n    machine_b.checkpoint(\"machine_b_setup\");\n    \n    // Import on Machine B\n    machine_b.log_step(\"Import on Machine B\");\n    let output = machine_b.run_ms(\u0026[\"import\", export_path.to_str().unwrap()]).await;\n    machine_b.assert_success(\u0026output, \"import\");\n    machine_b.checkpoint(\"imported\");\n    \n    // Verify on Machine B\n    machine_b.log_step(\"Verify on Machine B\");\n    let output = machine_b.run_ms(\u0026[\"list\"]).await;\n    machine_b.assert_output_contains(\u0026output, \"sync-test-skill\");\n    machine_b.checkpoint(\"verification_complete\");\n    \n    machine_a.generate_report();\n    machine_b.generate_report();\n}\n```\n\n#### Scenario 5: Error Recovery\n```rust\n#[tokio::test]\nasync fn test_error_recovery() {\n    let fixture = E2EFixture::new(\"error_recovery\").await;\n    fixture.run_ms(\u0026[\"init\"]).await;\n    \n    // Create skills\n    for i in 1..=10 {\n        fixture.create_skill(\n            \u0026format!(\"recovery-skill-{}\", i),\n            \u0026format!(\"Skill {} for recovery testing\", i)\n        );\n    }\n    fixture.checkpoint(\"skills_created\");\n    \n    // Step 1: Start build process\n    fixture.log_step(\"Start build that will be interrupted\");\n    let build_handle = fixture.run_ms_async(\u0026[\"build\", \"--all\"]).await;\n    \n    // Step 2: Interrupt after partial completion\n    fixture.log_step(\"Interrupt build\");\n    tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n    fixture.interrupt_process(build_handle);\n    fixture.checkpoint(\"interrupted\");\n    \n    // Step 3: Check state after interruption\n    fixture.log_step(\"Check state after interruption\");\n    fixture.verify_no_corruption();\n    fixture.checkpoint(\"state_verified\");\n    \n    // Step 4: Resume build\n    fixture.log_step(\"Resume build\");\n    let output = fixture.run_ms(\u0026[\"build\", \"--resume\"]).await;\n    fixture.assert_success(\u0026output, \"build resume\");\n    fixture.checkpoint(\"resumed\");\n    \n    // Step 5: Verify final state\n    fixture.log_step(\"Verify final state\");\n    let output = fixture.run_ms(\u0026[\"list\"]).await;\n    for i in 1..=10 {\n        fixture.assert_output_contains(\u0026output, \u0026format!(\"recovery-skill-{}\", i));\n    }\n    fixture.checkpoint(\"verification_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n#### Scenario 6: Performance Regression\n```rust\n#[tokio::test]\nasync fn test_performance_regression() {\n    let fixture = E2EFixture::new(\"performance_regression\").await;\n    fixture.run_ms(\u0026[\"init\"]).await;\n    \n    // Create many skills for performance testing\n    fixture.log_step(\"Create 1000 skills\");\n    for i in 1..=1000 {\n        fixture.create_skill(\n            \u0026format!(\"perf-skill-{}\", i),\n            \u0026format!(\"Performance test skill number {} with various keywords\", i)\n        );\n    }\n    fixture.checkpoint(\"skills_created\");\n    \n    // Benchmark: Index time\n    fixture.log_step(\"Benchmark: Index\");\n    let start = std::time::Instant::now();\n    let output = fixture.run_ms(\u0026[\"index\"]).await;\n    let index_time = start.elapsed();\n    fixture.assert_success(\u0026output, \"index\");\n    fixture.record_benchmark(\"index_1000_skills\", index_time);\n    fixture.assert_under_threshold(\"index_1000_skills\", Duration::from_secs(10));\n    fixture.checkpoint(\"index_complete\");\n    \n    // Benchmark: Search time (p99)\n    fixture.log_step(\"Benchmark: Search p99\");\n    let mut search_times = Vec::new();\n    for query in \u0026[\"rust\", \"error handling\", \"async await\", \"performance\", \"testing\"] {\n        let start = std::time::Instant::now();\n        fixture.run_ms(\u0026[\"search\", query]).await;\n        search_times.push(start.elapsed());\n    }\n    search_times.sort();\n    let p99 = search_times[search_times.len() * 99 / 100];\n    fixture.record_benchmark(\"search_p99\", p99);\n    fixture.assert_under_threshold(\"search_p99\", Duration::from_millis(50));\n    fixture.checkpoint(\"search_benchmark_complete\");\n    \n    // Benchmark: Load time\n    fixture.log_step(\"Benchmark: Load\");\n    let start = std::time::Instant::now();\n    fixture.run_ms(\u0026[\"load\", \"perf-skill-500\"]).await;\n    let load_time = start.elapsed();\n    fixture.record_benchmark(\"load_skill\", load_time);\n    fixture.assert_under_threshold(\"load_skill\", Duration::from_millis(100));\n    fixture.checkpoint(\"load_benchmark_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n### 2. E2E Fixture Implementation\n\n```rust\npub struct E2EFixture {\n    inner: TestFixture,\n    steps: Vec\u003cTestStep\u003e,\n    checkpoints: Vec\u003cCheckpoint\u003e,\n    benchmarks: HashMap\u003cString, Duration\u003e,\n}\n\nstruct TestStep {\n    number: usize,\n    name: String,\n    timestamp: DateTime\u003cUtc\u003e,\n}\n\nstruct Checkpoint {\n    name: String,\n    timestamp: DateTime\u003cUtc\u003e,\n    db_state: String,\n    file_count: usize,\n}\n\nimpl E2EFixture {\n    pub fn log_step(\u0026mut self, name: \u0026str) {\n        let step = TestStep {\n            number: self.steps.len() + 1,\n            name: name.to_string(),\n            timestamp: Utc::now(),\n        };\n        println!(\"\\n[STEP {}] {} @ {}\", step.number, step.name, step.timestamp);\n        self.steps.push(step);\n    }\n    \n    pub fn checkpoint(\u0026mut self, name: \u0026str) {\n        let checkpoint = Checkpoint {\n            name: name.to_string(),\n            timestamp: Utc::now(),\n            db_state: self.capture_db_state(),\n            file_count: self.count_files(),\n        };\n        println!(\"[CHECKPOINT] {} @ {}\", checkpoint.name, checkpoint.timestamp);\n        println!(\"[CHECKPOINT] DB: {}\", checkpoint.db_state);\n        println!(\"[CHECKPOINT] Files: {}\", checkpoint.file_count);\n        self.checkpoints.push(checkpoint);\n    }\n    \n    pub fn generate_report(\u0026self) {\n        // Generate JUnit XML\n        self.generate_junit_xml();\n        \n        // Generate HTML report\n        self.generate_html_report();\n        \n        // Print summary\n        self.print_summary();\n    }\n    \n    fn generate_junit_xml(\u0026self) {\n        let xml = format!(r#\"\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003ctestsuite name=\"{}\" tests=\"{}\" failures=\"0\" time=\"{}\"\u003e\n{}\n\u003c/testsuite\u003e\"#,\n            self.inner.test_name,\n            self.steps.len(),\n            self.total_time().as_secs_f64(),\n            self.steps.iter().map(|s| format!(\n                r#\"  \u003ctestcase name=\"{}\" time=\"0.0\"/\u003e\"#, s.name\n            )).collect::\u003cVec\u003c_\u003e\u003e().join(\"\\n\")\n        );\n        \n        let report_path = self.inner.temp_dir.path().join(\"junit-report.xml\");\n        std::fs::write(\u0026report_path, xml).expect(\"Failed to write JUnit report\");\n        println!(\"[REPORT] JUnit XML: {:?}\", report_path);\n    }\n    \n    fn generate_html_report(\u0026self) {\n        let html = format!(r#\"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eE2E Test Report: {}\u003c/title\u003e\n    \u003cstyle\u003e\n        body {{ font-family: sans-serif; margin: 20px; }}\n        .step {{ margin: 10px 0; padding: 10px; background: #f5f5f5; }}\n        .checkpoint {{ margin: 10px 0; padding: 10px; background: #e0ffe0; }}\n        .expandable {{ cursor: pointer; }}\n        .details {{ display: none; margin-left: 20px; }}\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003ch1\u003eE2E Test Report: {}\u003c/h1\u003e\n    \u003ch2\u003eSteps\u003c/h2\u003e\n    {}\n    \u003ch2\u003eCheckpoints\u003c/h2\u003e\n    {}\n    \u003ch2\u003eBenchmarks\u003c/h2\u003e\n    {}\n\u003c/body\u003e\n\u003c/html\u003e\"#,\n            self.inner.test_name,\n            self.inner.test_name,\n            self.render_steps_html(),\n            self.render_checkpoints_html(),\n            self.render_benchmarks_html()\n        );\n        \n        let report_path = self.inner.temp_dir.path().join(\"report.html\");\n        std::fs::write(\u0026report_path, html).expect(\"Failed to write HTML report\");\n        println!(\"[REPORT] HTML: {:?}\", report_path);\n    }\n}\n```\n\n### 3. Logging Requirements\n\nEvery E2E test must log:\n- **Timestamp**: For every step and checkpoint\n- **Command invocations**: Full command with all arguments\n- **stdout/stderr**: Captured separately\n- **Timing**: Duration for each step\n- **Database state**: At each checkpoint\n- **JUnit XML**: Generated for CI integration\n- **HTML report**: With expandable details for debugging\n\n### 4. CI Integration\n\nAdd to CI pipeline:\n```yaml\ne2e-tests:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v4\n    - name: Run E2E tests\n      run: cargo test --test e2e -- --test-threads=1\n    - name: Upload test reports\n      uses: actions/upload-artifact@v4\n      with:\n        name: e2e-reports\n        path: |\n          target/e2e-reports/*.xml\n          target/e2e-reports/*.html\n    - name: Publish test results\n      uses: dorny/test-reporter@v1\n      with:\n        name: E2E Test Results\n        path: target/e2e-reports/*.xml\n        reporter: java-junit\n```\n\n## Acceptance Criteria\n\n1. [ ] Fresh install workflow test passing\n2. [ ] Skill creation workflow test passing\n3. [ ] Bundle workflow test passing\n4. [ ] Multi-machine sync test passing\n5. [ ] Error recovery test passing\n6. [ ] Performance regression test passing\n7. [ ] JUnit XML reports generated\n8. [ ] HTML reports with expandable details\n9. [ ] All tests log timestamps for every step\n10. [ ] All tests capture stdout/stderr separately\n11. [ ] Database state logged at checkpoints\n12. [ ] Performance thresholds enforced\n\n## Dependencies\n\n- meta_skill-9pr (Integration Test Framework) - provides TestFixture base","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T22:56:33.829543302-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.859151863-05:00","labels":["e2e","scripts","testing"],"dependencies":[{"issue_id":"meta_skill-2kd","depends_on_id":"meta_skill-9pr","type":"blocks","created_at":"2026-01-13T22:56:38.556953575-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-2kd","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.178879716-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-327","title":"RU (Repo Updater) Integration for Skill Sync","description":"# RU (Repo Updater) Integration for Skill Sync\n\n## Overview\n\nIntegrate ru (repo_updater) from /data/projects/repo_updater as the repository synchronization layer for ms. ru provides battle-tested GitHub repo syncing with parallel operations, conflict detection, and automation-friendly JSON output.\n\n**Location**: `/data/projects/repo_updater`\n**Documentation**: `/data/projects/repo_updater/README.md`\n\n## Why ru (not custom implementation)\n\n| Aspect | Custom Implementation | ru Integration |\n|--------|----------------------|----------------|\n| **Maturity** | New, untested | Production-ready |\n| **Parallel sync** | Must build | Built-in with work-stealing |\n| **Conflict detection** | Manual | Automatic with resolution commands |\n| **Git plumbing** | String parsing | Reliable rev-list/porcelain |\n| **Exit codes** | Define own | Semantic (0-5) |\n| **Resume** | Build from scratch | --resume supported |\n\n## Use Cases for ms\n\n### 1. Skill Repository Sync\nSkills can be distributed as GitHub repositories:\n- User maintains skill repos in repos.d/skills.txt\n- `ms sync` calls ru to sync all skill repos\n- ru handles clone, pull, conflict detection\n- ms re-indexes after sync\n\n### 2. Skill Source Discovery\nru provides a list of all user's repositories:\n- `ru list --paths` outputs repo paths\n- ms can scan these for skills\n- Integration with ms index --discover\n\n### 3. Multi-Machine Skill Sync\nWhen skills are stored in Git repos:\n- Changes pushed to GitHub from machine A\n- ru sync on machine B pulls updates\n- ms automatically re-indexes changed skills\n\n### 4. Bundle Distribution via GitHub\nGitHub-hosted skill bundles:\n- `ms bundle publish` creates GitHub release\n- Other users add repo to ru config\n- `ru sync` + `ms bundle install` updates skills\n\n## Architecture\n\n```rust\n/// ru client for repository sync\nstruct RuClient {\n    /// Path to ru binary\n    ru_path: PathBuf,\n    /// Default flags for automation\n    default_flags: Vec\u003cString\u003e,\n}\n\nimpl RuClient {\n    /// Sync all configured repos\n    async fn sync(\u0026self, opts: SyncOptions) -\u003e Result\u003cSyncResult\u003e {\n        // Call: ru sync --non-interactive --json\n    }\n    \n    /// Get list of all repo paths\n    async fn list_paths(\u0026self) -\u003e Result\u003cVec\u003cPathBuf\u003e\u003e {\n        // Call: ru list --paths\n    }\n    \n    /// Check sync status without changes\n    async fn status(\u0026self) -\u003e Result\u003cRepoStatus\u003e {\n        // Call: ru status --no-fetch --json\n    }\n    \n    /// Sync specific repo\n    async fn sync_repo(\u0026self, repo: \u0026str) -\u003e Result\u003cSyncResult\u003e {\n        // Call: ru sync --filter \u003crepo\u003e --json\n    }\n}\n\nstruct SyncOptions {\n    parallel: Option\u003cu32\u003e,     // -j4\n    dry_run: bool,             // --dry-run\n    autostash: bool,           // --autostash\n}\n\n#[derive(Deserialize)]\nstruct SyncResult {\n    cloned: Vec\u003cString\u003e,\n    updated: Vec\u003cString\u003e,\n    current: Vec\u003cString\u003e,\n    conflicts: Vec\u003cConflictInfo\u003e,\n    exit_code: u8,\n}\n\n#[derive(Deserialize)]\nstruct ConflictInfo {\n    repo: String,\n    status: String,  // diverged, dirty, auth_failed\n    resolution: String,  // Copy-paste command\n}\n```\n\n## CLI Commands\n\n```bash\n# Sync skill repositories (wraps ru)\nms sync                     # Sync all skill repos\nms sync --parallel 4        # Parallel sync\nms sync --dry-run           # Preview changes\nms sync --status            # Status without sync\n\n# Discover skills in synced repos\nms index --discover         # Scan ru repos for skills\nms index --from-ru          # Index only ru-managed repos\n\n# Repo management integration\nms repo list               # List skill repos\nms repo add \u003crepo\u003e         # Add to ru config + skill sources\nms repo remove \u003crepo\u003e      # Remove from both\n\n# Status and health\nms sync status             # Sync status summary\nms sync health             # Repo health check\n```\n\n## Exit Code Mapping\n\nru exit codes (from ru docs):\n- 0 = All repos synced successfully\n- 1 = Partial success (some repos had issues)\n- 2 = Conflicts detected (need attention)\n- 3 = System error (git not found, etc.)\n- 4 = Bad arguments\n- 5 = Interrupted (can --resume)\n\nms maps these for user feedback:\n```rust\nfn handle_sync_result(exit_code: u8) -\u003e MsResult {\n    match exit_code {\n        0 =\u003e Ok(SyncSuccess),\n        1 =\u003e Ok(PartialSuccess { warning: \"Some repos skipped\" }),\n        2 =\u003e Err(ConflictsNeedAttention),\n        3 =\u003e Err(SystemError(\"Git/ru issue\")),\n        4 =\u003e Err(InvalidConfig),\n        5 =\u003e Ok(Interrupted { can_resume: true }),\n    }\n}\n```\n\n## Configuration\n\n```yaml\n# ~/.ms/config.yaml\nsync:\n  # Use ru for repo sync\n  backend: ru\n  \n  # ru-managed skill repos\n  skill_repos:\n    - \"Dicklesworthstone/claude-code-skills\"\n    - \"myorg/internal-skills@main\"\n  \n  # Auto-reindex after sync\n  auto_reindex: true\n  \n  # Parallel workers\n  parallel: 4\n  \n  # Autostash on conflict\n  autostash: true\n```\n\n## Tasks\n\n1. [ ] Detect ru installation and version\n2. [ ] Implement RuClient wrapper\n3. [ ] Parse ru JSON output format\n4. [ ] Build ms sync CLI commands\n5. [ ] Integrate with skill discovery (ms index --from-ru)\n6. [ ] Add repo management commands\n7. [ ] Handle exit codes with user feedback\n8. [ ] Auto-reindex after successful sync\n9. [ ] Document ru configuration for skills\n10. [ ] Handle ru unavailable gracefully\n\n## Testing Requirements\n\n- ru integration tests (sync, list, status)\n- Exit code handling correctness\n- JSON output parsing\n- Conflict scenario handling\n- Auto-reindex triggering\n- Graceful degradation without ru\n\n## Acceptance Criteria\n\n- ru detected and integrated\n- ms sync works with ru backend\n- Conflicts reported with resolutions\n- Auto-reindex after sync\n- Skill repos configurable\n- Works without ru (manual mode)\n\n## Dependencies\n\n- Phase 5 foundation (bundle distribution)\n- Multi-machine sync bead (meta_skill-ujr)\n- Skill discovery/indexing infrastructure\n\n## References\n\n- ru repository: /data/projects/repo_updater\n- ru README: /data/projects/repo_updater/README.md\n- AGENTS.md ru section\n- Plan Section 5.x (multi-machine sync)\n\nLabels: [phase-5 integration sync ru multi-machine]","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:18:04.189594404-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:18:04.189594404-05:00","labels":["integration","multi-machine","phase-5","ru","sync"],"dependencies":[{"issue_id":"meta_skill-327","depends_on_id":"meta_skill-ujr","type":"blocks","created_at":"2026-01-13T23:18:20.003887715-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-327","depends_on_id":"meta_skill-yu1","type":"blocks","created_at":"2026-01-13T23:18:21.125093961-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-330","title":"[P4] Interactive Build TUI","description":"# Interactive Build TUI\n\n## Overview\n\nProvide an interactive, guided UI for `ms build` with checkpoints, progress indicators, and inline review of extracted patterns. The TUI is a UX layer over the build pipeline and must not alter core behavior.\n\n---\n\n## Tasks\n\n1. Implement TUI flows (stage progress, pause, resume).\n2. Show extracted patterns and allow user accept/reject.\n3. Display uncertainty items and allow escalation.\n4. Export final SkillSpec preview before commit.\n\n---\n\n## Testing Requirements\n\n- Integration tests: TUI uses same pipeline as `ms build`.\n- Snapshot tests for screen layouts.\n- E2E: scripted TUI flow (accept/reject patterns).\n\n---\n\n## Acceptance Criteria\n\n- TUI never bypasses safety filters.\n- TUI can resume from checkpoints.\n- TUI outputs identical SkillSpec as non‑TUI build.\n\n---\n\n## Dependencies\n\n- `meta_skill-ztm` ms build Command","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:50.439572634-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:54:32.846388341-05:00","labels":["build","phase-4","tui"],"dependencies":[{"issue_id":"meta_skill-330","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:13.126510003-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-330","depends_on_id":"meta_skill-9r9","type":"blocks","created_at":"2026-01-13T22:26:13.151767172-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-330","depends_on_id":"meta_skill-ztm","type":"blocks","created_at":"2026-01-13T23:55:50.898963277-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-36x","title":"CASS Mining: Debugging Workflows","description":"Deep dive into debugging patterns across projects, systematic bug hunting, root cause analysis methodologies.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:38.882245568-05:00","created_by":"ubuntu","updated_at":"2026-01-13T21:12:20.589569786-05:00","closed_at":"2026-01-13T21:12:20.589569786-05:00","close_reason":"Added Section 37: Debugging Workflows and Methodologies (~615 lines). CASS mined brenner_bot, cass, caam, mcp_agent_mail, fix_my_documents_backend, and agentic_coding_flywheel_setup for debugging patterns covering: systematic debugging philosophy, race condition hunting, error handling detection, performance profiling, N+1 query patterns, test failure analysis, investigation report formats, structured logging, concurrency debugging, timeout handling, and comprehensive checklists by bug type.","labels":["cass-mining"]}
{"id":"meta_skill-4d7","title":"CASS Mining: Inner Truth/Abstract Principles (brenner_bot)","description":"Deep dive into brenner_bot CASS sessions for inner truth extraction, abstract principles, multi-model synthesis triangulation, metaprompt refinement patterns. This is a gold mine of skill methodology.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-13T17:47:17.426402691-05:00","created_by":"ubuntu","updated_at":"2026-01-13T17:51:40.014892136-05:00","closed_at":"2026-01-13T17:51:40.014892136-05:00","close_reason":"Section 28 added to plan with Brenner methodology for skill extraction","labels":["cass-mining"]}
{"id":"meta_skill-4g1","title":"Uncertainty Queue (Active Learning)","description":"## Section Reference\nSection 5.15 - Uncertainty Queue and Active Learning\n\n## Overview\nWhen generalization confidence is too low, queue candidates for targeted evidence gathering. Generate 3-7 targeted CASS queries per uncertainty (positive, negative, boundary cases).\n\n## Core Concept\nActive learning loop: when the system cannot confidently generalize a pattern, it queues the uncertainty and generates targeted queries to gather more evidence. This closes the feedback loop between pattern mining and evidence collection.\n\n## Data Structures\n\n```rust\n/// An item in the uncertainty queue awaiting resolution\nstruct UncertaintyItem {\n    /// Unique identifier for this uncertainty\n    id: UncertaintyId,\n    /// The pattern candidate that triggered uncertainty\n    pattern_candidate: ExtractedPattern,\n    /// Why confidence is too low\n    reason: MissingSignal,\n    /// Current confidence score (0.0-1.0)\n    confidence: f32,\n    /// Minimum confidence threshold for acceptance\n    threshold: f32,\n    /// Generated queries to gather evidence\n    suggested_queries: Vec\u003cSuggestedQuery\u003e,\n    /// Current resolution status\n    status: UncertaintyStatus,\n    /// When this item was created\n    created_at: DateTime\u003cUtc\u003e,\n    /// When this was last updated\n    updated_at: DateTime\u003cUtc\u003e,\n    /// Resolution attempts history\n    attempts: Vec\u003cResolutionAttempt\u003e,\n}\n\n/// Why confidence is insufficient\nenum MissingSignal {\n    /// Not enough examples to generalize\n    InsufficientInstances { \n        have: u32, \n        need: u32,\n        variance: f32,\n    },\n    /// Examples show high variation\n    HighVariance {\n        variance_score: f32,\n        conflicting_aspects: Vec\u003cString\u003e,\n    },\n    /// Found examples that contradict the pattern\n    CounterExampleFound {\n        counter_example: SessionId,\n        contradiction: String,\n    },\n    /// Scope/applicability unclear\n    AmbiguousScope {\n        possible_scopes: Vec\u003cScopeCandidate\u003e,\n    },\n    /// Preconditions unclear\n    UnclearPreconditions {\n        candidates: Vec\u003cPredicate\u003e,\n    },\n    /// Effect boundaries unknown\n    UnknownBoundaries {\n        dimension: String,\n        observed_range: (f32, f32),\n    },\n}\n\n/// Status of uncertainty resolution\nenum UncertaintyStatus {\n    /// Waiting in queue\n    Pending,\n    /// Currently gathering evidence\n    InProgress { \n        started_at: DateTime\u003cUtc\u003e,\n        queries_completed: u32,\n    },\n    /// Resolved - pattern accepted\n    Resolved { \n        new_confidence: f32,\n        resolution: Resolution,\n    },\n    /// Resolved - pattern rejected\n    Rejected { \n        reason: String,\n    },\n    /// Stalled - needs human input\n    NeedsHuman { \n        reason: String,\n    },\n    /// Expired - aged out\n    Expired,\n}\n\nenum Resolution {\n    /// Gathered enough evidence to accept\n    EvidenceGathered { new_sessions: Vec\u003cSessionId\u003e },\n    /// Refined pattern to be more specific\n    PatternRefined { new_pattern: ExtractedPattern },\n    /// Split into multiple patterns\n    PatternSplit { patterns: Vec\u003cExtractedPattern\u003e },\n    /// Human provided clarification\n    HumanClarified { annotation: String },\n}\n\n/// A suggested query to gather evidence\nstruct SuggestedQuery {\n    /// Query type\n    query_type: QueryType,\n    /// Natural language query\n    query: String,\n    /// CASS-formatted query if applicable\n    cass_query: Option\u003cString\u003e,\n    /// What evidence this would provide\n    expected_evidence: String,\n    /// Priority (higher = more valuable)\n    priority: u32,\n    /// Whether this query has been executed\n    executed: bool,\n    /// Results if executed\n    results: Option\u003cQueryResults\u003e,\n}\n\nenum QueryType {\n    /// Looking for positive examples\n    Positive,\n    /// Looking for negative examples / counter-examples\n    Negative,\n    /// Looking for boundary cases\n    Boundary,\n    /// Looking for scope clarification\n    ScopeClarification,\n    /// Looking for precondition evidence\n    PreconditionEvidence,\n}\n\n/// The uncertainty queue\nstruct UncertaintyQueue {\n    /// All items in the queue\n    items: Vec\u003cUncertaintyItem\u003e,\n    /// Configuration\n    config: UncertaintyConfig,\n    /// Statistics\n    stats: QueueStats,\n}\n\nstruct UncertaintyConfig {\n    /// Minimum confidence to skip queue\n    min_confidence: f32,\n    /// Maximum items to hold before forcing resolution\n    max_queue_size: usize,\n    /// Number of queries to generate per uncertainty\n    queries_per_uncertainty: RangeInclusive\u003cu32\u003e, // 3..=7\n    /// How long before expiry\n    expiry_duration: Duration,\n    /// Auto-resolve if possible\n    auto_resolve: bool,\n}\n\nstruct QueueStats {\n    total_queued: u64,\n    total_resolved: u64,\n    total_rejected: u64,\n    total_expired: u64,\n    average_resolution_time: Duration,\n    average_queries_needed: f32,\n}\n```\n\n## Query Generation\n\n```rust\ntrait QueryGenerator {\n    /// Generate targeted queries for an uncertainty\n    fn generate_queries(\n        \u0026self,\n        uncertainty: \u0026UncertaintyItem,\n        existing_evidence: \u0026[Session],\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n    \n    /// Generate positive example queries\n    fn generate_positive_queries(\n        \u0026self,\n        pattern: \u0026ExtractedPattern,\n        count: u32,\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n    \n    /// Generate negative/counter-example queries  \n    fn generate_negative_queries(\n        \u0026self,\n        pattern: \u0026ExtractedPattern,\n        count: u32,\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n    \n    /// Generate boundary case queries\n    fn generate_boundary_queries(\n        \u0026self,\n        pattern: \u0026ExtractedPattern,\n        count: u32,\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n}\n\n/// Example query generation for a file-handling pattern\nfn example_query_generation(pattern: \u0026ExtractedPattern) -\u003e Vec\u003cSuggestedQuery\u003e {\n    vec![\n        SuggestedQuery {\n            query_type: QueryType::Positive,\n            query: \"Show sessions where I successfully handled large files with streaming\".into(),\n            cass_query: Some(\"topic:file-handling AND outcome:success AND size:large\".into()),\n            expected_evidence: \"More positive examples of the pattern\".into(),\n            priority: 3,\n            executed: false,\n            results: None,\n        },\n        SuggestedQuery {\n            query_type: QueryType::Negative,\n            query: \"Show sessions where file handling failed or I had to retry\".into(),\n            cass_query: Some(\"topic:file-handling AND (outcome:failure OR action:retry)\".into()),\n            expected_evidence: \"Counter-examples or boundary failures\".into(),\n            priority: 2,\n            executed: false,\n            results: None,\n        },\n        SuggestedQuery {\n            query_type: QueryType::Boundary,\n            query: \"Show sessions with medium-sized files around 1GB\".into(),\n            cass_query: Some(\"topic:file-handling AND size:1gb..2gb\".into()),\n            expected_evidence: \"Evidence for where pattern boundaries lie\".into(),\n            priority: 1,\n            executed: false,\n            results: None,\n        },\n    ]\n}\n```\n\n## Resolution Engine\n\n```rust\ntrait UncertaintyResolver {\n    /// Attempt to resolve an uncertainty with new evidence\n    fn attempt_resolution(\n        \u0026self,\n        uncertainty: \u0026mut UncertaintyItem,\n        new_evidence: \u0026[Session],\n    ) -\u003e ResolutionResult;\n    \n    /// Check if uncertainty can be auto-resolved\n    fn can_auto_resolve(\u0026self, uncertainty: \u0026UncertaintyItem) -\u003e bool;\n    \n    /// Escalate to human if needed\n    fn escalate_to_human(\u0026self, uncertainty: \u0026mut UncertaintyItem, reason: \u0026str);\n}\n\nenum ResolutionResult {\n    /// Resolved successfully\n    Resolved(Resolution),\n    /// Need more evidence\n    NeedsMoreEvidence { remaining_queries: Vec\u003cSuggestedQuery\u003e },\n    /// Pattern should be rejected\n    Reject { reason: String },\n    /// Needs human intervention\n    Escalate { reason: String },\n}\n\nfn attempt_resolution(\n    uncertainty: \u0026mut UncertaintyItem,\n    new_sessions: \u0026[Session],\n) -\u003e ResolutionResult {\n    // Re-extract pattern with new evidence\n    let all_sessions = collect_all_sessions(uncertainty, new_sessions);\n    let refined_pattern = extract_pattern(\u0026all_sessions);\n    \n    // Calculate new confidence\n    let new_confidence = calculate_confidence(\u0026refined_pattern, \u0026all_sessions);\n    \n    if new_confidence \u003e= uncertainty.threshold {\n        return ResolutionResult::Resolved(Resolution::EvidenceGathered {\n            new_sessions: new_sessions.iter().map(|s| s.id).collect(),\n        });\n    }\n    \n    // Check for counter-examples\n    if let Some(counter) = find_counter_examples(\u0026refined_pattern, new_sessions) {\n        if counter.is_fundamental {\n            return ResolutionResult::Reject {\n                reason: format!(\"Counter-example invalidates pattern: {}\", counter.description),\n            };\n        }\n        // Maybe pattern needs refinement\n        return ResolutionResult::NeedsMoreEvidence {\n            remaining_queries: generate_refinement_queries(\u0026counter),\n        };\n    }\n    \n    // Check if more queries available\n    let remaining: Vec\u003c_\u003e = uncertainty.suggested_queries\n        .iter()\n        .filter(|q| !q.executed)\n        .cloned()\n        .collect();\n    \n    if remaining.is_empty() {\n        ResolutionResult::Escalate {\n            reason: \"All queries exhausted, still below threshold\".into(),\n        }\n    } else {\n        ResolutionResult::NeedsMoreEvidence { remaining_queries: remaining }\n    }\n}\n```\n\n## CLI Commands\n\n```bash\n# List uncertainties in queue\nms uncertainties list\nms uncertainties list --status pending\nms uncertainties list --reason insufficient-instances\n\n# Show details of specific uncertainty\nms uncertainties show \u003cuncertainty-id\u003e\n\n# Mine sessions to resolve uncertainties\nms uncertainties --mine\nms uncertainties --mine --limit 5\n\n# Execute suggested queries\nms uncertainties query \u003cuncertainty-id\u003e\nms uncertainties query \u003cuncertainty-id\u003e --query-index 0\n\n# Manually resolve uncertainty\nms uncertainties resolve \u003cuncertainty-id\u003e --accept\nms uncertainties resolve \u003cuncertainty-id\u003e --reject --reason \"Pattern too specific\"\n\n# Build with auto-resolution\nms build --auto-resolve-uncertainties\n\n# Queue statistics\nms uncertainties stats\n\n# Expire old uncertainties\nms uncertainties prune --older-than 30d\n```\n\n## Output Format\n\n```\n$ ms uncertainties list\n\nUNCERTAINTY QUEUE (3 items)\n================================================================================\n\n[U-7f3a] Pattern: error-handling-with-retry\n  Reason: InsufficientInstances (have: 2, need: 5)\n  Confidence: 0.42 (threshold: 0.70)\n  Queries: 5 suggested, 1 executed\n  Status: Pending\n  Age: 2 days\n\n[U-8b2c] Pattern: git-branch-naming\n  Reason: HighVariance (variance: 0.68)\n  Confidence: 0.55 (threshold: 0.70)  \n  Queries: 4 suggested, 3 executed\n  Status: InProgress\n  Age: 5 days\n\n[U-9d1e] Pattern: test-file-organization\n  Reason: AmbiguousScope (2 possible scopes)\n  Confidence: 0.38 (threshold: 0.70)\n  Queries: 6 suggested, 0 executed\n  Status: NeedsHuman\n  Age: 12 days\n```\n\n## Integration Points\n\n- **Specific-to-General Transformation** (meta_skill-9r9): Uncertainties arise during generalization\n- **Pattern Extraction Pipeline**: Queue candidates from extraction\n- **CASS Query Interface**: Execute generated queries\n- **Anti-Pattern Mining**: Counter-examples feed anti-pattern detection\n- **Build Pipeline**: --auto-resolve-uncertainties flag integration\n\n## Queue Management\n\n```rust\nimpl UncertaintyQueue {\n    /// Add new uncertainty to queue\n    fn enqueue(\u0026mut self, item: UncertaintyItem) -\u003e Result\u003cUncertaintyId, QueueError\u003e;\n    \n    /// Get next item to process\n    fn next(\u0026mut self) -\u003e Option\u003c\u0026mut UncertaintyItem\u003e;\n    \n    /// Process queue with available evidence\n    fn process(\u0026mut self, evidence_store: \u0026EvidenceStore) -\u003e ProcessResult;\n    \n    /// Prune expired items\n    fn prune_expired(\u0026mut self) -\u003e Vec\u003cUncertaintyItem\u003e;\n    \n    /// Get queue statistics\n    fn stats(\u0026self) -\u003e QueueStats;\n}\n```\n\n## Testing Requirements\n\n- Unit tests for each MissingSignal type\n- Query generation tests (verify 3-7 queries generated)\n- Resolution flow tests\n- Queue management tests (enqueue, dequeue, prune)\n- Integration test: full uncertainty lifecycle\n- Test auto-resolution with mock evidence\n- Test escalation to human","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:53:29.725303472-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:53:29.725303472-05:00","labels":["active-learning","phase-4","uncertainty"],"dependencies":[{"issue_id":"meta_skill-4g1","depends_on_id":"meta_skill-9r9","type":"blocks","created_at":"2026-01-13T22:57:36.081054335-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-4ih","title":"Phase 2: Search (Hybrid Search Engine)","description":"# Epic: Phase 2 Search (Hybrid Search Engine)\n\n## Goal\n\nImplement hybrid skill search (BM25 + hash embeddings + RRF) with filters and alias support.\n\n---\n\n## Scope\n\n- Tantivy BM25 index\n- Hash embedding backend\n- RRF fusion\n- Search filters + alias resolution\n- `ms search` command\n\n---\n\n## Acceptance Criteria\n\n- Search returns deterministic ranked results.\n- Robot output schema stable.\n- Index updates do not drift.\n\n---\n\n## Child Beads\n\n- `meta_skill-mh8` Tantivy BM25\n- `meta_skill-ch6` Hash Embeddings\n- `meta_skill-93z` RRF Fusion\n- `meta_skill-5e6` Search Filters\n- `meta_skill-r6k` Skill Alias System\n- `meta_skill-0ki` ms search Command","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:53.002811842-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:13:31.831879564-05:00","dependencies":[{"issue_id":"meta_skill-4ih","depends_on_id":"meta_skill-6hm","type":"blocks","created_at":"2026-01-13T22:21:01.826297143-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-4ki","title":"Phase 4: CASS Integration (Skill Mining)","description":"# Phase 4: CASS Integration\n\nThe killer feature: generating skills from CASS session history.\n\n## Core Components\n1. **CASS client** - Subprocess integration with robot mode\n2. **Pattern extraction** - Mine commands, solutions, pitfalls from sessions\n3. **Pattern IR** - Typed intermediate representation\n4. **Specific-to-general transformation** - Extract universal patterns\n5. **Session marking** - Mark sessions as exemplary/anti-pattern\n6. **Provenance graph** - Link rules to evidence\n7. **Redaction pipeline** - Strip secrets/PII before extraction\n8. **Taint tracking** - Ensure unsafe content never leaks\n9. **Interactive build TUI** - Guided skill generation\n10. **Autonomous generation** - Hours-long checkpointed builds\n11. **Session quality scoring** - Filter low-quality sessions\n12. **Anti-pattern mining** - Extract what NOT to do\n13. **Uncertainty queue** - Active learning for low-confidence patterns\n\n## Key Design Decisions\n- Brenner Method: extract generative grammar, not summaries\n- Counterexamples are first-class patterns\n- Steady-state detection for convergence\n- Provenance is compressed (pointer + fetch on demand)\n\n## Success Criteria\n- `ms build --from-cass \"topic\"` extracts patterns\n- `ms build --guided` runs interactive TUI\n- `ms build --autonomous --duration 4h` runs with checkpoints\n- Generated skills include provenance links\n- Redaction prevents secrets in output","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:54.26921675-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:54.26921675-05:00","dependencies":[{"issue_id":"meta_skill-4ki","depends_on_id":"meta_skill-y73","type":"blocks","created_at":"2026-01-13T22:21:01.877739821-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-4tx","title":"Phase 6: Polish \u0026 Auto-Update","description":"# Phase 6: Polish \u0026 Auto-Update\n\nFinal polish, TUI, MCP server, doctor command, and auto-update.\n\n## Core Components\n1. **Doctor command** - Comprehensive health checks with --fix\n2. **Shell integration** - Completions, aliases, environment setup\n3. **MCP server mode** - Native agent tool-use integration\n4. **Auto-update** - Self-updating binary via GitHub Releases\n5. **Interactive build TUI** - Ratatui-based interface\n6. **Skill effectiveness tracking** - A/B experiments, feedback loop\n7. **Quality scoring updates** - Learn from usage\n8. **Skill versioning** - Semantic versions with migrations\n9. **Error recovery** - Retry, rate limit handling, graceful degradation\n10. **Skill templates** - Pre-built patterns for rapid creation\n\n## Key Design Decisions\n- MCP eliminates subprocess overhead for agents\n- Doctor checks are categorized and auto-fixable\n- Update checks happen in background, apply on restart\n- Experiments can target individual slices\n\n## Success Criteria\n- `ms doctor` identifies and fixes issues\n- `ms mcp serve` provides tool definitions\n- `ms update` self-updates the binary\n- TUI is responsive and intuitive\n- Effectiveness data improves skill quality over time","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-13T22:20:56.361593099-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:56.361593099-05:00","dependencies":[{"issue_id":"meta_skill-4tx","depends_on_id":"meta_skill-yu1","type":"blocks","created_at":"2026-01-13T22:21:01.929265045-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-5e6","title":"[P2] Search Filters","description":"# Search Filters\n\n## Overview\n\nProvide structured filters for search and suggestion: tags, layers, deprecated status, min quality, coverage groups. Filters must work for both human and robot mode.\n\n---\n\n## Tasks\n\n1. Define filter schema and parse CLI flags.\n2. Apply filters consistently in BM25 + embedding results.\n3. Expose filters in robot output (for audits).\n\n---\n\n## Testing Requirements\n\n- Unit tests for filter parsing.\n- Integration tests on fixture indexes.\n\n---\n\n## Acceptance Criteria\n\n- Filters correctly narrow results.\n- Robot output contains applied filters.\n\n---\n\n## Dependencies\n\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:23:05.345921069-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:00:31.722866303-05:00","labels":["filters","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-5e6","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T22:23:13.595096509-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-5e6","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:00:40.063283927-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-5s0","title":"[P1] Rust Project Scaffolding","description":"## Overview\n\nBootstrap the Rust project structure following the exact patterns from xf (~23k LOC). This includes Cargo.toml configuration, module organization, error handling, and foundational types that all other beads depend on.\n\n## Background \u0026 Rationale\n\n### Why Follow xf Exactly\n\nThe xf project (X Archive Search) is our reference implementation:\n- Battle-tested architecture for CLI tools with SQLite + Tantivy\n- Proven patterns for dual persistence (SQLite + Git)\n- Established error handling and logging patterns\n- Working CI/CD pipeline we can adapt\n- ~23k LOC of production-quality Rust code\n\n### Key Architectural Decisions\n\n1. **Workspace vs Single Crate**: Single crate initially (simpler), workspace later if needed\n2. **Error Handling**: thiserror + anyhow pattern (thiserror for library, anyhow for binary)\n3. **Async**: tokio runtime (consistent with ecosystem tools)\n4. **CLI Framework**: clap v4 with derive macros\n5. **Logging**: tracing + tracing-subscriber (structured, async-aware)\n\n---\n\n## Complete File Layout (from Plan Section 2.3)\n\nThis is the authoritative file layout from the big plan. All paths must match exactly.\n\n```\nmeta_skill/\n├── Cargo.toml\n├── Cargo.lock\n├── src/\n│   ├── main.rs                    # Entry point, CLI setup\n│   ├── lib.rs                     # Library root\n│   ├── cli/\n│   │   ├── mod.rs\n│   │   ├── commands/\n│   │   │   ├── mod.rs\n│   │   │   ├── index.rs           # ms index\n│   │   │   ├── search.rs          # ms search\n│   │   │   ├── load.rs            # ms load\n│   │   │   ├── suggest.rs         # ms suggest\n│   │   │   ├── edit.rs            # ms edit\n│   │   │   ├── fmt.rs             # ms fmt\n│   │   │   ├── diff.rs            # ms diff\n│   │   │   ├── alias.rs           # ms alias\n│   │   │   ├── requirements.rs    # ms requirements\n│   │   │   ├── build.rs           # ms build (CASS integration)\n│   │   │   ├── bundle.rs          # ms bundle\n│   │   │   ├── update.rs          # ms update\n│   │   │   ├── doctor.rs          # ms doctor\n│   │   │   ├── prune.rs           # ms prune\n│   │   │   ├── init.rs            # ms init\n│   │   │   └── config.rs          # ms config\n│   │   └── output.rs              # Robot mode, human mode formatting\n│   ├── core/\n│   │   ├── mod.rs\n│   │   ├── skill.rs               # Skill struct and parsing\n│   │   ├── registry.rs            # Skill registry management\n│   │   ├── disclosure.rs          # Progressive disclosure logic\n│   │   ├── safety.rs              # Destructive ops policy + approvals\n│   │   ├── requirements.rs        # Environment requirement checks\n│   │   ├── spec_lens.rs           # Round-trip spec ↔ markdown mapping\n│   │   └── validation.rs          # Skill validation\n│   ├── storage/\n│   │   ├── mod.rs\n│   │   ├── sqlite.rs              # SQLite operations\n│   │   ├── git.rs                 # Git persistence layer\n│   │   └── migrations.rs          # Schema migrations\n│   ├── search/\n│   │   ├── mod.rs\n│   │   ├── tantivy.rs             # Full-text indexing\n│   │   ├── embeddings.rs          # Embedder trait + hash embedder\n│   │   ├── embeddings_local.rs    # Optional local ML embedder\n│   │   ├── hybrid.rs              # RRF fusion\n│   │   └── context.rs             # Context-aware ranking\n│   ├── cass/\n│   │   ├── mod.rs\n│   │   ├── client.rs              # CASS CLI integration\n│   │   ├── mining.rs              # Pattern extraction\n│   │   ├── synthesis.rs           # Skill generation\n│   │   └── refinement.rs          # Iterative improvement\n│   ├── bundler/\n│   │   ├── mod.rs\n│   │   ├── package.rs             # Bundle creation\n│   │   ├── github.rs              # GitHub publishing\n│   │   └── install.rs             # Bundle installation\n│   ├── updater/\n│   │   ├── mod.rs\n│   │   ├── check.rs               # Version checking\n│   │   ├── download.rs            # Binary download\n│   │   └── verify.rs              # SHA256 verification\n│   └── utils/\n│       ├── mod.rs\n│       ├── fs.rs                  # Filesystem utilities\n│       ├── git.rs                 # Git utilities\n│       └── format.rs              # Output formatting\n├── migrations/\n│   ├── 001_initial_schema.sql\n│   ├── 002_add_fts.sql\n│   └── 003_add_vectors.sql\n├── tests/\n│   ├── integration/\n│   └── fixtures/\n├── .github/\n│   └── workflows/\n│       ├── ci.yml\n│       └── release.yml\n└── README.md\n```\n\n**Runtime Artifacts:**\n- `.ms/skillpack.bin` (or per-skill pack objects) caches parsed spec, slices,\n  embeddings, and predicate analysis for low-latency load/suggest.\n- Markdown remains a compiled view; runtime uses the pack by default.\n\n---\n\n## Key Data Structures\n\n### Error Types\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum MsError {\n    // Storage errors\n    #[error(\"Database error: {0}\")]\n    Database(#[from] rusqlite::Error),\n    \n    #[error(\"Git error: {0}\")]\n    Git(#[from] git2::Error),\n    \n    #[error(\"IO error: {0}\")]\n    Io(#[from] std::io::Error),\n    \n    // Skill errors\n    #[error(\"Skill not found: {0}\")]\n    SkillNotFound(String),\n    \n    #[error(\"Invalid skill format: {0}\")]\n    InvalidSkill(String),\n    \n    #[error(\"Skill validation failed: {0}\")]\n    ValidationFailed(String),\n    \n    // Search errors\n    #[error(\"Search index error: {0}\")]\n    SearchIndex(#[from] tantivy::TantivyError),\n    \n    #[error(\"Query parse error: {0}\")]\n    QueryParse(String),\n    \n    // CASS errors\n    #[error(\"CASS not available: {0}\")]\n    CassUnavailable(String),\n    \n    #[error(\"Mining failed: {0}\")]\n    MiningFailed(String),\n    \n    // Config errors\n    #[error(\"Config error: {0}\")]\n    Config(String),\n    \n    #[error(\"Missing required config: {0}\")]\n    MissingConfig(String),\n    \n    // Transaction errors\n    #[error(\"Transaction failed: {0}\")]\n    TransactionFailed(String),\n    \n    #[error(\"Two-phase commit failed at {phase}: {reason}\")]\n    TwoPhaseCommitFailed { phase: String, reason: String },\n    \n    // Safety errors\n    #[error(\"Operation requires approval: {0}\")]\n    ApprovalRequired(String),\n    \n    #[error(\"Destructive operation blocked: {0}\")]\n    DestructiveBlocked(String),\n}\n\npub type Result\u003cT\u003e = std::result::Result\u003cT, MsError\u003e;\n```\n\n### CLI Structure\n\n```rust\nuse clap::{Parser, Subcommand};\n\n#[derive(Parser)]\n#[command(name = \"ms\")]\n#[command(author, version, about, long_about = None)]\n#[command(propagate_version = true)]\npub struct Cli {\n    /// Enable robot mode (JSON output)\n    #[arg(long, global = true)]\n    pub robot: bool,\n    \n    /// Increase verbosity (-v, -vv, -vvv)\n    #[arg(short, long, action = clap::ArgAction::Count, global = true)]\n    pub verbose: u8,\n    \n    /// Suppress all output except errors\n    #[arg(short, long, global = true)]\n    pub quiet: bool,\n    \n    /// Config file path (default: ~/.config/ms/config.toml)\n    #[arg(long, global = true)]\n    pub config: Option\u003cPathBuf\u003e,\n    \n    #[command(subcommand)]\n    pub command: Commands,\n}\n\n#[derive(Subcommand)]\npub enum Commands {\n    /// Initialize ms in current directory or globally\n    Init(InitArgs),\n    \n    /// Index skills from configured paths\n    Index(IndexArgs),\n    \n    /// Search for skills\n    Search(SearchArgs),\n    \n    /// Load a skill with progressive disclosure\n    Load(LoadArgs),\n    \n    /// Get context-aware skill suggestions\n    Suggest(SuggestArgs),\n    \n    /// Show skill details\n    Show(ShowArgs),\n    \n    /// Edit a skill (structured round-trip)\n    Edit(EditArgs),\n    \n    /// Format skill files\n    Fmt(FmtArgs),\n    \n    /// Semantic diff between skills\n    Diff(DiffArgs),\n    \n    /// Manage skill aliases\n    Alias(AliasArgs),\n    \n    /// Check environment requirements\n    Requirements(RequirementsArgs),\n    \n    /// Build skills from CASS sessions\n    Build(BuildArgs),\n    \n    /// Manage skill bundles\n    Bundle(BundleArgs),\n    \n    /// Check for and apply updates\n    Update(UpdateArgs),\n    \n    /// Health checks and repairs\n    Doctor(DoctorArgs),\n    \n    /// Prune tombstoned/outdated data\n    Prune(PruneArgs),\n    \n    /// Manage configuration\n    Config(ConfigArgs),\n}\n```\n\n### Application Context\n\n```rust\nuse std::path::PathBuf;\nuse std::sync::Arc;\n\n/// Global application context shared across commands\npub struct AppContext {\n    /// Path to ms root directory (~/.local/share/ms or .ms/)\n    pub ms_root: PathBuf,\n    \n    /// Path to config file\n    pub config_path: PathBuf,\n    \n    /// Loaded configuration\n    pub config: Config,\n    \n    /// Database connection (lazy initialized)\n    pub db: Arc\u003cDatabase\u003e,\n    \n    /// Git archive (lazy initialized)\n    pub git: Arc\u003cGitArchive\u003e,\n    \n    /// Search index (lazy initialized)\n    pub search: Arc\u003cSearchIndex\u003e,\n    \n    /// Robot mode flag\n    pub robot_mode: bool,\n    \n    /// Verbosity level\n    pub verbosity: u8,\n}\n\nimpl AppContext {\n    /// Create context from CLI args\n    pub fn from_cli(cli: \u0026Cli) -\u003e Result\u003cSelf\u003e {\n        let ms_root = Self::find_ms_root()?;\n        let config_path = cli.config.clone()\n            .unwrap_or_else(|| dirs::config_dir().unwrap().join(\"ms/config.toml\"));\n        let config = Config::load(\u0026config_path)?;\n        \n        Ok(Self {\n            ms_root,\n            config_path,\n            config,\n            db: Arc::new(Database::open(ms_root.join(\"ms.db\"))?),\n            git: Arc::new(GitArchive::open(ms_root.join(\"archive\"))?),\n            search: Arc::new(SearchIndex::open(ms_root.join(\"index\"))?),\n            robot_mode: cli.robot,\n            verbosity: cli.verbose,\n        })\n    }\n    \n    /// Find ms root (search up from cwd for .ms/, fallback to ~/.local/share/ms)\n    fn find_ms_root() -\u003e Result\u003cPathBuf\u003e {\n        // Implementation\n        unimplemented!()\n    }\n}\n```\n\n---\n\n## Cargo.toml Template\n\n```toml\n[package]\nname = \"ms\"\nversion = \"0.1.0\"\nedition = \"2021\"\nauthors = [\"Your Name \u003cyou@example.com\u003e\"]\ndescription = \"Meta Skill - Mine CASS sessions to generate Claude Code skills\"\nlicense = \"MIT\"\nrepository = \"https://github.com/your-org/meta_skill\"\nkeywords = [\"cli\", \"skills\", \"ai\", \"claude\"]\ncategories = [\"command-line-utilities\", \"development-tools\"]\n\n[dependencies]\n# CLI\nclap = { version = \"4\", features = [\"derive\", \"env\"] }\n\n# Async runtime\ntokio = { version = \"1\", features = [\"full\"] }\n\n# Error handling\nthiserror = \"1\"\nanyhow = \"1\"\n\n# Serialization\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\nserde_yaml = \"0.9\"\ntoml = \"0.8\"\n\n# Database\nrusqlite = { version = \"0.31\", features = [\"bundled\", \"blob\", \"backup\", \"functions\"] }\n\n# Search\ntantivy = \"0.22\"\n\n# Git\ngit2 = \"0.19\"\n\n# Logging\ntracing = \"0.1\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\", \"json\"] }\n\n# Utilities\nchrono = { version = \"0.4\", features = [\"serde\"] }\nuuid = { version = \"1\", features = [\"v4\"] }\ndirs = \"5\"\nwalkdir = \"2\"\nglob = \"0.3\"\nregex = \"1\"\nsha2 = \"0.10\"\nhex = \"0.4\"\n\n[dev-dependencies]\ntempfile = \"3\"\nassert_cmd = \"2\"\npredicates = \"3\"\ninsta = \"1\"\n\n[profile.release]\nlto = true\ncodegen-units = 1\nstrip = true\n\n[[bin]]\nname = \"ms\"\npath = \"src/main.rs\"\n```\n\n---\n\n## Tasks\n\n### Task 1: Create Project Structure\n- [ ] Run `cargo new ms` \n- [ ] Create all directory structure from layout above\n- [ ] Create placeholder mod.rs files in each directory\n- [ ] Verify project compiles with `cargo check`\n\n### Task 2: Configure Cargo.toml\n- [ ] Add all dependencies from template above\n- [ ] Configure features (rusqlite bundled, etc.)\n- [ ] Set up dev-dependencies for testing\n- [ ] Configure release profile optimizations\n\n### Task 3: Implement Error Types\n- [ ] Create src/error.rs with MsError enum\n- [ ] Add all error variants listed above\n- [ ] Implement From traits for automatic conversion\n- [ ] Export from lib.rs as `pub use error::{MsError, Result}`\n\n### Task 4: Set Up CLI Framework\n- [ ] Create src/cli/mod.rs\n- [ ] Define Cli struct with clap derive\n- [ ] Define Commands enum with all subcommands\n- [ ] Create arg structs for each command (initially empty)\n- [ ] Wire up to main.rs\n\n### Task 5: Implement Logging\n- [ ] Set up tracing-subscriber in main.rs\n- [ ] Configure env-filter for log levels\n- [ ] Support JSON output for robot mode\n- [ ] Add `#[instrument]` to key functions\n\n### Task 6: Create AppContext\n- [ ] Implement AppContext struct\n- [ ] Add find_ms_root() logic\n- [ ] Lazy initialization for expensive resources\n- [ ] Pass context to all commands\n\n### Task 7: Stub All Modules\n- [ ] Create placeholder files for all modules\n- [ ] Add basic struct/function stubs\n- [ ] Ensure `cargo check` passes\n- [ ] Add TODO comments for implementation\n\n### Task 8: Set Up CI/CD\n- [ ] Create .github/workflows/ci.yml\n- [ ] Add cargo fmt check\n- [ ] Add cargo clippy\n- [ ] Add cargo test\n- [ ] Create release.yml for binary releases\n\n---\n\n## Acceptance Criteria\n\n1. **Project Compiles**: `cargo check` passes with no errors\n2. **CLI Parses**: `ms --help` shows all commands\n3. **Robot Flag**: `--robot` flag recognized globally\n4. **Verbose Flag**: `-v`, `-vv`, `-vvv` parsed correctly\n5. **Error Types**: MsError covers all failure modes\n6. **Logging Works**: Log output respects verbosity level\n7. **Module Structure**: All modules from layout created and re-exported\n8. **Tests Pass**: `cargo test` runs successfully\n\n---\n\n## Testing Requirements\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use assert_cmd::Command;\n    use predicates::prelude::*;\n\n    #[test]\n    fn test_cli_help() {\n        let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n        cmd.arg(\"--help\")\n            .assert()\n            .success()\n            .stdout(predicate::str::contains(\"Usage:\"));\n    }\n\n    #[test]\n    fn test_cli_version() {\n        let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n        cmd.arg(\"--version\")\n            .assert()\n            .success()\n            .stdout(predicate::str::contains(env!(\"CARGO_PKG_VERSION\")));\n    }\n\n    #[test]\n    fn test_robot_mode_global() {\n        let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n        cmd.args([\"--robot\", \"--help\"])\n            .assert()\n            .success();\n    }\n}\n```\n\n---\n\n## Logging Requirements\n\nAll operations must log:\n- **TRACE**: Function entry/exit, parameter values\n- **DEBUG**: Internal state, decision points\n- **INFO**: User-visible operations (indexing, loading)\n- **WARN**: Recoverable issues, deprecations\n- **ERROR**: Failures that stop the operation\n\n---\n\n## References\n\n- **Plan Section 2.3**: File Layout (Following xf Pattern)\n- **xf implementation**: /data/projects/xf/src/\n- **Blocks**: All other Phase 1 beads depend on this\n- **Phase**: P1 Foundation","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:21:58.323525006-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:57:36.598307945-05:00","labels":["phase-1","rust","setup"]}
{"id":"meta_skill-628","title":"[Cross-Cutting] CI/CD Pipeline","description":"# CI/CD Pipeline (Cross‑Cutting)\n\n## Overview\n\nProvide a deterministic, audited CI/CD pipeline for ms. This includes linting, tests, benchmarks, security audits, release packaging, and artifact signing. CI should be the *primary* enforcement point for quality gates defined across beads.\n\n---\n\n## Required Workflows\n\n1. **ci.yml**\n   - Lint: `cargo fmt --check`, `cargo clippy -- -D warnings`\n   - Unit/Integration: `cargo test --all-features`\n   - UBS gate: run `ubs` on changed files\n   - Security: `cargo audit` + RUSTSEC\n2. **e2e.yml**\n   - Run CLI flows with detailed logging\n   - Validate robot JSON outputs\n3. **bench.yml** (optional but recommended)\n   - Run Criterion benchmarks (search, pack, suggest)\n4. **release.yml**\n   - Build multi‑platform artifacts\n   - Generate checksums + signatures\n   - Create GitHub Release\n\n---\n\n## Tasks\n\n1. Add GitHub Actions workflow files (`ci.yml`, `e2e.yml`, `release.yml`).\n2. Cache cargo registry + target for speed.\n3. Upload test artifacts: logs, coverage, snapshots.\n4. Enforce UBS gate in CI (exit \u003e0 fails build).\n5. Enforce security audits (`cargo audit`).\n6. Add release signing + checksum verification.\n7. Add CI status badge to README.\n\n---\n\n## Testing \u0026 Logging Requirements\n\n- All test workflows must emit structured logs (timestamps + phase).\n- Upload `tests/logs/*.log` and `target/criterion/*` as artifacts.\n- Coverage report should be generated and stored per run.\n\n---\n\n## Acceptance Criteria\n\n- CI runs on every PR and push.\n- Failing UBS or security audit blocks merge.\n- Release pipeline produces signed artifacts.\n- E2E tests provide reproducible logs and screenshots (if any UI).\n\n---\n\n## Dependencies\n\n- `meta_skill-9ok` Testing Strategy\n- `meta_skill-7t2` Unit Test Infrastructure\n- `meta_skill-9pr` Integration Test Framework\n- `meta_skill-2kd` E2E Test Scripts\n- `meta_skill-ftb` Benchmark Tests\n- `meta_skill-27c` UBS Integration\n- `meta_skill-n9r` Security Hardening","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:29:08.211299891-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:45:54.120909123-05:00","labels":["automation","ci-cd","cross-cutting"],"dependencies":[{"issue_id":"meta_skill-628","depends_on_id":"meta_skill-9ok","type":"blocks","created_at":"2026-01-13T23:46:01.919417195-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-628","depends_on_id":"meta_skill-7t2","type":"blocks","created_at":"2026-01-13T23:46:09.995726184-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-628","depends_on_id":"meta_skill-9pr","type":"blocks","created_at":"2026-01-13T23:46:19.330939911-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-628","depends_on_id":"meta_skill-2kd","type":"blocks","created_at":"2026-01-13T23:46:27.798229493-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-628","depends_on_id":"meta_skill-ftb","type":"blocks","created_at":"2026-01-13T23:46:35.413922946-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-628","depends_on_id":"meta_skill-27c","type":"blocks","created_at":"2026-01-13T23:46:42.904787703-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-67m","title":"[P6] Shell Integration","description":"# Shell Integration\n\n## Overview\n\nIntegrate ms suggestions into the shell (zsh/bash/fish) via hooks. This provides context‑aware suggestions without manual CLI invocation.\n\n---\n\n## Tasks\n\n1. Provide shell hook scripts for supported shells.\n2. Hook into command execution to capture context.\n3. Rate‑limit suggestion prompts.\n4. Respect cooldowns + context fingerprinting.\n\n---\n\n## Testing Requirements\n\n- Integration tests for hook installation.\n- E2E: simulated shell session with suggestions.\n\n---\n\n## Acceptance Criteria\n\n- Hooks are safe and removable.\n- Suggestions appear only when relevant.\n- Shell performance impact \u003c5ms per command.\n\n---\n\n## Dependencies\n\n- `meta_skill-o8o` Context‑Aware Suggestions\n- `meta_skill-8df` Context Fingerprints \u0026 Cooldowns","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:21.298706295-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:11:18.172070894-05:00","labels":["completions","phase-6","shell"],"dependencies":[{"issue_id":"meta_skill-67m","depends_on_id":"meta_skill-14h","type":"blocks","created_at":"2026-01-13T22:28:36.922166232-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-67m","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-14T00:11:26.582268104-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-67m","depends_on_id":"meta_skill-8df","type":"blocks","created_at":"2026-01-14T00:11:35.475838142-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-6fi","title":"[P5] Bundle Format and Manifest","description":"# Bundle Format and Manifest\n\n## Overview\n\nDefine the portable bundle format used to distribute skills. Bundles must be deterministic, signed, and support incremental updates.\n\n---\n\n## Key Requirements\n\n- Manifest includes skills, versions, hashes, and dependencies.\n- Content‑addressed blobs for integrity and dedupe.\n- Signature verification on install/update.\n- Lockfile for reproducible installs.\n\n---\n\n## Tasks\n\n1. Define manifest schema (JSON/TOML).\n2. Implement blob store + hash verification.\n3. Add signature validation hooks.\n4. Support delta updates (missing blobs only).\n\n---\n\n## Testing Requirements\n\n- Unit tests for manifest parsing.\n- Integration tests: pack → verify → install.\n- Tamper tests: invalid signature must fail.\n\n---\n\n## Acceptance Criteria\n\n- Bundles deterministic across builds.\n- Signature checks enforced by default.\n- Delta updates work without full re‑download.\n\n---\n\n## Dependencies\n\n- `meta_skill-qs1` SQLite Database Layer\n- `meta_skill-b98` Git Archive Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:03.231665909-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:06:06.591048976-05:00","labels":["bundles","format","phase-5"],"dependencies":[{"issue_id":"meta_skill-6fi","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:27:15.347650553-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-6fi","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:06:15.156011245-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-6hm","title":"Phase 1: Foundation (Core Infrastructure)","description":"# Epic: Phase 1 Foundation (Core Infrastructure)\n\n## Goal\n\nDeliver the core persistence + data model foundation so all higher phases can build safely. This phase establishes the dual‑persistence architecture, locking, and CLI scaffolding.\n\n---\n\n## Scope\n\n- Rust project scaffolding\n- SQLite layer + migrations\n- Git archive layer\n- Two‑phase commit + global locking\n- SkillSpec data model\n- Core CLI commands + robot mode\n\n---\n\n## Acceptance Criteria\n\n- ms can `init`, `index`, `list`, `show` against local skill dirs.\n- Dual persistence is crash‑safe (2PC + lock).\n- SkillSpec compiles deterministically.\n\n---\n\n## Child Beads\n\n- `meta_skill-5s0` Rust Project Scaffolding\n- `meta_skill-qs1` SQLite Database Layer\n- `meta_skill-b98` Git Archive Layer\n- `meta_skill-fus` Two‑Phase Commit\n- `meta_skill-igx` Global File Locking\n- `meta_skill-ik6` SkillSpec Data Model\n- `meta_skill-14h` CLI Commands (init/index/list/show)\n- `meta_skill-vqr` Robot Mode Infrastructure","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:51.975096697-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:13:09.314968743-05:00"}
{"id":"meta_skill-6st","title":"CASS Mining: REST API Design Patterns","description":"Deep dive into command-to-endpoint mapping, OpenAPI specs, acceptance criteria patterns, API versioning strategies.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:42.136787849-05:00","created_by":"ubuntu","updated_at":"2026-01-13T21:53:49.449202641-05:00","closed_at":"2026-01-13T21:53:49.449202641-05:00","close_reason":"Section 39 added: REST API Design Patterns covering Zod schemas, OpenAPI generation, error taxonomies, auth patterns, cursor pagination, idempotency middleware","labels":["cass-mining"]}
{"id":"meta_skill-7b9","title":"[P5] One-URL Sharing","description":"# One‑URL Sharing\n\n## Overview\n\nEnable sharing a skill bundle via a single URL (download + verify + install). This is the fast path for human‑to‑human sharing.\n\n---\n\n## Tasks\n\n1. Implement short URL generation for bundles.\n2. Host bundle artifact with checksum + signature.\n3. Add `ms install \u003curl\u003e` workflow.\n4. Cache downloaded bundles for reuse.\n\n---\n\n## Testing Requirements\n\n- Integration tests: share → install → verify.\n- Failure tests: invalid URL or signature.\n\n---\n\n## Acceptance Criteria\n\n- URL installs bundle in one command.\n- Signature verification enforced.\n\n---\n\n## Dependencies\n\n- `meta_skill-6fi` Bundle Format and Manifest\n- `meta_skill-08m` GitHub Integration (or hosting)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:27:06.627988316-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:10:15.356714925-05:00","labels":["phase-5","sharing","url"],"dependencies":[{"issue_id":"meta_skill-7b9","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.484259335-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7b9","depends_on_id":"meta_skill-08m","type":"blocks","created_at":"2026-01-13T22:27:15.511781593-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7dg","title":"[P5] ms bundle Command","description":"# ms bundle Command\n\n## Overview\n\nCLI for creating, installing, and inspecting bundles. Must validate manifests, signatures, and apply safe merge semantics.\n\n---\n\n## Tasks\n\n1. Implement `ms bundle create` (pack skills + manifest).\n2. Implement `ms bundle install` (verify + import).\n3. Implement `ms bundle list/show`.\n4. Support `--verify` and `--no-verify` flags.\n\n---\n\n## Testing Requirements\n\n- Integration tests: create → install → verify.\n- Tamper tests: invalid checksum/signature fails.\n- Snapshot tests: human output formatting.\n\n---\n\n## Acceptance Criteria\n\n- Bundles install deterministically.\n- Verification enforced by default.\n- Import respects local modification safety.\n\n---\n\n## Dependencies\n\n- `meta_skill-6fi` Bundle Format and Manifest\n- `meta_skill-swe` Local Modification Safety","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:08.032768966-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:06:47.832626426-05:00","labels":["bundles","cli","phase-5"],"dependencies":[{"issue_id":"meta_skill-7dg","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.565726317-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7dg","depends_on_id":"meta_skill-08m","type":"blocks","created_at":"2026-01-13T22:27:15.594783428-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7dg","depends_on_id":"meta_skill-swe","type":"blocks","created_at":"2026-01-13T22:27:15.623131092-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7t2","title":"Unit Test Infrastructure","description":"## Overview\n\nEstablish comprehensive unit test infrastructure for the meta_skill CLI using table-driven tests and property-based tests with proptest. This bead implements Section 18.2 of the Testing Strategy with NO MOCKS - all tests use real implementations with real data fixtures.\n\n## Requirements\n\n### 1. Table-Driven Test Framework\n\nCreate a test utilities module at `src/test_utils/mod.rs`:\n\n```rust\n/// Table-driven test case structure\npub struct TestCase\u003cI, E\u003e {\n    pub name: \u0026'static str,\n    pub input: I,\n    pub expected: E,\n    pub should_panic: bool,\n}\n\n/// Run table-driven tests with detailed logging\npub fn run_table_tests\u003cI, E, F\u003e(cases: Vec\u003cTestCase\u003cI, E\u003e\u003e, test_fn: F)\nwhere\n    I: std::fmt::Debug + Clone,\n    E: std::fmt::Debug + PartialEq,\n    F: Fn(I) -\u003e E,\n{\n    for case in cases {\n        let start = std::time::Instant::now();\n        println!(\"[TEST] Running: {}\", case.name);\n        println!(\"[TEST] Input: {:?}\", case.input);\n        \n        let result = test_fn(case.input.clone());\n        let elapsed = start.elapsed();\n        \n        println!(\"[TEST] Expected: {:?}\", case.expected);\n        println!(\"[TEST] Actual: {:?}\", result);\n        println!(\"[TEST] Timing: {:?}\", elapsed);\n        \n        assert_eq!(result, case.expected, \"Test '{}' failed\", case.name);\n        println!(\"[TEST] PASSED: {} ({:?})\\n\", case.name, elapsed);\n    }\n}\n```\n\n### 2. Property-Based Tests with proptest\n\nAdd to Cargo.toml:\n```toml\n[dev-dependencies]\nproptest = \"1.4\"\nproptest-derive = \"0.4\"\n```\n\nProperty categories to test:\n\n#### 2.1 Idempotence (Serialize/Deserialize Roundtrip)\n```rust\nproptest! {\n    #[test]\n    fn test_skill_spec_roundtrip(skill in arb_skill_spec()) {\n        let serialized = serde_json::to_string(\u0026skill)?;\n        let deserialized: SkillSpec = serde_json::from_str(\u0026serialized)?;\n        prop_assert_eq!(skill, deserialized);\n    }\n    \n    #[test]\n    fn test_config_roundtrip(config in arb_config()) {\n        let toml_str = toml::to_string(\u0026config)?;\n        let parsed: Config = toml::from_str(\u0026toml_str)?;\n        prop_assert_eq!(config, parsed);\n    }\n}\n```\n\n#### 2.2 Determinism (Same Input = Same Output)\n```rust\n#[test]\nfn test_fnv1a_deterministic() {\n    let embeddings: Vec\u003c_\u003e = (0..100).map(|_| hash_embedding(\"test\")).collect();\n    assert!(embeddings.windows(2).all(|w| w[0] == w[1]));\n}\n\nproptest! {\n    #[test]\n    fn test_skill_id_generation_unique(name in \"[a-z]{3,20}\", desc in \".{0,100}\") {\n        let id1 = generate_skill_id(\u0026name, \u0026desc);\n        let id2 = generate_skill_id(\u0026name, \u0026desc);\n        prop_assert_eq!(id1, id2);\n    }\n    \n    #[test]\n    fn test_hash_embedding_deterministic(text in \".*\") {\n        let hash1 = hash_embedding(\u0026text);\n        let hash2 = hash_embedding(\u0026text);\n        prop_assert_eq!(hash1, hash2);\n    }\n}\n```\n\n#### 2.3 Safety (Never Panic on Arbitrary Input)\n```rust\nproptest! {\n    #[test]\n    fn test_parser_never_panics(input in \".*\") {\n        // Should not panic, may return error\n        let _ = parse_skill_content(\u0026input);\n    }\n    \n    #[test]\n    fn test_search_query_never_panics(query in \".*\") {\n        let _ = parse_search_query(\u0026query);\n    }\n    \n    #[test]\n    fn test_validator_never_panics(arbitrary in any::\u003cVec\u003cu8\u003e\u003e()) {\n        let input = String::from_utf8_lossy(\u0026arbitrary);\n        let _ = validate_skill_name(\u0026input);\n    }\n}\n```\n\n### 3. Test Fixture System\n\nCreate `src/test_utils/fixtures.rs`:\n\n```rust\nuse std::path::PathBuf;\nuse tempfile::TempDir;\n\n/// Test fixture providing isolated filesystem environment\npub struct UnitTestFixture {\n    pub temp_dir: TempDir,\n    pub data_path: PathBuf,\n}\n\nimpl UnitTestFixture {\n    pub fn new() -\u003e Self {\n        let temp_dir = TempDir::new().expect(\"Failed to create temp dir\");\n        let data_path = temp_dir.path().to_path_buf();\n        \n        println!(\"[FIXTURE] Created temp directory: {:?}\", data_path);\n        \n        Self { temp_dir, data_path }\n    }\n    \n    /// Create a test file with content\n    pub fn create_file(\u0026self, relative_path: \u0026str, content: \u0026str) -\u003e PathBuf {\n        let full_path = self.data_path.join(relative_path);\n        if let Some(parent) = full_path.parent() {\n            std::fs::create_dir_all(parent).expect(\"Failed to create parent dirs\");\n        }\n        std::fs::write(\u0026full_path, content).expect(\"Failed to write file\");\n        println!(\"[FIXTURE] Created file: {:?} ({} bytes)\", full_path, content.len());\n        full_path\n    }\n    \n    /// Create a test skill file\n    pub fn create_skill(\u0026self, name: \u0026str, content: \u0026str) -\u003e PathBuf {\n        self.create_file(\u0026format!(\"skills/{}/SKILL.md\", name), content)\n    }\n}\n\nimpl Drop for UnitTestFixture {\n    fn drop(\u0026mut self) {\n        println!(\"[FIXTURE] Cleaning up temp directory: {:?}\", self.data_path);\n    }\n}\n```\n\n### 4. Arbitrary Generators for proptest\n\nCreate `src/test_utils/arbitrary.rs`:\n\n```rust\nuse proptest::prelude::*;\n\n/// Generate arbitrary SkillSpec\npub fn arb_skill_spec() -\u003e impl Strategy\u003cValue = SkillSpec\u003e {\n    (\n        \"[a-z][a-z0-9_]{2,30}\",           // name\n        \".{10,200}\",                       // description\n        prop::collection::vec(\".{5,50}\", 0..5),  // tags\n        prop::option::of(\".{10,500}\"),    // content\n    ).prop_map(|(name, description, tags, content)| {\n        SkillSpec {\n            name,\n            description,\n            tags,\n            content,\n            ..Default::default()\n        }\n    })\n}\n\n/// Generate arbitrary Config\npub fn arb_config() -\u003e impl Strategy\u003cValue = Config\u003e {\n    (\n        any::\u003cbool\u003e(),                     // auto_index\n        1usize..100,                       // max_results\n        prop::option::of(\"[a-z]{3,10}\"),  // default_bundle\n    ).prop_map(|(auto_index, max_results, default_bundle)| {\n        Config {\n            auto_index,\n            max_results,\n            default_bundle,\n            ..Default::default()\n        }\n    })\n}\n\n/// Generate arbitrary search query\npub fn arb_search_query() -\u003e impl Strategy\u003cValue = String\u003e {\n    prop::string::string_regex(\"[a-zA-Z0-9 ]{1,100}\")\n        .unwrap()\n}\n```\n\n### 5. Detailed Logging Requirements\n\nEvery test must log:\n- **Test name**: Clear identifier\n- **Inputs**: All input values in debug format\n- **Expected output**: What the test expects\n- **Actual output**: What was actually produced\n- **Timing**: Duration of test execution\n- **Pass/Fail status**: Clear indication\n\nCreate logging helper at `src/test_utils/logging.rs`:\n\n```rust\nuse std::time::Instant;\n\npub struct TestLogger {\n    test_name: String,\n    start_time: Instant,\n}\n\nimpl TestLogger {\n    pub fn new(test_name: \u0026str) -\u003e Self {\n        println!(\"\\n{'='.repeat(60)}\");\n        println!(\"[TEST START] {}\", test_name);\n        println!(\"{'='.repeat(60)}\");\n        Self {\n            test_name: test_name.to_string(),\n            start_time: Instant::now(),\n        }\n    }\n    \n    pub fn log_input\u003cT: std::fmt::Debug\u003e(\u0026self, name: \u0026str, value: \u0026T) {\n        println!(\"[INPUT] {}: {:?}\", name, value);\n    }\n    \n    pub fn log_expected\u003cT: std::fmt::Debug\u003e(\u0026self, value: \u0026T) {\n        println!(\"[EXPECTED] {:?}\", value);\n    }\n    \n    pub fn log_actual\u003cT: std::fmt::Debug\u003e(\u0026self, value: \u0026T) {\n        println!(\"[ACTUAL] {:?}\", value);\n    }\n    \n    pub fn pass(\u0026self) {\n        let elapsed = self.start_time.elapsed();\n        println!(\"[RESULT] PASSED in {:?}\", elapsed);\n        println!(\"{'='.repeat(60)}\\n\");\n    }\n    \n    pub fn fail(\u0026self, reason: \u0026str) {\n        let elapsed = self.start_time.elapsed();\n        println!(\"[RESULT] FAILED in {:?}\", elapsed);\n        println!(\"[REASON] {}\", reason);\n        println!(\"{'='.repeat(60)}\\n\");\n    }\n}\n```\n\n### 6. Coverage Requirements\n\nTarget: \u003e80% coverage for core modules\n\nModules requiring full coverage:\n- `src/core/skill.rs` - SkillSpec parsing/validation\n- `src/core/config.rs` - Configuration loading/saving\n- `src/search/hash_embed.rs` - Hash embedding generation\n- `src/search/bm25.rs` - BM25 scoring\n- `src/search/rrf.rs` - RRF fusion\n- `src/db/sqlite.rs` - Database operations\n- `src/parser/skill_md.rs` - SKILL.md parsing\n\nAdd coverage configuration to `.cargo/config.toml`:\n```toml\n[env]\nCARGO_INCREMENTAL = \"0\"\nRUSTFLAGS = \"-Cinstrument-coverage\"\nLLVM_PROFILE_FILE = \"coverage/default-%p-%m.profraw\"\n```\n\n### 7. Test Organization\n\n```\ntests/\n├── unit/\n│   ├── mod.rs\n│   ├── skill_spec_tests.rs\n│   ├── config_tests.rs\n│   ├── parser_tests.rs\n│   ├── hash_embed_tests.rs\n│   ├── bm25_tests.rs\n│   └── rrf_tests.rs\n├── properties/\n│   ├── mod.rs\n│   ├── roundtrip_tests.rs\n│   ├── determinism_tests.rs\n│   └── safety_tests.rs\n└── fixtures/\n    ├── skills/\n    │   ├── valid_minimal.md\n    │   ├── valid_full.md\n    │   └── invalid_*.md\n    └── configs/\n        ├── default.toml\n        └── custom.toml\n```\n\n## Acceptance Criteria\n\n1. [ ] Table-driven test framework implemented with detailed logging\n2. [ ] Property-based tests for idempotence, determinism, safety\n3. [ ] Test fixture system with temp directory management\n4. [ ] Arbitrary generators for all core types\n5. [ ] All parsers have table-driven tests\n6. [ ] All serializers have roundtrip property tests\n7. [ ] All validators have safety property tests\n8. [ ] Coverage \u003e80% for core modules\n9. [ ] Tests run in CI with coverage reporting\n10. [ ] Test output includes timing for all tests\n\n## Dependencies\n\n- meta_skill-5s0 (Rust Project Scaffolding) - provides project structure","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T22:51:49.869175982-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.804171956-05:00","labels":["infrastructure","testing","unit-tests"],"dependencies":[{"issue_id":"meta_skill-7t2","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:54:23.059055914-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7t2","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.101334497-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7va","title":"[P3] ms load Command","description":"# ms load Command\n\n## Overview\n\nLoad skills into context with progressive disclosure or token‑budgeted packing. This is the main entry point for injecting knowledge into agents and must honor predicates, overlays, dependencies, and policy slices.\n\n---\n\n## Core Behaviors\n\n- **Disclosure Levels**: minimal → overview → standard → full → complete.\n- **Token Packing**: constrained packer with coverage + novelty constraints.\n- **Predicates**: include/exclude slices based on context.\n- **Overlays**: apply block‑level overrides before rendering.\n- **Dependencies**: auto‑load prerequisites at configurable levels.\n- **Meta‑skills**: allow composed bundles to load as a single unit.\n\n---\n\n## Tasks\n\n1. Implement load pipeline: resolve → overlay → predicate filter → pack → render.\n2. Support `--pack`, `--level`, `--explain`, `--deps` flags.\n3. Emit robot output schema with slices + tokens + provenance hints.\n4. Provide `--preview` to show what would load (no side effects).\n\n---\n\n## Testing Requirements\n\n- Unit tests: predicate filtering, overlay merge, dependency resolution ordering.\n- Integration tests: load with pack constraints + contracts.\n- Snapshot tests: human output formatting.\n- E2E: `ms load` on fixture skills with logging.\n\n---\n\n## Acceptance Criteria\n\n- Output respects token budget and mandatory policy slices.\n- Dependencies load deterministically in correct order.\n- Predicates correctly include/exclude slices.\n- Robot output valid and stable.\n\n---\n\n## Dependencies\n\n- `meta_skill-9ik` Token Packer\n- `meta_skill-cn4` Block‑Level Overlays\n- `meta_skill-jka` Dependency Graph Resolution\n- `meta_skill-1jl` Conditional Predicates\n- `meta_skill-7ws` Meta‑Skills","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:17.798003642-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:49:17.065189036-05:00","labels":["cli","load","phase-3"],"dependencies":[{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-9ik","type":"blocks","created_at":"2026-01-13T22:24:26.008014903-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-1jl","type":"blocks","created_at":"2026-01-13T22:24:26.034413754-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-jka","type":"blocks","created_at":"2026-01-13T22:54:02.861277974-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-cn4","type":"blocks","created_at":"2026-01-13T22:54:05.781296512-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-7ws","type":"blocks","created_at":"2026-01-13T23:00:21.792544553-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7ws","title":"Meta-Skills (Composed Slice Bundles)","description":"# Meta-Skills: First-Class Composed Slice Bundles\n\n## Overview\n\nMeta-skills are a powerful abstraction that allows users to load curated combinations of slices from multiple skills as cohesive \"task kits.\" Rather than manually loading individual slices from various skills, a meta-skill bundles them together with intelligent defaults and optimal packing strategies.\n\n**Example Use Case:**\n```bash\nms load frontend-polish\n```\nThis single command loads slices from:\n- `nextjs-ui` (component patterns, routing, SSR)\n- `a11y` (accessibility guidelines, ARIA patterns)\n- `react-patterns` (hooks, state management, performance)\n\nAll optimally packed to fit within context budget while maximizing task relevance.\n\n## Background \u0026 Rationale\n\n### Problem Statement\n\nAs the skill ecosystem grows, users face cognitive overhead in:\n1. Discovering which skills contain relevant slices for their task\n2. Manually loading multiple slices from different skills\n3. Managing context budget when combining slices\n4. Ensuring slice compatibility and avoiding redundancy\n\n### Solution: Meta-Skills\n\nMeta-skills solve this by providing:\n- **Curated Bundles**: Expert-composed combinations for common workflows\n- **Automatic Packing**: Intelligent fitting within context constraints\n- **Version Coherence**: Ensuring slice versions work together\n- **Task Optimization**: Slices ordered/filtered by task relevance\n\n### Relationship to Plan Section 6.6\n\nThis implements the \"Meta-skills\" concept from Section 6.6, which describes:\n\u003e \"Meta-skills are first-class compositions that combine slices from multiple skills into task kits.\"\n\n## Core Data Structures\n\n### MetaSkill Struct\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// A meta-skill is a curated bundle of slices from one or more skills,\n/// designed for a specific workflow or task type.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MetaSkill {\n    /// Unique identifier (e.g., \"frontend-polish\", \"rust-safety\")\n    pub id: String,\n    \n    /// Human-readable name for display\n    pub name: String,\n    \n    /// Detailed description of what this meta-skill provides\n    pub description: String,\n    \n    /// Ordered list of slice references from various skills\n    /// Order matters: earlier slices have higher priority for packing\n    pub slices: Vec\u003cMetaSkillSliceRef\u003e,\n    \n    /// Strategy for resolving skill versions\n    pub pin_strategy: PinStrategy,\n    \n    /// Optional metadata for categorization and search\n    pub metadata: MetaSkillMetadata,\n    \n    /// Minimum context budget required (in tokens)\n    pub min_context_tokens: usize,\n    \n    /// Recommended context budget for full experience\n    pub recommended_context_tokens: usize,\n}\n\n/// Metadata for meta-skill discovery and categorization\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct MetaSkillMetadata {\n    /// Author or maintainer\n    pub author: Option\u003cString\u003e,\n    \n    /// Semantic version of this meta-skill definition\n    pub version: String,\n    \n    /// Tags for search/filtering\n    pub tags: Vec\u003cString\u003e,\n    \n    /// Tech stacks this meta-skill is designed for\n    pub tech_stacks: Vec\u003cString\u003e,\n    \n    /// When this meta-skill was last updated\n    pub updated_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n```\n\n### MetaSkillSliceRef Struct\n\n```rust\n/// A reference to one or more slices within a specific skill\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MetaSkillSliceRef {\n    /// The skill ID to pull slices from (e.g., \"nextjs-ui\")\n    pub skill_id: String,\n    \n    /// Specific slice IDs to include from this skill\n    /// If empty, includes all slices (filtered by level)\n    pub slice_ids: Vec\u003cString\u003e,\n    \n    /// Override disclosure level for these slices\n    /// None means use the slice's default level\n    pub level: Option\u003cDisclosureLevel\u003e,\n    \n    /// Priority weight for packing decisions (higher = more important)\n    pub priority: u8,\n    \n    /// Whether this slice group is required or optional\n    pub required: bool,\n    \n    /// Conditions under which to include these slices\n    pub conditions: Vec\u003cSliceCondition\u003e,\n}\n\n/// Disclosure level for progressive disclosure\n#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]\npub enum DisclosureLevel {\n    /// Always visible, essential information\n    Core,\n    /// Shown when user asks for more detail\n    Extended,\n    /// Deep-dive information, rarely needed\n    Deep,\n}\n\n/// Conditions for conditional slice inclusion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SliceCondition {\n    /// Include only if tech stack matches\n    TechStack(String),\n    /// Include only if file pattern exists in project\n    FileExists(String),\n    /// Include only if environment variable is set\n    EnvVar(String),\n    /// Include only if another slice is included\n    DependsOn { skill_id: String, slice_id: String },\n}\n```\n\n### PinStrategy Enum\n\n```rust\n/// Strategy for resolving skill versions when loading meta-skills\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]\npub enum PinStrategy {\n    /// Always use the latest version compatible with other constraints\n    /// This is the default for most meta-skills\n    LatestCompatible,\n    \n    /// Pin to an exact version string (e.g., \"1.2.3\")\n    /// Use when reproducibility is critical\n    ExactVersion(String),\n    \n    /// Allow floating within a major version (e.g., \"1.x\")\n    /// Balances stability with updates\n    FloatingMajor,\n    \n    /// Use whatever version is currently installed locally\n    /// Fastest but least predictable\n    LocalInstalled,\n    \n    /// Custom version constraints per skill\n    PerSkill(HashMap\u003cString, String\u003e),\n}\n\nimpl Default for PinStrategy {\n    fn default() -\u003e Self {\n        PinStrategy::LatestCompatible\n    }\n}\n```\n\n## MetaSkill Manager Implementation\n\n```rust\nuse std::path::PathBuf;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n/// Manages meta-skill definitions, resolution, and loading\npub struct MetaSkillManager {\n    /// Registry of available meta-skills\n    registry: Arc\u003cRwLock\u003cMetaSkillRegistry\u003e\u003e,\n    \n    /// Skill loader for resolving individual skills\n    skill_loader: Arc\u003cSkillLoader\u003e,\n    \n    /// Context budget manager\n    context_budget: Arc\u003cContextBudget\u003e,\n    \n    /// Cache for resolved meta-skills\n    resolution_cache: Arc\u003cRwLock\u003cResolutionCache\u003e\u003e,\n    \n    /// Logger for detailed tracing\n    logger: Arc\u003cdyn MetaSkillLogger\u003e,\n}\n\nimpl MetaSkillManager {\n    /// Load a meta-skill by ID, resolving all slice references\n    pub async fn load(\u0026self, meta_skill_id: \u0026str) -\u003e Result\u003cLoadedMetaSkill, MetaSkillError\u003e {\n        self.logger.log_load_start(meta_skill_id);\n        \n        // 1. Look up the meta-skill definition\n        let meta_skill = self.registry.read().await\n            .get(meta_skill_id)\n            .ok_or_else(|| MetaSkillError::NotFound(meta_skill_id.to_string()))?\n            .clone();\n        \n        self.logger.log_meta_skill_found(\u0026meta_skill);\n        \n        // 2. Resolve all skill versions according to pin strategy\n        let resolved_skills = self.resolve_skills(\u0026meta_skill).await?;\n        \n        // 3. Collect all referenced slices\n        let mut slices = Vec::new();\n        for slice_ref in \u0026meta_skill.slices {\n            let skill = resolved_skills.get(\u0026slice_ref.skill_id)\n                .ok_or_else(|| MetaSkillError::SkillNotResolved(slice_ref.skill_id.clone()))?;\n            \n            // Check conditions\n            if !self.evaluate_conditions(\u0026slice_ref.conditions).await? {\n                self.logger.log_slice_skipped(\u0026slice_ref, \"conditions not met\");\n                continue;\n            }\n            \n            // Resolve slice IDs (empty means all)\n            let slice_ids = if slice_ref.slice_ids.is_empty() {\n                skill.all_slice_ids()\n            } else {\n                slice_ref.slice_ids.clone()\n            };\n            \n            for slice_id in slice_ids {\n                if let Some(slice) = skill.get_slice(\u0026slice_id) {\n                    slices.push(ResolvedSlice {\n                        skill_id: slice_ref.skill_id.clone(),\n                        slice: slice.clone(),\n                        level: slice_ref.level,\n                        priority: slice_ref.priority,\n                        required: slice_ref.required,\n                    });\n                }\n            }\n        }\n        \n        self.logger.log_slices_collected(slices.len());\n        \n        // 4. Pack slices into context budget\n        let packed = self.pack_slices(slices, \u0026meta_skill).await?;\n        \n        self.logger.log_load_complete(meta_skill_id, \u0026packed);\n        \n        Ok(LoadedMetaSkill {\n            meta_skill,\n            resolved_skills,\n            packed_slices: packed,\n            loaded_at: chrono::Utc::now(),\n        })\n    }\n    \n    /// Resolve all skill versions according to the pin strategy\n    async fn resolve_skills(\n        \u0026self,\n        meta_skill: \u0026MetaSkill,\n    ) -\u003e Result\u003cHashMap\u003cString, Arc\u003cSkill\u003e\u003e, MetaSkillError\u003e {\n        let mut resolved = HashMap::new();\n        \n        for slice_ref in \u0026meta_skill.slices {\n            if resolved.contains_key(\u0026slice_ref.skill_id) {\n                continue;\n            }\n            \n            let version = match \u0026meta_skill.pin_strategy {\n                PinStrategy::LatestCompatible =\u003e {\n                    self.skill_loader.resolve_latest(\u0026slice_ref.skill_id).await?\n                }\n                PinStrategy::ExactVersion(v) =\u003e v.clone(),\n                PinStrategy::FloatingMajor =\u003e {\n                    self.skill_loader.resolve_floating_major(\u0026slice_ref.skill_id).await?\n                }\n                PinStrategy::LocalInstalled =\u003e {\n                    self.skill_loader.get_local_version(\u0026slice_ref.skill_id).await?\n                }\n                PinStrategy::PerSkill(versions) =\u003e {\n                    versions.get(\u0026slice_ref.skill_id)\n                        .cloned()\n                        .unwrap_or_else(|| \"latest\".to_string())\n                }\n            };\n            \n            let skill = self.skill_loader.load(\u0026slice_ref.skill_id, \u0026version).await?;\n            resolved.insert(slice_ref.skill_id.clone(), skill);\n        }\n        \n        Ok(resolved)\n    }\n    \n    /// Pack slices into the available context budget\n    async fn pack_slices(\n        \u0026self,\n        slices: Vec\u003cResolvedSlice\u003e,\n        meta_skill: \u0026MetaSkill,\n    ) -\u003e Result\u003cPackedSlices, MetaSkillError\u003e {\n        let budget = self.context_budget.available().await;\n        \n        self.logger.log_packing_start(slices.len(), budget);\n        \n        // Sort by priority (required first, then by priority weight)\n        let mut sorted_slices = slices;\n        sorted_slices.sort_by(|a, b| {\n            match (a.required, b.required) {\n                (true, false) =\u003e std::cmp::Ordering::Less,\n                (false, true) =\u003e std::cmp::Ordering::Greater,\n                _ =\u003e b.priority.cmp(\u0026a.priority),\n            }\n        });\n        \n        let mut packed = PackedSlices::new(budget);\n        \n        for slice in sorted_slices {\n            let slice_tokens = slice.slice.estimate_tokens();\n            \n            if packed.can_fit(slice_tokens) {\n                packed.add(slice);\n                self.logger.log_slice_packed(\u0026slice.slice.id, slice_tokens);\n            } else if slice.required {\n                return Err(MetaSkillError::InsufficientBudget {\n                    required: slice_tokens,\n                    available: packed.remaining(),\n                    slice_id: slice.slice.id.clone(),\n                });\n            } else {\n                self.logger.log_slice_dropped(\u0026slice.slice.id, \"budget exceeded\");\n            }\n        }\n        \n        Ok(packed)\n    }\n}\n```\n\n## Meta-Skill Definition Format (TOML)\n\nMeta-skills are defined in TOML files for easy authoring:\n\n```toml\n# ~/.meta_skill/meta-skills/frontend-polish.toml\n\n[meta_skill]\nid = \"frontend-polish\"\nname = \"Frontend Polish Kit\"\ndescription = \"\"\"\nA comprehensive bundle for polishing frontend applications.\nIncludes UI component patterns, accessibility guidelines, and React best practices.\nOptimized for Next.js projects but works with any React setup.\n\"\"\"\n\n[meta_skill.metadata]\nauthor = \"meta_skill-community\"\nversion = \"1.0.0\"\ntags = [\"frontend\", \"react\", \"accessibility\", \"ui\"]\ntech_stacks = [\"nextjs\", \"react\"]\n\n[meta_skill.pin_strategy]\ntype = \"LatestCompatible\"\n\n# Context requirements\nmin_context_tokens = 4000\nrecommended_context_tokens = 12000\n\n# Slice references - order matters for packing priority\n\n[[slices]]\nskill_id = \"nextjs-ui\"\nslice_ids = [\"component-patterns\", \"routing-best-practices\", \"ssr-guidelines\"]\npriority = 100\nrequired = true\n\n[[slices]]\nskill_id = \"a11y\"\nslice_ids = [\"aria-patterns\", \"keyboard-navigation\", \"screen-reader-tips\"]\npriority = 90\nrequired = true\nlevel = \"Core\"\n\n[[slices]]\nskill_id = \"react-patterns\"\nslice_ids = [\"hooks-patterns\", \"state-management\", \"performance-optimization\"]\npriority = 80\nrequired = false\n\n[[slices]]\nskill_id = \"react-patterns\"\nslice_ids = [\"advanced-composition\", \"render-props\", \"hoc-patterns\"]\npriority = 50\nrequired = false\nlevel = \"Extended\"\n\n# Conditional slice - only included for TypeScript projects\n[[slices]]\nskill_id = \"typescript-react\"\nslice_ids = [\"type-safe-props\", \"generic-components\"]\npriority = 70\nrequired = false\n\n[[slices.conditions]]\ntype = \"FileExists\"\npattern = \"tsconfig.json\"\n```\n\n## Parsing and Validation\n\n```rust\nuse std::path::Path;\n\n/// Parser for meta-skill TOML definitions\npub struct MetaSkillParser;\n\nimpl MetaSkillParser {\n    /// Parse a meta-skill from a TOML file\n    pub fn parse_file(path: \u0026Path) -\u003e Result\u003cMetaSkill, ParseError\u003e {\n        let content = std::fs::read_to_string(path)\n            .map_err(|e| ParseError::IoError(path.to_path_buf(), e))?;\n        \n        Self::parse_str(\u0026content, path)\n    }\n    \n    /// Parse a meta-skill from a TOML string\n    pub fn parse_str(content: \u0026str, source: \u0026Path) -\u003e Result\u003cMetaSkill, ParseError\u003e {\n        let raw: RawMetaSkillToml = toml::from_str(content)\n            .map_err(|e| ParseError::TomlError(source.to_path_buf(), e))?;\n        \n        Self::validate_and_convert(raw, source)\n    }\n    \n    /// Validate the parsed structure and convert to domain type\n    fn validate_and_convert(\n        raw: RawMetaSkillToml,\n        source: \u0026Path,\n    ) -\u003e Result\u003cMetaSkill, ParseError\u003e {\n        // Validate required fields\n        if raw.meta_skill.id.is_empty() {\n            return Err(ParseError::MissingField(source.to_path_buf(), \"id\"));\n        }\n        \n        if raw.slices.is_empty() {\n            return Err(ParseError::MissingField(source.to_path_buf(), \"slices\"));\n        }\n        \n        // Validate slice references\n        for (i, slice) in raw.slices.iter().enumerate() {\n            if slice.skill_id.is_empty() {\n                return Err(ParseError::InvalidSlice(\n                    source.to_path_buf(),\n                    i,\n                    \"skill_id cannot be empty\",\n                ));\n            }\n        }\n        \n        // Convert to domain type\n        Ok(MetaSkill {\n            id: raw.meta_skill.id,\n            name: raw.meta_skill.name,\n            description: raw.meta_skill.description,\n            slices: raw.slices.into_iter().map(|s| s.into()).collect(),\n            pin_strategy: raw.meta_skill.pin_strategy.unwrap_or_default().into(),\n            metadata: raw.meta_skill.metadata.unwrap_or_default().into(),\n            min_context_tokens: raw.meta_skill.min_context_tokens.unwrap_or(2000),\n            recommended_context_tokens: raw.meta_skill.recommended_context_tokens.unwrap_or(8000),\n        })\n    }\n}\n```\n\n## Registry Implementation\n\n```rust\n/// Registry for discovering and managing meta-skills\npub struct MetaSkillRegistry {\n    /// Map of meta-skill ID to definition\n    meta_skills: HashMap\u003cString, MetaSkill\u003e,\n    \n    /// Index for tag-based search\n    tag_index: HashMap\u003cString, Vec\u003cString\u003e\u003e,\n    \n    /// Index for tech-stack-based search\n    tech_stack_index: HashMap\u003cString, Vec\u003cString\u003e\u003e,\n    \n    /// Paths to meta-skill directories\n    search_paths: Vec\u003cPathBuf\u003e,\n}\n\nimpl MetaSkillRegistry {\n    /// Create a new registry with default search paths\n    pub fn new() -\u003e Self {\n        let mut search_paths = vec![];\n        \n        // User meta-skills\n        if let Some(home) = dirs::home_dir() {\n            search_paths.push(home.join(\".meta_skill\").join(\"meta-skills\"));\n        }\n        \n        // Project-local meta-skills\n        search_paths.push(PathBuf::from(\".meta_skill\").join(\"meta-skills\"));\n        \n        // System meta-skills\n        search_paths.push(PathBuf::from(\"/usr/share/meta_skill/meta-skills\"));\n        \n        Self {\n            meta_skills: HashMap::new(),\n            tag_index: HashMap::new(),\n            tech_stack_index: HashMap::new(),\n            search_paths,\n        }\n    }\n    \n    /// Scan all search paths and load meta-skill definitions\n    pub fn scan(\u0026mut self) -\u003e Result\u003cScanResult, RegistryError\u003e {\n        let mut result = ScanResult::default();\n        \n        for path in \u0026self.search_paths {\n            if !path.exists() {\n                continue;\n            }\n            \n            for entry in std::fs::read_dir(path)? {\n                let entry = entry?;\n                let file_path = entry.path();\n                \n                if file_path.extension().map(|e| e == \"toml\").unwrap_or(false) {\n                    match MetaSkillParser::parse_file(\u0026file_path) {\n                        Ok(meta_skill) =\u003e {\n                            self.index_meta_skill(\u0026meta_skill);\n                            self.meta_skills.insert(meta_skill.id.clone(), meta_skill);\n                            result.loaded += 1;\n                        }\n                        Err(e) =\u003e {\n                            result.errors.push((file_path, e));\n                        }\n                    }\n                }\n            }\n        }\n        \n        Ok(result)\n    }\n    \n    /// Index a meta-skill for search\n    fn index_meta_skill(\u0026mut self, meta_skill: \u0026MetaSkill) {\n        // Index by tags\n        for tag in \u0026meta_skill.metadata.tags {\n            self.tag_index\n                .entry(tag.clone())\n                .or_default()\n                .push(meta_skill.id.clone());\n        }\n        \n        // Index by tech stack\n        for stack in \u0026meta_skill.metadata.tech_stacks {\n            self.tech_stack_index\n                .entry(stack.clone())\n                .or_default()\n                .push(meta_skill.id.clone());\n        }\n    }\n    \n    /// Search meta-skills by various criteria\n    pub fn search(\u0026self, query: \u0026MetaSkillQuery) -\u003e Vec\u003c\u0026MetaSkill\u003e {\n        let mut results: Vec\u003c\u0026MetaSkill\u003e = self.meta_skills.values().collect();\n        \n        // Filter by text search\n        if let Some(text) = \u0026query.text {\n            let text_lower = text.to_lowercase();\n            results.retain(|ms| {\n                ms.name.to_lowercase().contains(\u0026text_lower)\n                    || ms.description.to_lowercase().contains(\u0026text_lower)\n                    || ms.id.to_lowercase().contains(\u0026text_lower)\n            });\n        }\n        \n        // Filter by tags\n        if !query.tags.is_empty() {\n            results.retain(|ms| {\n                query.tags.iter().any(|t| ms.metadata.tags.contains(t))\n            });\n        }\n        \n        // Filter by tech stack\n        if let Some(stack) = \u0026query.tech_stack {\n            results.retain(|ms| ms.metadata.tech_stacks.contains(stack));\n        }\n        \n        results\n    }\n}\n```\n\n## CLI Integration\n\n```rust\n/// CLI subcommand for meta-skill operations\npub enum MetaSkillCommand {\n    /// Load a meta-skill into the current context\n    Load {\n        /// Meta-skill ID to load\n        id: String,\n        /// Override context budget\n        #[arg(long)]\n        budget: Option\u003cusize\u003e,\n        /// Force reload even if already loaded\n        #[arg(long)]\n        force: bool,\n    },\n    /// List available meta-skills\n    List {\n        /// Filter by tag\n        #[arg(long)]\n        tag: Option\u003cString\u003e,\n        /// Filter by tech stack\n        #[arg(long)]\n        stack: Option\u003cString\u003e,\n    },\n    /// Show details of a specific meta-skill\n    Show {\n        /// Meta-skill ID\n        id: String,\n    },\n    /// Create a new meta-skill definition\n    Create {\n        /// Meta-skill ID\n        id: String,\n        /// Interactive mode\n        #[arg(long)]\n        interactive: bool,\n    },\n}\n```\n\n## Tasks\n\n### Task 1: Define Core Data Structures\n- [ ] Create `src/meta_skills/types.rs` with all struct definitions\n- [ ] Implement `Serialize`/`Deserialize` for all types\n- [ ] Add validation methods to each struct\n- [ ] Write unit tests for serialization round-trips\n\n### Task 2: Implement MetaSkillParser\n- [ ] Create `src/meta_skills/parser.rs`\n- [ ] Implement TOML parsing with proper error handling\n- [ ] Add validation for required fields\n- [ ] Support all condition types in slice references\n- [ ] Write tests with valid and invalid TOML inputs\n\n### Task 3: Implement MetaSkillRegistry\n- [ ] Create `src/meta_skills/registry.rs`\n- [ ] Implement search path discovery\n- [ ] Build tag and tech-stack indexes\n- [ ] Implement search functionality\n- [ ] Add caching for parsed definitions\n\n### Task 4: Implement MetaSkillManager\n- [ ] Create `src/meta_skills/manager.rs`\n- [ ] Implement skill version resolution per pin strategy\n- [ ] Implement condition evaluation\n- [ ] Implement slice packing algorithm\n- [ ] Add comprehensive logging throughout\n\n### Task 5: Implement CLI Commands\n- [ ] Add `ms meta-skill load` subcommand\n- [ ] Add `ms meta-skill list` subcommand\n- [ ] Add `ms meta-skill show` subcommand\n- [ ] Add `ms meta-skill create` subcommand\n\n### Task 6: Create Built-in Meta-Skills\n- [ ] Create `frontend-polish.toml` meta-skill\n- [ ] Create `rust-safety.toml` meta-skill\n- [ ] Create `api-design.toml` meta-skill\n- [ ] Document meta-skill authoring guide\n\n## Acceptance Criteria\n\n1. **Definition Loading**: Meta-skill TOML files parse correctly with full validation\n2. **Skill Resolution**: All pin strategies resolve skill versions correctly\n3. **Slice Packing**: Slices are packed optimally within context budget\n4. **Required Enforcement**: Required slices always included or error raised\n5. **Condition Evaluation**: All condition types evaluate correctly\n6. **Registry Search**: Search by text, tags, and tech stack works correctly\n7. **CLI Integration**: All commands work and produce helpful output\n8. **Logging**: All operations produce detailed trace logs\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_meta_skill_parse_valid() {\n        let toml = r#\"\n            [meta_skill]\n            id = \"test-meta\"\n            name = \"Test Meta-Skill\"\n            description = \"A test meta-skill\"\n            \n            [[slices]]\n            skill_id = \"skill-1\"\n            slice_ids = [\"slice-a\", \"slice-b\"]\n            priority = 100\n            required = true\n        \"#;\n        \n        let result = MetaSkillParser::parse_str(toml, Path::new(\"test.toml\"));\n        assert!(result.is_ok());\n        \n        let ms = result.unwrap();\n        assert_eq!(ms.id, \"test-meta\");\n        assert_eq!(ms.slices.len(), 1);\n        assert!(ms.slices[0].required);\n    }\n    \n    #[test]\n    fn test_pin_strategy_resolution() {\n        // Test each pin strategy type\n    }\n    \n    #[test]\n    fn test_slice_packing_respects_budget() {\n        // Test that packing stays within budget\n    }\n    \n    #[test]\n    fn test_required_slices_must_fit() {\n        // Test that required slices cause error if they don't fit\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_meta_skill_load_end_to_end() {\n    // Set up test registry with sample meta-skills\n    // Load a meta-skill\n    // Verify all slices resolved correctly\n}\n\n#[tokio::test]\nasync fn test_condition_evaluation() {\n    // Test file-exists condition\n    // Test tech-stack condition\n    // Test depends-on condition\n}\n```\n\n### Logging Requirements\n\nAll operations must log with the following detail levels:\n\n```rust\n// DEBUG level - development troubleshooting\nlog::debug!(\"Parsing meta-skill from {:?}\", path);\nlog::debug!(\"Resolved skill {} to version {}\", skill_id, version);\nlog::debug!(\"Evaluating condition: {:?}\", condition);\n\n// INFO level - normal operation visibility\nlog::info!(\"Loading meta-skill: {}\", meta_skill_id);\nlog::info!(\"Packed {} slices using {} tokens\", count, tokens);\n\n// WARN level - recoverable issues\nlog::warn!(\"Slice {} dropped due to budget constraints\", slice_id);\nlog::warn!(\"Condition evaluation failed, skipping slice: {}\", slice_id);\n\n// ERROR level - failures\nlog::error!(\"Failed to parse meta-skill {}: {}\", path, error);\nlog::error!(\"Required slice {} exceeds available budget\", slice_id);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-0an` (Micro-Slicing Engine) - Need slice infrastructure\n- **Blocks**: `meta_skill-7va` (ms load Command) - Load command uses meta-skills\n\n## References\n\n- Plan Section 6.6: Meta-skills concept\n- Plan Section 6.1: Micro-slicing engine (dependency)\n- Plan Section 4.1: Progressive disclosure levels","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:54:18.6537013-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:54:18.6537013-05:00","labels":["composition","meta-skills","phase-3"],"dependencies":[{"issue_id":"meta_skill-7ws","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T23:00:20.519752956-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-897","title":"CASS Mining: Optimization Patterns","description":"Deep dive into cost analytics optimization, O(n log k) vs O(n log n) patterns, topk heap collectors, performance optimization workflows. Extract actionable skill patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:15.413819545-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:25:14.416904398-05:00","closed_at":"2026-01-13T18:25:14.416904398-05:00","close_reason":"Section 31 added: Optimization Patterns and Methodology (~1,550 lines of comprehensive optimization patterns from CASS mining)","labels":["cass-mining"]}
{"id":"meta_skill-8df","title":"Context Fingerprints \u0026 Suggestion Cooldowns","description":"# Context Fingerprints \u0026 Suggestion Cooldowns\n\n## Overview\n\nThis feature prevents suggestion spam by computing a \"fingerprint\" of the current context and implementing cooldowns for skill suggestions. When a user dismisses or ignores a suggestion, and the context hasn't meaningfully changed, the system should not re-suggest the same skills.\n\n**Problem Statement:**\nWithout fingerprinting and cooldowns, the suggestion system would repeatedly suggest the same skills every time the user invokes any command, leading to:\n- User frustration from repetitive suggestions\n- Degraded trust in the suggestion system\n- Increased cognitive load filtering out noise\n\n## Background \u0026 Rationale\n\n### Section 7.2.1 Reference\n\nFrom the plan Section 7.2.1:\n\u003e \"Prevent suggestion spam when context hasn't meaningfully changed. Compute context fingerprint from repo root, git head, diff hash, open files, recent commands.\"\n\n### What Constitutes \"Meaningful Change\"?\n\nThe fingerprint captures signals that indicate the user's working context has shifted:\n\n1. **Repository Root**: Different project entirely\n2. **Git HEAD**: New commits, branch switches\n3. **Diff Hash**: Uncommitted changes (staged + unstaged)\n4. **Open Files Hash**: Files user is actively editing\n5. **Recent Commands Hash**: CLI activity patterns\n\nWhen ANY of these change significantly, the fingerprint changes, allowing fresh suggestions.\n\n### Cooldown Behavior\n\n- When a skill is suggested and dismissed/ignored, record the fingerprint\n- Do not re-suggest that skill until fingerprint changes\n- Cooldown entries expire after configurable TTL (default: 1 hour)\n- Per-skill cooldowns (dismissing skill A doesn't affect skill B)\n\n## Core Data Structures\n\n### ContextFingerprint Struct\n\n```rust\nuse std::path::PathBuf;\nuse std::hash::{Hash, Hasher};\nuse std::collections::hash_map::DefaultHasher;\n\n/// A fingerprint capturing the current working context.\n/// Used to detect meaningful changes that should reset suggestion cooldowns.\n#[derive(Debug, Clone, PartialEq, Eq, Hash)]\npub struct ContextFingerprint {\n    /// Absolute path to the repository root (or project root if not git)\n    pub repo_root: PathBuf,\n    \n    /// Current git HEAD commit hash (None if not a git repo or detached)\n    pub git_head: Option\u003cString\u003e,\n    \n    /// Hash of the current git diff (staged + unstaged changes)\n    /// Changes when user modifies files\n    pub diff_hash: u64,\n    \n    /// Hash of the set of currently open files (from editor integration)\n    /// Changes when user opens/closes files\n    pub open_files_hash: u64,\n    \n    /// Hash of recent command history (last N commands)\n    /// Captures workflow patterns\n    pub recent_commands_hash: u64,\n}\n\nimpl ContextFingerprint {\n    /// Create a new fingerprint from current context\n    pub fn capture(ctx: \u0026ContextCapture) -\u003e Self {\n        Self {\n            repo_root: ctx.repo_root.clone(),\n            git_head: ctx.git_head.clone(),\n            diff_hash: ctx.compute_diff_hash(),\n            open_files_hash: ctx.compute_open_files_hash(),\n            recent_commands_hash: ctx.compute_commands_hash(),\n        }\n    }\n    \n    /// Compute a single u64 hash of the entire fingerprint for storage\n    pub fn as_u64(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        self.hash(\u0026mut hasher);\n        hasher.finish()\n    }\n    \n    /// Check if this fingerprint differs meaningfully from another\n    /// Returns a ChangeSignificance indicating how different they are\n    pub fn compare(\u0026self, other: \u0026Self) -\u003e ChangeSignificance {\n        // Different repo is always a major change\n        if self.repo_root != other.repo_root {\n            return ChangeSignificance::Major;\n        }\n        \n        // Different git HEAD is a major change (new commits, branch switch)\n        if self.git_head != other.git_head {\n            return ChangeSignificance::Major;\n        }\n        \n        // Collect minor changes\n        let mut minor_changes = 0;\n        \n        if self.diff_hash != other.diff_hash {\n            minor_changes += 1;\n        }\n        \n        if self.open_files_hash != other.open_files_hash {\n            minor_changes += 1;\n        }\n        \n        if self.recent_commands_hash != other.recent_commands_hash {\n            minor_changes += 1;\n        }\n        \n        match minor_changes {\n            0 =\u003e ChangeSignificance::None,\n            1 =\u003e ChangeSignificance::Minor,\n            _ =\u003e ChangeSignificance::Moderate,\n        }\n    }\n}\n\n/// How significantly has the context changed?\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub enum ChangeSignificance {\n    /// No change detected\n    None,\n    /// Small change (e.g., one file edited)\n    Minor,\n    /// Moderate change (e.g., multiple files, different commands)\n    Moderate,\n    /// Major change (different repo, different branch/commit)\n    Major,\n}\n```\n\n### ContextCapture Struct\n\n```rust\nuse std::path::PathBuf;\nuse std::process::Command;\n\n/// Captures raw context data for fingerprint computation\npub struct ContextCapture {\n    pub repo_root: PathBuf,\n    pub git_head: Option\u003cString\u003e,\n    pub diff_content: Option\u003cString\u003e,\n    pub open_files: Vec\u003cPathBuf\u003e,\n    pub recent_commands: Vec\u003cString\u003e,\n}\n\nimpl ContextCapture {\n    /// Capture current context from the environment\n    pub fn capture_current() -\u003e Result\u003cSelf, CaptureError\u003e {\n        let repo_root = Self::find_repo_root()?;\n        let git_head = Self::get_git_head(\u0026repo_root);\n        let diff_content = Self::get_git_diff(\u0026repo_root);\n        let open_files = Self::get_open_files()?;\n        let recent_commands = Self::get_recent_commands()?;\n        \n        Ok(Self {\n            repo_root,\n            git_head,\n            diff_content,\n            open_files,\n            recent_commands,\n        })\n    }\n    \n    /// Find the repository or project root\n    fn find_repo_root() -\u003e Result\u003cPathBuf, CaptureError\u003e {\n        // Try git first\n        let output = Command::new(\"git\")\n            .args([\"rev-parse\", \"--show-toplevel\"])\n            .output();\n        \n        if let Ok(output) = output {\n            if output.status.success() {\n                let path = String::from_utf8_lossy(\u0026output.stdout);\n                return Ok(PathBuf::from(path.trim()));\n            }\n        }\n        \n        // Fall back to current directory\n        std::env::current_dir().map_err(CaptureError::IoError)\n    }\n    \n    /// Get current git HEAD commit hash\n    fn get_git_head(repo_root: \u0026Path) -\u003e Option\u003cString\u003e {\n        let output = Command::new(\"git\")\n            .args([\"rev-parse\", \"HEAD\"])\n            .current_dir(repo_root)\n            .output()\n            .ok()?;\n        \n        if output.status.success() {\n            Some(String::from_utf8_lossy(\u0026output.stdout).trim().to_string())\n        } else {\n            None\n        }\n    }\n    \n    /// Get current git diff (staged + unstaged)\n    fn get_git_diff(repo_root: \u0026Path) -\u003e Option\u003cString\u003e {\n        let output = Command::new(\"git\")\n            .args([\"diff\", \"HEAD\"])\n            .current_dir(repo_root)\n            .output()\n            .ok()?;\n        \n        if output.status.success() {\n            Some(String::from_utf8_lossy(\u0026output.stdout).to_string())\n        } else {\n            None\n        }\n    }\n    \n    /// Get list of open files from editor integration\n    fn get_open_files() -\u003e Result\u003cVec\u003cPathBuf\u003e, CaptureError\u003e {\n        // Check for VS Code workspace state\n        if let Some(files) = Self::get_vscode_open_files() {\n            return Ok(files);\n        }\n        \n        // Check for Neovim RPC\n        if let Some(files) = Self::get_neovim_open_files() {\n            return Ok(files);\n        }\n        \n        // Fall back to recently modified files in repo\n        Self::get_recently_modified_files()\n    }\n    \n    /// Get recent commands from history\n    fn get_recent_commands() -\u003e Result\u003cVec\u003cString\u003e, CaptureError\u003e {\n        // Read from ms command history file\n        let history_path = dirs::data_dir()\n            .unwrap_or_default()\n            .join(\"meta_skill\")\n            .join(\"command_history\");\n        \n        if history_path.exists() {\n            let content = std::fs::read_to_string(\u0026history_path)?;\n            let commands: Vec\u003cString\u003e = content\n                .lines()\n                .rev()\n                .take(20) // Last 20 commands\n                .map(|s| s.to_string())\n                .collect();\n            Ok(commands)\n        } else {\n            Ok(vec![])\n        }\n    }\n    \n    /// Compute hash of diff content\n    pub fn compute_diff_hash(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        if let Some(diff) = \u0026self.diff_content {\n            diff.hash(\u0026mut hasher);\n        }\n        hasher.finish()\n    }\n    \n    /// Compute hash of open files set\n    pub fn compute_open_files_hash(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        let mut sorted_files: Vec\u003c_\u003e = self.open_files.iter().collect();\n        sorted_files.sort();\n        for file in sorted_files {\n            file.hash(\u0026mut hasher);\n        }\n        hasher.finish()\n    }\n    \n    /// Compute hash of recent commands\n    pub fn compute_commands_hash(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        for cmd in \u0026self.recent_commands {\n            cmd.hash(\u0026mut hasher);\n        }\n        hasher.finish()\n    }\n}\n```\n\n### SuggestionCooldownCache Struct\n\n```rust\nuse std::collections::HashMap;\nuse chrono::{DateTime, Utc, Duration};\n\n/// Type alias for skill identifiers\npub type SkillId = String;\n\n/// Cache for tracking suggestion cooldowns per skill\n#[derive(Debug, Clone)]\npub struct SuggestionCooldownCache {\n    /// Map from skill ID to cooldown entry\n    entries: HashMap\u003cSkillId, CooldownEntry\u003e,\n    \n    /// Maximum number of entries to store (LRU eviction)\n    max_entries: usize,\n    \n    /// Default cooldown duration\n    default_ttl: Duration,\n    \n    /// Minimum context change significance to reset cooldown\n    reset_threshold: ChangeSignificance,\n}\n\n/// A single cooldown entry for a skill\n#[derive(Debug, Clone)]\npub struct CooldownEntry {\n    /// The skill IDs this cooldown applies to\n    pub skill_ids: Vec\u003cSkillId\u003e,\n    \n    /// When this suggestion was made\n    pub suggested_at: DateTime\u003cUtc\u003e,\n    \n    /// Fingerprint when suggestion was made (as u64 for storage efficiency)\n    pub fingerprint: u64,\n    \n    /// How the user responded to the suggestion\n    pub user_response: SuggestionResponse,\n    \n    /// When this entry expires (None = never auto-expire)\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\n/// How did the user respond to a suggestion?\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum SuggestionResponse {\n    /// User explicitly dismissed (\"no thanks\")\n    Dismissed,\n    /// User ignored (didn't interact)\n    Ignored,\n    /// User accepted (loaded the skill)\n    Accepted,\n    /// User snoozed (\"remind me later\")\n    Snoozed { until: DateTime\u003cUtc\u003e },\n}\n\nimpl SuggestionCooldownCache {\n    /// Create a new cooldown cache with default settings\n    pub fn new() -\u003e Self {\n        Self {\n            entries: HashMap::new(),\n            max_entries: 1000,\n            default_ttl: Duration::hours(1),\n            reset_threshold: ChangeSignificance::Moderate,\n        }\n    }\n    \n    /// Create with custom configuration\n    pub fn with_config(config: CooldownConfig) -\u003e Self {\n        Self {\n            entries: HashMap::new(),\n            max_entries: config.max_entries,\n            default_ttl: config.default_ttl,\n            reset_threshold: config.reset_threshold,\n        }\n    }\n    \n    /// Check if a skill is currently on cooldown\n    pub fn is_on_cooldown(\n        \u0026self,\n        skill_id: \u0026SkillId,\n        current_fingerprint: \u0026ContextFingerprint,\n    ) -\u003e CooldownStatus {\n        let entry = match self.entries.get(skill_id) {\n            Some(e) =\u003e e,\n            None =\u003e return CooldownStatus::NotOnCooldown,\n        };\n        \n        // Check expiration\n        if let Some(expires) = entry.expires_at {\n            if Utc::now() \u003e expires {\n                return CooldownStatus::Expired;\n            }\n        }\n        \n        // Check if context has changed enough\n        let current_fp_hash = current_fingerprint.as_u64();\n        if current_fp_hash != entry.fingerprint {\n            // Fingerprint changed - need to determine significance\n            // For now, any change resets (we store hash, not full fingerprint)\n            return CooldownStatus::ContextChanged;\n        }\n        \n        // Still on cooldown\n        CooldownStatus::OnCooldown {\n            since: entry.suggested_at,\n            response: entry.user_response,\n        }\n    }\n    \n    /// Record a suggestion and user response\n    pub fn record_suggestion(\n        \u0026mut self,\n        skill_id: SkillId,\n        fingerprint: \u0026ContextFingerprint,\n        response: SuggestionResponse,\n    ) {\n        // Evict oldest if at capacity\n        if self.entries.len() \u003e= self.max_entries {\n            self.evict_oldest();\n        }\n        \n        let expires_at = match response {\n            SuggestionResponse::Accepted =\u003e None, // Don't cooldown accepted\n            SuggestionResponse::Snoozed { until } =\u003e Some(until),\n            _ =\u003e Some(Utc::now() + self.default_ttl),\n        };\n        \n        let entry = CooldownEntry {\n            skill_ids: vec![skill_id.clone()],\n            suggested_at: Utc::now(),\n            fingerprint: fingerprint.as_u64(),\n            user_response: response,\n            expires_at,\n        };\n        \n        self.entries.insert(skill_id, entry);\n    }\n    \n    /// Record suggestion for multiple skills at once\n    pub fn record_batch_suggestion(\n        \u0026mut self,\n        skill_ids: Vec\u003cSkillId\u003e,\n        fingerprint: \u0026ContextFingerprint,\n        response: SuggestionResponse,\n    ) {\n        for skill_id in skill_ids {\n            self.record_suggestion(skill_id, fingerprint, response);\n        }\n    }\n    \n    /// Clear cooldown for a specific skill\n    pub fn clear_cooldown(\u0026mut self, skill_id: \u0026SkillId) {\n        self.entries.remove(skill_id);\n    }\n    \n    /// Clear all expired entries\n    pub fn cleanup_expired(\u0026mut self) {\n        let now = Utc::now();\n        self.entries.retain(|_, entry| {\n            entry.expires_at.map(|exp| exp \u003e now).unwrap_or(true)\n        });\n    }\n    \n    /// Evict oldest entry (LRU)\n    fn evict_oldest(\u0026mut self) {\n        if let Some(oldest_key) = self.entries\n            .iter()\n            .min_by_key(|(_, e)| e.suggested_at)\n            .map(|(k, _)| k.clone())\n        {\n            self.entries.remove(\u0026oldest_key);\n        }\n    }\n    \n    /// Get statistics about the cache\n    pub fn stats(\u0026self) -\u003e CooldownStats {\n        let now = Utc::now();\n        let mut active = 0;\n        let mut expired = 0;\n        let mut by_response = HashMap::new();\n        \n        for entry in self.entries.values() {\n            if entry.expires_at.map(|exp| exp \u003e now).unwrap_or(true) {\n                active += 1;\n            } else {\n                expired += 1;\n            }\n            \n            *by_response.entry(entry.user_response).or_insert(0) += 1;\n        }\n        \n        CooldownStats {\n            total_entries: self.entries.len(),\n            active_cooldowns: active,\n            expired_pending_cleanup: expired,\n            by_response,\n        }\n    }\n}\n\n/// Result of checking cooldown status\n#[derive(Debug, Clone)]\npub enum CooldownStatus {\n    /// Not on cooldown - safe to suggest\n    NotOnCooldown,\n    /// Was on cooldown but expired\n    Expired,\n    /// Context changed enough to reset cooldown\n    ContextChanged,\n    /// Still on cooldown\n    OnCooldown {\n        since: DateTime\u003cUtc\u003e,\n        response: SuggestionResponse,\n    },\n}\n\nimpl CooldownStatus {\n    /// Should we suggest this skill?\n    pub fn should_suggest(\u0026self) -\u003e bool {\n        !matches!(self, CooldownStatus::OnCooldown { .. })\n    }\n}\n```\n\n### Persistence Layer\n\n```rust\nuse std::path::Path;\nuse serde::{Deserialize, Serialize};\n\n/// Persistent storage for cooldown cache\n#[derive(Debug, Serialize, Deserialize)]\npub struct CooldownCacheStorage {\n    pub version: u32,\n    pub entries: Vec\u003cStoredCooldownEntry\u003e,\n    pub last_updated: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct StoredCooldownEntry {\n    pub skill_id: String,\n    pub suggested_at: DateTime\u003cUtc\u003e,\n    pub fingerprint: u64,\n    pub response: String, // Serialized SuggestionResponse\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\nimpl SuggestionCooldownCache {\n    /// Load cache from disk\n    pub fn load(path: \u0026Path) -\u003e Result\u003cSelf, CacheError\u003e {\n        if !path.exists() {\n            return Ok(Self::new());\n        }\n        \n        let content = std::fs::read_to_string(path)?;\n        let storage: CooldownCacheStorage = serde_json::from_str(\u0026content)?;\n        \n        let mut cache = Self::new();\n        for entry in storage.entries {\n            cache.entries.insert(\n                entry.skill_id.clone(),\n                CooldownEntry {\n                    skill_ids: vec![entry.skill_id],\n                    suggested_at: entry.suggested_at,\n                    fingerprint: entry.fingerprint,\n                    user_response: serde_json::from_str(\u0026entry.response)?,\n                    expires_at: entry.expires_at,\n                },\n            );\n        }\n        \n        // Cleanup expired on load\n        cache.cleanup_expired();\n        \n        Ok(cache)\n    }\n    \n    /// Save cache to disk\n    pub fn save(\u0026self, path: \u0026Path) -\u003e Result\u003c(), CacheError\u003e {\n        // Ensure directory exists\n        if let Some(parent) = path.parent() {\n            std::fs::create_dir_all(parent)?;\n        }\n        \n        let entries: Vec\u003cStoredCooldownEntry\u003e = self.entries\n            .iter()\n            .filter(|(_, e)| e.expires_at.map(|exp| exp \u003e Utc::now()).unwrap_or(true))\n            .map(|(skill_id, entry)| StoredCooldownEntry {\n                skill_id: skill_id.clone(),\n                suggested_at: entry.suggested_at,\n                fingerprint: entry.fingerprint,\n                response: serde_json::to_string(\u0026entry.user_response).unwrap(),\n                expires_at: entry.expires_at,\n            })\n            .collect();\n        \n        let storage = CooldownCacheStorage {\n            version: 1,\n            entries,\n            last_updated: Utc::now(),\n        };\n        \n        let content = serde_json::to_string_pretty(\u0026storage)?;\n        std::fs::write(path, content)?;\n        \n        Ok(())\n    }\n}\n```\n\n## Integration with Suggestion Engine\n\n```rust\n/// Suggestion engine with cooldown support\npub struct SuggestionEngine {\n    /// Skill matcher for finding relevant skills\n    skill_matcher: SkillMatcher,\n    \n    /// Cooldown cache\n    cooldown_cache: SuggestionCooldownCache,\n    \n    /// Logger for detailed tracing\n    logger: Arc\u003cdyn SuggestionLogger\u003e,\n}\n\nimpl SuggestionEngine {\n    /// Get suggestions, respecting cooldowns\n    pub fn get_suggestions(\n        \u0026self,\n        context: \u0026ContextCapture,\n        max_suggestions: usize,\n    ) -\u003e Vec\u003cSkillSuggestion\u003e {\n        // Compute current fingerprint\n        let fingerprint = ContextFingerprint::capture(context);\n        \n        self.logger.log_fingerprint_computed(\u0026fingerprint);\n        \n        // Get all matching skills\n        let all_matches = self.skill_matcher.find_matches(context);\n        \n        self.logger.log_matches_found(all_matches.len());\n        \n        // Filter by cooldown\n        let mut suggestions = Vec::new();\n        for matched in all_matches {\n            let cooldown_status = self.cooldown_cache.is_on_cooldown(\n                \u0026matched.skill_id,\n                \u0026fingerprint,\n            );\n            \n            self.logger.log_cooldown_check(\u0026matched.skill_id, \u0026cooldown_status);\n            \n            if cooldown_status.should_suggest() {\n                suggestions.push(matched);\n            }\n        }\n        \n        self.logger.log_suggestions_after_filter(suggestions.len());\n        \n        // Take top N\n        suggestions.truncate(max_suggestions);\n        suggestions\n    }\n    \n    /// Record user response to suggestion\n    pub fn record_response(\n        \u0026mut self,\n        skill_id: \u0026SkillId,\n        response: SuggestionResponse,\n        context: \u0026ContextCapture,\n    ) {\n        let fingerprint = ContextFingerprint::capture(context);\n        \n        self.logger.log_response_recorded(skill_id, \u0026response);\n        \n        self.cooldown_cache.record_suggestion(\n            skill_id.clone(),\n            \u0026fingerprint,\n            response,\n        );\n    }\n}\n```\n\n## Configuration\n\n```rust\n/// Configuration for cooldown behavior\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CooldownConfig {\n    /// Maximum entries in cache\n    pub max_entries: usize,\n    \n    /// Default TTL for cooldowns\n    pub default_ttl: Duration,\n    \n    /// Minimum change significance to reset cooldown\n    pub reset_threshold: ChangeSignificance,\n    \n    /// Per-response-type TTL overrides\n    pub response_ttls: HashMap\u003cString, Duration\u003e,\n    \n    /// Whether to persist cooldowns across sessions\n    pub persist: bool,\n    \n    /// Path for persistence file\n    pub persistence_path: Option\u003cPathBuf\u003e,\n}\n\nimpl Default for CooldownConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_entries: 1000,\n            default_ttl: Duration::hours(1),\n            reset_threshold: ChangeSignificance::Moderate,\n            response_ttls: HashMap::new(),\n            persist: true,\n            persistence_path: None,\n        }\n    }\n}\n```\n\n## Tasks\n\n### Task 1: Implement ContextFingerprint\n- [ ] Create `src/context/fingerprint.rs`\n- [ ] Implement hashing for all fingerprint components\n- [ ] Add comparison with significance levels\n- [ ] Write tests for fingerprint stability\n\n### Task 2: Implement ContextCapture\n- [ ] Create `src/context/capture.rs`\n- [ ] Implement git HEAD detection\n- [ ] Implement git diff hashing\n- [ ] Implement open files detection (VS Code, Neovim)\n- [ ] Implement command history capture\n\n### Task 3: Implement SuggestionCooldownCache\n- [ ] Create `src/suggestions/cooldown.rs`\n- [ ] Implement cooldown checking logic\n- [ ] Implement LRU eviction\n- [ ] Implement expiration cleanup\n- [ ] Add cache statistics\n\n### Task 4: Implement Persistence\n- [ ] Create `src/suggestions/cooldown_storage.rs`\n- [ ] Implement JSON serialization\n- [ ] Implement load with migration support\n- [ ] Implement atomic save with backup\n\n### Task 5: Integrate with Suggestion Engine\n- [ ] Modify suggestion engine to check cooldowns\n- [ ] Add response recording API\n- [ ] Wire up configuration loading\n- [ ] Add CLI flags for cooldown bypass\n\n### Task 6: Add CLI Commands\n- [ ] Add `ms suggest --ignore-cooldowns` flag\n- [ ] Add `ms suggest --reset-cooldowns` command\n- [ ] Add `ms cooldown list` to show active cooldowns\n- [ ] Add `ms cooldown clear \u003cskill-id\u003e` command\n\n## Acceptance Criteria\n\n1. **Fingerprint Accuracy**: Fingerprints correctly detect context changes\n2. **Cooldown Enforcement**: Dismissed suggestions don't reappear until context changes\n3. **Expiration**: Cooldowns expire after TTL even without context change\n4. **Persistence**: Cooldowns survive session restarts\n5. **Performance**: Fingerprint computation \u003c 50ms\n6. **LRU Eviction**: Cache doesn't grow unbounded\n7. **Configuration**: All behaviors configurable via config file\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_fingerprint_detects_git_head_change() {\n        let fp1 = ContextFingerprint {\n            repo_root: PathBuf::from(\"/project\"),\n            git_head: Some(\"abc123\".to_string()),\n            diff_hash: 0,\n            open_files_hash: 0,\n            recent_commands_hash: 0,\n        };\n        \n        let fp2 = ContextFingerprint {\n            git_head: Some(\"def456\".to_string()),\n            ..fp1.clone()\n        };\n        \n        assert_eq!(fp1.compare(\u0026fp2), ChangeSignificance::Major);\n    }\n    \n    #[test]\n    fn test_cooldown_respects_fingerprint() {\n        let mut cache = SuggestionCooldownCache::new();\n        let fp = ContextFingerprint { /* ... */ };\n        \n        cache.record_suggestion(\"skill-1\".to_string(), \u0026fp, SuggestionResponse::Dismissed);\n        \n        // Same fingerprint should be on cooldown\n        assert!(!cache.is_on_cooldown(\"skill-1\", \u0026fp).should_suggest());\n        \n        // Different fingerprint should not be on cooldown\n        let fp2 = ContextFingerprint {\n            git_head: Some(\"different\".to_string()),\n            ..fp\n        };\n        assert!(cache.is_on_cooldown(\"skill-1\", \u0026fp2).should_suggest());\n    }\n    \n    #[test]\n    fn test_cooldown_expires() {\n        // Test TTL expiration\n    }\n    \n    #[test]\n    fn test_lru_eviction() {\n        // Test that oldest entries are evicted\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_suggestion_engine_with_cooldowns() {\n    // Set up engine\n    // Get suggestions\n    // Dismiss one\n    // Get suggestions again - dismissed should be filtered\n}\n\n#[tokio::test]\nasync fn test_cooldown_persistence() {\n    // Create cache, add entries\n    // Save to disk\n    // Load from disk\n    // Verify entries restored\n}\n```\n\n### Logging Requirements\n\n```rust\n// DEBUG level\nlog::debug!(\"Computing context fingerprint...\");\nlog::debug!(\"Git HEAD: {:?}, diff hash: {}\", git_head, diff_hash);\nlog::debug!(\"Checking cooldown for skill {}: {:?}\", skill_id, status);\n\n// INFO level\nlog::info!(\"Context fingerprint changed: {:?}\", significance);\nlog::info!(\"Filtered {} suggestions due to cooldowns\", filtered_count);\n\n// WARN level\nlog::warn!(\"Failed to detect git HEAD: {}\", error);\nlog::warn!(\"Cooldown cache at capacity, evicting oldest entries\");\n\n// ERROR level\nlog::error!(\"Failed to load cooldown cache: {}\", error);\nlog::error!(\"Failed to save cooldown cache: {}\", error);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-o8o` (Context-Aware Suggestions) - Core suggestion infrastructure\n\n## References\n\n- Plan Section 7.2.1: Cooldown rationale\n- Plan Section 7.2: Context-aware suggestion system","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:56:17.759595169-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:56:17.759595169-05:00","labels":["context","cooldowns","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-8df","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:00:22.578870842-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-8gl","title":"Skill Simulation Sandbox (ms simulate)","description":"# Skill Simulation Sandbox (ms simulate)\n\n**Phase 6 - Section 18.10**\n\nSimulate skill end-to-end in a controlled workspace before publishing. This catches broken commands, missing assumptions, and brittle steps that would fail in real usage.\n\n---\n\n## Overview\n\nSkills often contain commands, code snippets, and workflows that make assumptions about the environment. The simulation sandbox:\n\n1. **Creates Isolated Workspace**: Temporary environment with mock files and tools\n2. **Executes Skill Steps**: Runs commands and workflows from the skill\n3. **Captures Output**: Records stdout, stderr, exit codes, and file changes\n4. **Validates Results**: Compares against assertions and expected outcomes\n5. **Generates Reports**: Produces detailed simulation transcript\n\nThis catches issues like:\n- Commands that don't exist or have wrong syntax\n- Missing dependencies or tools\n- Incorrect file paths or permissions\n- Steps that only work on specific platforms\n- Race conditions or timing issues\n\n---\n\n## Core Behavior\n\n### Simulation Workflow\n\n```\n1. Parse skill content to extract executable elements\n2. Create temporary workspace with:\n   - Required directory structure\n   - Mock files (from fixtures or generated)\n   - Mock tools (stubs for dangerous commands)\n3. For each executable element:\n   - Set up isolated environment\n   - Execute in sandbox\n   - Capture all output\n   - Check assertions\n4. Generate simulation report\n5. Clean up workspace\n```\n\n### What Gets Simulated\n\n```rust\n/// Elements that can be simulated from a skill\n#[derive(Debug, Clone)]\npub enum SimulatableElement {\n    /// Shell command from code block\n    Command {\n        command: String,\n        language: String,\n        context: CommandContext,\n    },\n    \n    /// Code snippet that should compile/run\n    CodeSnippet {\n        code: String,\n        language: String,\n        should_compile: bool,\n        should_run: bool,\n    },\n    \n    /// File operations (create, modify, read)\n    FileOperation {\n        operation: FileOp,\n        path: String,\n    },\n    \n    /// Workflow with multiple steps\n    Workflow {\n        name: String,\n        steps: Vec\u003cWorkflowStep\u003e,\n    },\n    \n    /// API call or network request\n    ApiCall {\n        method: String,\n        url: String,\n        expected_response: Option\u003cString\u003e,\n    },\n}\n\n#[derive(Debug, Clone)]\npub struct CommandContext {\n    /// Working directory for command\n    pub cwd: Option\u003cString\u003e,\n    \n    /// Required environment variables\n    pub env: HashMap\u003cString, String\u003e,\n    \n    /// Expected exit code\n    pub expected_exit_code: Option\u003ci32\u003e,\n    \n    /// Expected stdout pattern\n    pub expected_stdout: Option\u003cString\u003e,\n    \n    /// Whether command is destructive\n    pub is_destructive: bool,\n}\n\n#[derive(Debug, Clone)]\npub enum FileOp {\n    Create { content: String },\n    Append { content: String },\n    Read,\n    Delete,\n    Move { to: String },\n}\n\n#[derive(Debug, Clone)]\npub struct WorkflowStep {\n    pub name: String,\n    pub element: SimulatableElement,\n    pub depends_on: Vec\u003cString\u003e,\n}\n```\n\n---\n\n## Core Data Structures\n\n### Simulation Sandbox\n\n```rust\nuse std::collections::HashMap;\nuse std::path::PathBuf;\nuse std::process::Output;\nuse tempfile::TempDir;\n\n/// Isolated sandbox for skill simulation\npub struct SimulationSandbox {\n    /// Temporary workspace directory\n    workspace: TempDir,\n    \n    /// Mock tool registry\n    mock_tools: HashMap\u003cString, MockTool\u003e,\n    \n    /// Environment variables\n    env: HashMap\u003cString, String\u003e,\n    \n    /// File system snapshot before simulation\n    initial_state: FileSystemState,\n    \n    /// Captured outputs\n    outputs: Vec\u003cCapturedOutput\u003e,\n    \n    /// Simulation configuration\n    config: SimulationConfig,\n}\n\n#[derive(Debug, Clone)]\npub struct SimulationConfig {\n    /// Allow network access\n    pub allow_network: bool,\n    \n    /// Allow file system access outside workspace\n    pub allow_external_fs: bool,\n    \n    /// Maximum execution time per command\n    pub command_timeout: Duration,\n    \n    /// Maximum total simulation time\n    pub total_timeout: Duration,\n    \n    /// Commands to mock (replace with stubs)\n    pub mock_commands: Vec\u003cString\u003e,\n    \n    /// Whether to use Docker for isolation\n    pub use_container: bool,\n    \n    /// Resource limits\n    pub limits: ResourceLimits,\n}\n\n#[derive(Debug, Clone)]\npub struct ResourceLimits {\n    pub max_memory_mb: usize,\n    pub max_cpu_percent: usize,\n    pub max_disk_mb: usize,\n    pub max_processes: usize,\n}\n\nimpl Default for SimulationConfig {\n    fn default() -\u003e Self {\n        Self {\n            allow_network: false,\n            allow_external_fs: false,\n            command_timeout: Duration::from_secs(30),\n            total_timeout: Duration::from_secs(300),\n            mock_commands: vec![\n                \"rm -rf /\".to_string(),\n                \"sudo\".to_string(),\n                \"docker\".to_string(),\n            ],\n            use_container: false,\n            limits: ResourceLimits {\n                max_memory_mb: 512,\n                max_cpu_percent: 50,\n                max_disk_mb: 100,\n                max_processes: 10,\n            },\n        }\n    }\n}\n\nimpl SimulationSandbox {\n    /// Create a new simulation sandbox\n    pub fn new(config: SimulationConfig) -\u003e Result\u003cSelf, SimulationError\u003e {\n        let workspace = TempDir::new()?;\n        \n        Ok(Self {\n            workspace,\n            mock_tools: HashMap::new(),\n            env: HashMap::new(),\n            initial_state: FileSystemState::empty(),\n            outputs: Vec::new(),\n            config,\n        })\n    }\n    \n    /// Set up workspace with fixtures\n    pub fn setup_fixtures(\u0026mut self, fixtures_path: \u0026Path) -\u003e Result\u003c(), SimulationError\u003e {\n        // Copy all files from fixtures to workspace\n        self.copy_dir_recursive(fixtures_path, self.workspace.path())?;\n        \n        // Record initial state\n        self.initial_state = self.capture_fs_state()?;\n        \n        Ok(())\n    }\n    \n    /// Add a mock tool\n    pub fn add_mock_tool(\u0026mut self, name: \u0026str, mock: MockTool) {\n        self.mock_tools.insert(name.to_string(), mock);\n    }\n    \n    /// Execute a command in the sandbox\n    pub fn execute_command(\u0026mut self, cmd: \u0026str, context: \u0026CommandContext) -\u003e Result\u003cCommandResult, SimulationError\u003e {\n        // Check if command should be mocked\n        let cmd_name = cmd.split_whitespace().next().unwrap_or(\"\");\n        if let Some(mock) = self.mock_tools.get(cmd_name) {\n            return self.execute_mock(cmd, mock);\n        }\n        \n        // Check if command is in blocked list\n        for blocked in \u0026self.config.mock_commands {\n            if cmd.contains(blocked) {\n                return Err(SimulationError::BlockedCommand(cmd.to_string()));\n            }\n        }\n        \n        // Set up command\n        let working_dir = context.cwd\n            .as_ref()\n            .map(|p| self.workspace.path().join(p))\n            .unwrap_or_else(|| self.workspace.path().to_path_buf());\n        \n        let mut command = std::process::Command::new(\"sh\");\n        command.arg(\"-c\").arg(cmd);\n        command.current_dir(\u0026working_dir);\n        \n        // Set environment\n        command.env_clear();\n        for (k, v) in \u0026self.env {\n            command.env(k, v);\n        }\n        for (k, v) in \u0026context.env {\n            command.env(k, v);\n        }\n        \n        // Restrict PATH if not allowing external access\n        if !self.config.allow_external_fs {\n            let safe_path = \"/usr/bin:/bin:/usr/local/bin\";\n            command.env(\"PATH\", safe_path);\n        }\n        \n        // Execute with timeout\n        let start = std::time::Instant::now();\n        let output = self.execute_with_timeout(\u0026mut command, self.config.command_timeout)?;\n        let duration = start.elapsed();\n        \n        let result = CommandResult {\n            command: cmd.to_string(),\n            exit_code: output.status.code().unwrap_or(-1),\n            stdout: String::from_utf8_lossy(\u0026output.stdout).to_string(),\n            stderr: String::from_utf8_lossy(\u0026output.stderr).to_string(),\n            duration,\n            working_dir: working_dir.to_string_lossy().to_string(),\n        };\n        \n        // Capture output\n        self.outputs.push(CapturedOutput::Command(result.clone()));\n        \n        Ok(result)\n    }\n    \n    fn execute_with_timeout(\n        \u0026self,\n        command: \u0026mut std::process::Command,\n        timeout: Duration,\n    ) -\u003e Result\u003cOutput, SimulationError\u003e {\n        use std::process::Stdio;\n        \n        command.stdout(Stdio::piped());\n        command.stderr(Stdio::piped());\n        \n        let mut child = command.spawn()?;\n        \n        let start = std::time::Instant::now();\n        loop {\n            match child.try_wait()? {\n                Some(_) =\u003e {\n                    return child.wait_with_output().map_err(SimulationError::from);\n                }\n                None =\u003e {\n                    if start.elapsed() \u003e timeout {\n                        child.kill()?;\n                        return Err(SimulationError::Timeout(timeout));\n                    }\n                    std::thread::sleep(Duration::from_millis(100));\n                }\n            }\n        }\n    }\n    \n    fn execute_mock(\u0026self, cmd: \u0026str, mock: \u0026MockTool) -\u003e Result\u003cCommandResult, SimulationError\u003e {\n        Ok(CommandResult {\n            command: cmd.to_string(),\n            exit_code: mock.exit_code,\n            stdout: mock.stdout.clone(),\n            stderr: mock.stderr.clone(),\n            duration: Duration::from_millis(1),\n            working_dir: self.workspace.path().to_string_lossy().to_string(),\n        })\n    }\n    \n    /// Execute a code snippet\n    pub fn execute_code(\u0026mut self, code: \u0026str, language: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let result = match language {\n            \"rust\" =\u003e self.execute_rust_code(code)?,\n            \"python\" =\u003e self.execute_python_code(code)?,\n            \"javascript\" | \"js\" =\u003e self.execute_js_code(code)?,\n            \"bash\" | \"sh\" =\u003e self.execute_bash_code(code)?,\n            _ =\u003e return Err(SimulationError::UnsupportedLanguage(language.to_string())),\n        };\n        \n        self.outputs.push(CapturedOutput::Code(result.clone()));\n        \n        Ok(result)\n    }\n    \n    fn execute_rust_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        // Create a temporary Cargo project\n        let project_dir = self.workspace.path().join(\"rust_sim\");\n        std::fs::create_dir_all(\u0026project_dir)?;\n        \n        // Write Cargo.toml\n        let cargo_toml = r#\"\n[package]\nname = \"simulation\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\"#;\n        std::fs::write(project_dir.join(\"Cargo.toml\"), cargo_toml)?;\n        \n        // Write source\n        let src_dir = project_dir.join(\"src\");\n        std::fs::create_dir_all(\u0026src_dir)?;\n        std::fs::write(src_dir.join(\"main.rs\"), code)?;\n        \n        // Try to compile\n        let compile_result = self.execute_command(\n            \"cargo build\",\n            \u0026CommandContext {\n                cwd: Some(\"rust_sim\".to_string()),\n                env: HashMap::new(),\n                expected_exit_code: Some(0),\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        let compiled = compile_result.exit_code == 0;\n        \n        // Try to run if compiled\n        let (ran, run_output) = if compiled {\n            let run_result = self.execute_command(\n                \"cargo run\",\n                \u0026CommandContext {\n                    cwd: Some(\"rust_sim\".to_string()),\n                    env: HashMap::new(),\n                    expected_exit_code: None,\n                    expected_stdout: None,\n                    is_destructive: false,\n                },\n            )?;\n            (run_result.exit_code == 0, Some(run_result))\n        } else {\n            (false, None)\n        };\n        \n        Ok(CodeResult {\n            language: \"rust\".to_string(),\n            code: code.to_string(),\n            compiled,\n            compile_output: Some(compile_result.stderr),\n            ran,\n            run_output: run_output.map(|r| r.stdout),\n            exit_code: run_output.map(|r| r.exit_code),\n        })\n    }\n    \n    fn execute_python_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let script_path = self.workspace.path().join(\"script.py\");\n        std::fs::write(\u0026script_path, code)?;\n        \n        let result = self.execute_command(\n            \"python3 script.py\",\n            \u0026CommandContext {\n                cwd: None,\n                env: HashMap::new(),\n                expected_exit_code: None,\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        Ok(CodeResult {\n            language: \"python\".to_string(),\n            code: code.to_string(),\n            compiled: true, // Python is interpreted\n            compile_output: None,\n            ran: result.exit_code == 0,\n            run_output: Some(result.stdout),\n            exit_code: Some(result.exit_code),\n        })\n    }\n    \n    fn execute_js_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let script_path = self.workspace.path().join(\"script.js\");\n        std::fs::write(\u0026script_path, code)?;\n        \n        let result = self.execute_command(\n            \"node script.js\",\n            \u0026CommandContext {\n                cwd: None,\n                env: HashMap::new(),\n                expected_exit_code: None,\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        Ok(CodeResult {\n            language: \"javascript\".to_string(),\n            code: code.to_string(),\n            compiled: true,\n            compile_output: None,\n            ran: result.exit_code == 0,\n            run_output: Some(result.stdout),\n            exit_code: Some(result.exit_code),\n        })\n    }\n    \n    fn execute_bash_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let script_path = self.workspace.path().join(\"script.sh\");\n        std::fs::write(\u0026script_path, code)?;\n        \n        let result = self.execute_command(\n            \"bash script.sh\",\n            \u0026CommandContext {\n                cwd: None,\n                env: HashMap::new(),\n                expected_exit_code: None,\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        Ok(CodeResult {\n            language: \"bash\".to_string(),\n            code: code.to_string(),\n            compiled: true,\n            compile_output: None,\n            ran: result.exit_code == 0,\n            run_output: Some(result.stdout),\n            exit_code: Some(result.exit_code),\n        })\n    }\n    \n    /// Capture file system changes\n    pub fn get_fs_changes(\u0026self) -\u003e Result\u003cFileSystemChanges, SimulationError\u003e {\n        let current_state = self.capture_fs_state()?;\n        Ok(self.initial_state.diff(\u0026current_state))\n    }\n    \n    fn capture_fs_state(\u0026self) -\u003e Result\u003cFileSystemState, SimulationError\u003e {\n        let mut state = FileSystemState::empty();\n        self.walk_dir(self.workspace.path(), \u0026mut state)?;\n        Ok(state)\n    }\n    \n    fn walk_dir(\u0026self, path: \u0026Path, state: \u0026mut FileSystemState) -\u003e Result\u003c(), SimulationError\u003e {\n        for entry in std::fs::read_dir(path)? {\n            let entry = entry?;\n            let path = entry.path();\n            \n            if path.is_dir() {\n                self.walk_dir(\u0026path, state)?;\n            } else {\n                let relative = path.strip_prefix(self.workspace.path())\n                    .unwrap_or(\u0026path)\n                    .to_string_lossy()\n                    .to_string();\n                let content = std::fs::read_to_string(\u0026path).unwrap_or_default();\n                let hash = Self::hash_content(\u0026content);\n                \n                state.files.insert(relative, FileInfo {\n                    hash,\n                    size: content.len(),\n                });\n            }\n        }\n        Ok(())\n    }\n    \n    fn hash_content(content: \u0026str) -\u003e String {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        hasher.update(content.as_bytes());\n        format!(\"{:x}\", hasher.finalize())[..16].to_string()\n    }\n    \n    fn copy_dir_recursive(\u0026self, src: \u0026Path, dst: \u0026Path) -\u003e Result\u003c(), SimulationError\u003e {\n        std::fs::create_dir_all(dst)?;\n        \n        for entry in std::fs::read_dir(src)? {\n            let entry = entry?;\n            let src_path = entry.path();\n            let dst_path = dst.join(entry.file_name());\n            \n            if src_path.is_dir() {\n                self.copy_dir_recursive(\u0026src_path, \u0026dst_path)?;\n            } else {\n                std::fs::copy(\u0026src_path, \u0026dst_path)?;\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n/// Mock tool for dangerous commands\n#[derive(Debug, Clone)]\npub struct MockTool {\n    pub exit_code: i32,\n    pub stdout: String,\n    pub stderr: String,\n}\n\n/// Command execution result\n#[derive(Debug, Clone)]\npub struct CommandResult {\n    pub command: String,\n    pub exit_code: i32,\n    pub stdout: String,\n    pub stderr: String,\n    pub duration: Duration,\n    pub working_dir: String,\n}\n\n/// Code execution result\n#[derive(Debug, Clone)]\npub struct CodeResult {\n    pub language: String,\n    pub code: String,\n    pub compiled: bool,\n    pub compile_output: Option\u003cString\u003e,\n    pub ran: bool,\n    pub run_output: Option\u003cString\u003e,\n    pub exit_code: Option\u003ci32\u003e,\n}\n\n/// Captured output during simulation\n#[derive(Debug, Clone)]\npub enum CapturedOutput {\n    Command(CommandResult),\n    Code(CodeResult),\n    FileChange(FileChange),\n}\n\n/// File system state snapshot\n#[derive(Debug, Clone)]\npub struct FileSystemState {\n    pub files: HashMap\u003cString, FileInfo\u003e,\n}\n\nimpl FileSystemState {\n    pub fn empty() -\u003e Self {\n        Self { files: HashMap::new() }\n    }\n    \n    pub fn diff(\u0026self, other: \u0026FileSystemState) -\u003e FileSystemChanges {\n        let mut created = Vec::new();\n        let mut modified = Vec::new();\n        let mut deleted = Vec::new();\n        \n        // Find created and modified\n        for (path, info) in \u0026other.files {\n            match self.files.get(path) {\n                None =\u003e created.push(path.clone()),\n                Some(old_info) if old_info.hash != info.hash =\u003e modified.push(path.clone()),\n                _ =\u003e {}\n            }\n        }\n        \n        // Find deleted\n        for path in self.files.keys() {\n            if !other.files.contains_key(path) {\n                deleted.push(path.clone());\n            }\n        }\n        \n        FileSystemChanges { created, modified, deleted }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FileInfo {\n    pub hash: String,\n    pub size: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct FileSystemChanges {\n    pub created: Vec\u003cString\u003e,\n    pub modified: Vec\u003cString\u003e,\n    pub deleted: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct FileChange {\n    pub path: String,\n    pub change_type: FileChangeType,\n}\n\n#[derive(Debug, Clone)]\npub enum FileChangeType {\n    Created,\n    Modified,\n    Deleted,\n}\n```\n\n---\n\n## Skill Content Parser\n\n```rust\n/// Parses skill content to extract simulatable elements\npub struct SkillContentParser {\n    /// Regex patterns for different content types\n    command_pattern: regex::Regex,\n    code_block_pattern: regex::Regex,\n}\n\nimpl SkillContentParser {\n    pub fn new() -\u003e Self {\n        Self {\n            command_pattern: regex::Regex::new(r\"```(?:bash|sh|shell)\\n([\\s\\S]*?)```\").unwrap(),\n            code_block_pattern: regex::Regex::new(r\"```(\\w+)\\n([\\s\\S]*?)```\").unwrap(),\n        }\n    }\n    \n    /// Extract all simulatable elements from skill content\n    pub fn parse(\u0026self, skill: \u0026Skill) -\u003e Vec\u003cSimulatableElement\u003e {\n        let mut elements = Vec::new();\n        \n        for (section_name, section) in \u0026skill.sections {\n            // Extract commands\n            for cap in self.command_pattern.captures_iter(\u0026section.content) {\n                let command = cap.get(1).unwrap().as_str().trim();\n                for line in command.lines() {\n                    let line = line.trim();\n                    if line.starts_with('$') || line.starts_with('#') {\n                        continue; // Skip prompts and comments\n                    }\n                    if !line.is_empty() {\n                        elements.push(SimulatableElement::Command {\n                            command: line.to_string(),\n                            language: \"bash\".to_string(),\n                            context: CommandContext {\n                                cwd: None,\n                                env: HashMap::new(),\n                                expected_exit_code: Some(0),\n                                expected_stdout: None,\n                                is_destructive: self.is_destructive(line),\n                            },\n                        });\n                    }\n                }\n            }\n            \n            // Extract code snippets\n            for cap in self.code_block_pattern.captures_iter(\u0026section.content) {\n                let language = cap.get(1).unwrap().as_str();\n                let code = cap.get(2).unwrap().as_str().trim();\n                \n                // Skip if this is a command block (already processed)\n                if language == \"bash\" || language == \"sh\" || language == \"shell\" {\n                    continue;\n                }\n                \n                elements.push(SimulatableElement::CodeSnippet {\n                    code: code.to_string(),\n                    language: language.to_string(),\n                    should_compile: self.should_compile(language),\n                    should_run: self.should_run(language, code),\n                });\n            }\n        }\n        \n        elements\n    }\n    \n    fn is_destructive(\u0026self, cmd: \u0026str) -\u003e bool {\n        let destructive_patterns = [\n            \"rm -rf\", \"rm -r\", \"rmdir\",\n            \"dd if=\", \"mkfs\",\n            \"\u003e /dev/\", \"| sudo\",\n        ];\n        \n        destructive_patterns.iter().any(|p| cmd.contains(p))\n    }\n    \n    fn should_compile(\u0026self, language: \u0026str) -\u003e bool {\n        matches!(language, \"rust\" | \"go\" | \"c\" | \"cpp\" | \"java\" | \"typescript\")\n    }\n    \n    fn should_run(\u0026self, language: \u0026str, code: \u0026str) -\u003e bool {\n        // Check if code has a main function or is a script\n        match language {\n            \"rust\" =\u003e code.contains(\"fn main()\"),\n            \"python\" =\u003e !code.contains(\"def \") || code.contains(\"if __name__\"),\n            \"javascript\" | \"js\" =\u003e true,\n            \"go\" =\u003e code.contains(\"func main()\"),\n            _ =\u003e false,\n        }\n    }\n}\n```\n\n---\n\n## Simulation Report\n\n```rust\n/// Complete simulation report\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SimulationReport {\n    /// Skill that was simulated\n    pub skill_id: String,\n    \n    /// When simulation started\n    pub started_at: DateTime\u003cUtc\u003e,\n    \n    /// Total simulation duration\n    pub duration: Duration,\n    \n    /// Overall result\n    pub result: SimulationResult,\n    \n    /// Individual element results\n    pub element_results: Vec\u003cElementResult\u003e,\n    \n    /// File system changes\n    pub fs_changes: FileSystemChanges,\n    \n    /// Issues found\n    pub issues: Vec\u003cSimulationIssue\u003e,\n    \n    /// Warnings\n    pub warnings: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum SimulationResult {\n    /// All elements simulated successfully\n    Success,\n    \n    /// Some elements failed\n    PartialSuccess { passed: usize, failed: usize },\n    \n    /// Critical failure\n    Failure { reason: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ElementResult {\n    /// Element that was simulated\n    pub element: String,\n    \n    /// Whether it succeeded\n    pub success: bool,\n    \n    /// Captured output\n    pub output: Option\u003cString\u003e,\n    \n    /// Error message if failed\n    pub error: Option\u003cString\u003e,\n    \n    /// Duration\n    pub duration: Duration,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SimulationIssue {\n    /// Issue severity\n    pub severity: IssueSeverity,\n    \n    /// Element that caused the issue\n    pub element: String,\n    \n    /// Issue description\n    pub description: String,\n    \n    /// Suggested fix\n    pub suggestion: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum IssueSeverity {\n    Error,\n    Warning,\n    Info,\n}\n\nimpl SimulationReport {\n    /// Generate human-readable report\n    pub fn to_text(\u0026self) -\u003e String {\n        let mut output = String::new();\n        \n        output.push_str(\u0026format!(\"Simulation Report: {}\\n\", self.skill_id));\n        output.push_str(\u0026\"=\".repeat(50));\n        output.push_str(\"\\n\\n\");\n        \n        // Summary\n        output.push_str(\u0026format!(\"Result: {:?}\\n\", self.result));\n        output.push_str(\u0026format!(\"Duration: {:.2}s\\n\\n\", self.duration.as_secs_f64()));\n        \n        // Element results\n        output.push_str(\"Element Results:\\n\");\n        output.push_str(\u0026\"-\".repeat(40));\n        output.push('\\n');\n        \n        for result in \u0026self.element_results {\n            let status = if result.success { \"[PASS]\" } else { \"[FAIL]\" };\n            output.push_str(\u0026format!(\"{} {} ({:.2}s)\\n\", status, result.element, result.duration.as_secs_f64()));\n            \n            if let Some(error) = \u0026result.error {\n                output.push_str(\u0026format!(\"       Error: {}\\n\", error));\n            }\n        }\n        \n        // Issues\n        if !self.issues.is_empty() {\n            output.push_str(\"\\nIssues Found:\\n\");\n            output.push_str(\u0026\"-\".repeat(40));\n            output.push('\\n');\n            \n            for issue in \u0026self.issues {\n                let severity = match issue.severity {\n                    IssueSeverity::Error =\u003e \"ERROR\",\n                    IssueSeverity::Warning =\u003e \"WARN\",\n                    IssueSeverity::Info =\u003e \"INFO\",\n                };\n                output.push_str(\u0026format!(\"[{}] {}: {}\\n\", severity, issue.element, issue.description));\n                if let Some(suggestion) = \u0026issue.suggestion {\n                    output.push_str(\u0026format!(\"       Suggestion: {}\\n\", suggestion));\n                }\n            }\n        }\n        \n        // File system changes\n        if !self.fs_changes.created.is_empty() || !self.fs_changes.modified.is_empty() {\n            output.push_str(\"\\nFile System Changes:\\n\");\n            output.push_str(\u0026\"-\".repeat(40));\n            output.push('\\n');\n            \n            for path in \u0026self.fs_changes.created {\n                output.push_str(\u0026format!(\"  + {}\\n\", path));\n            }\n            for path in \u0026self.fs_changes.modified {\n                output.push_str(\u0026format!(\"  ~ {}\\n\", path));\n            }\n            for path in \u0026self.fs_changes.deleted {\n                output.push_str(\u0026format!(\"  - {}\\n\", path));\n            }\n        }\n        \n        output\n    }\n    \n    /// Generate JSON transcript\n    pub fn to_json(\u0026self) -\u003e Result\u003cString, serde_json::Error\u003e {\n        serde_json::to_string_pretty(self)\n    }\n}\n```\n\n---\n\n## Simulation Engine\n\n```rust\n/// Main simulation engine\npub struct SimulationEngine {\n    /// Skill registry\n    registry: SkillRegistry,\n    \n    /// Content parser\n    parser: SkillContentParser,\n    \n    /// Default configuration\n    default_config: SimulationConfig,\n}\n\nimpl SimulationEngine {\n    pub fn new(registry: SkillRegistry) -\u003e Self {\n        Self {\n            registry,\n            parser: SkillContentParser::new(),\n            default_config: SimulationConfig::default(),\n        }\n    }\n    \n    /// Simulate a skill\n    pub fn simulate(\n        \u0026self,\n        skill_id: \u0026str,\n        fixtures_path: Option\u003c\u0026Path\u003e,\n        config: Option\u003cSimulationConfig\u003e,\n    ) -\u003e Result\u003cSimulationReport, SimulationError\u003e {\n        let config = config.unwrap_or_else(|| self.default_config.clone());\n        let skill = self.registry.get(\u0026SkillId(skill_id.to_string()))?;\n        \n        let started_at = Utc::now();\n        let start_time = std::time::Instant::now();\n        \n        // Create sandbox\n        let mut sandbox = SimulationSandbox::new(config)?;\n        \n        // Set up fixtures if provided\n        if let Some(fixtures) = fixtures_path {\n            sandbox.setup_fixtures(fixtures)?;\n        }\n        \n        // Parse skill content\n        let elements = self.parser.parse(\u0026skill);\n        \n        // Simulate each element\n        let mut element_results = Vec::new();\n        let mut issues = Vec::new();\n        \n        for element in elements {\n            let result = self.simulate_element(\u0026mut sandbox, \u0026element);\n            \n            match result {\n                Ok(elem_result) =\u003e {\n                    if !elem_result.success {\n                        issues.push(SimulationIssue {\n                            severity: IssueSeverity::Error,\n                            element: elem_result.element.clone(),\n                            description: elem_result.error.clone().unwrap_or_default(),\n                            suggestion: self.suggest_fix(\u0026element, \u0026elem_result),\n                        });\n                    }\n                    element_results.push(elem_result);\n                }\n                Err(e) =\u003e {\n                    element_results.push(ElementResult {\n                        element: format!(\"{:?}\", element),\n                        success: false,\n                        output: None,\n                        error: Some(e.to_string()),\n                        duration: Duration::ZERO,\n                    });\n                    issues.push(SimulationIssue {\n                        severity: IssueSeverity::Error,\n                        element: format!(\"{:?}\", element),\n                        description: e.to_string(),\n                        suggestion: None,\n                    });\n                }\n            }\n            \n            // Check total timeout\n            if start_time.elapsed() \u003e self.default_config.total_timeout {\n                issues.push(SimulationIssue {\n                    severity: IssueSeverity::Error,\n                    element: \"overall\".to_string(),\n                    description: \"Simulation timeout exceeded\".to_string(),\n                    suggestion: Some(\"Reduce number of elements or increase timeout\".to_string()),\n                });\n                break;\n            }\n        }\n        \n        // Get file system changes\n        let fs_changes = sandbox.get_fs_changes()?;\n        \n        // Determine overall result\n        let passed = element_results.iter().filter(|r| r.success).count();\n        let failed = element_results.iter().filter(|r| !r.success).count();\n        \n        let result = if failed == 0 {\n            SimulationResult::Success\n        } else if passed \u003e 0 {\n            SimulationResult::PartialSuccess { passed, failed }\n        } else {\n            SimulationResult::Failure { reason: \"All elements failed\".to_string() }\n        };\n        \n        Ok(SimulationReport {\n            skill_id: skill_id.to_string(),\n            started_at,\n            duration: start_time.elapsed(),\n            result,\n            element_results,\n            fs_changes,\n            issues,\n            warnings: Vec::new(),\n        })\n    }\n    \n    fn simulate_element(\n        \u0026self,\n        sandbox: \u0026mut SimulationSandbox,\n        element: \u0026SimulatableElement,\n    ) -\u003e Result\u003cElementResult, SimulationError\u003e {\n        let start = std::time::Instant::now();\n        \n        match element {\n            SimulatableElement::Command { command, context, .. } =\u003e {\n                let result = sandbox.execute_command(command, context)?;\n                \n                let success = result.exit_code == context.expected_exit_code.unwrap_or(0);\n                \n                Ok(ElementResult {\n                    element: format!(\"Command: {}\", command),\n                    success,\n                    output: Some(result.stdout),\n                    error: if success { None } else { Some(result.stderr) },\n                    duration: start.elapsed(),\n                })\n            }\n            \n            SimulatableElement::CodeSnippet { code, language, should_compile, should_run } =\u003e {\n                let result = sandbox.execute_code(code, language)?;\n                \n                let success = (!*should_compile || result.compiled) \n                    \u0026\u0026 (!*should_run || result.ran);\n                \n                Ok(ElementResult {\n                    element: format!(\"Code ({}): {}...\", language, \u0026code[..50.min(code.len())]),\n                    success,\n                    output: result.run_output.or(result.compile_output),\n                    error: if success { None } else { \n                        Some(result.compile_output.unwrap_or_else(|| \"Execution failed\".to_string()))\n                    },\n                    duration: start.elapsed(),\n                })\n            }\n            \n            SimulatableElement::FileOperation { operation, path } =\u003e {\n                // File operations are validated but not executed\n                // (they're handled by the sandbox automatically)\n                Ok(ElementResult {\n                    element: format!(\"File: {:?} on {}\", operation, path),\n                    success: true,\n                    output: None,\n                    error: None,\n                    duration: start.elapsed(),\n                })\n            }\n            \n            _ =\u003e {\n                Ok(ElementResult {\n                    element: format!(\"{:?}\", element),\n                    success: true,\n                    output: Some(\"Skipped (not implemented)\".to_string()),\n                    error: None,\n                    duration: start.elapsed(),\n                })\n            }\n        }\n    }\n    \n    fn suggest_fix(\u0026self, element: \u0026SimulatableElement, result: \u0026ElementResult) -\u003e Option\u003cString\u003e {\n        if result.success {\n            return None;\n        }\n        \n        match element {\n            SimulatableElement::Command { command, .. } =\u003e {\n                if result.error.as_ref().map(|e| e.contains(\"not found\")).unwrap_or(false) {\n                    Some(format!(\"Command '{}' not found. Add to prerequisites or use full path.\", \n                        command.split_whitespace().next().unwrap_or(command)))\n                } else if result.error.as_ref().map(|e| e.contains(\"permission denied\")).unwrap_or(false) {\n                    Some(\"Permission denied. Avoid commands requiring elevated privileges.\".to_string())\n                } else {\n                    None\n                }\n            }\n            SimulatableElement::CodeSnippet { language, .. } =\u003e {\n                if result.error.as_ref().map(|e| e.contains(\"error[E\")).unwrap_or(false) {\n                    Some(\"Rust compilation error. Ensure code snippet is complete and correct.\".to_string())\n                } else {\n                    Some(format!(\"Ensure {} is installed and code is syntactically correct.\", language))\n                }\n            }\n            _ =\u003e None,\n        }\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms simulate \u003cskill\u003e`\n\n```\nSimulate a skill in a controlled environment\n\nUSAGE:\n    ms simulate \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name to simulate\n\nOPTIONS:\n    --with-fixtures \u003cDIR\u003e    Use fixtures from directory\n    --config \u003cFILE\u003e          Use custom simulation config\n    --timeout \u003cSECS\u003e         Total simulation timeout [default: 300]\n    --allow-network          Allow network access during simulation\n    --verbose                Show detailed output\n    --record-transcript      Save detailed transcript\n\nOUTPUT EXAMPLE:\n    Simulating skill: rust-error-handling\n    \n    Setting up sandbox...\n    Using fixtures from: ./fixtures/\n    \n    Simulating elements:\n      [PASS] Command: cargo new example_project (0.23s)\n      [PASS] Command: cargo build (1.45s)\n      [PASS] Code (rust): fn main() { ... } (2.12s)\n      [FAIL] Command: cargo clippy (0.89s)\n             Error: clippy not installed\n             Suggestion: Add clippy to prerequisites\n      [PASS] Code (rust): use std::error::Error... (1.87s)\n    \n    File System Changes:\n      + example_project/\n      + example_project/Cargo.toml\n      + example_project/src/main.rs\n    \n    Result: PartialSuccess (4 passed, 1 failed)\n    Duration: 6.56s\n    \n    Issues Found:\n      [ERROR] Command: cargo clippy\n              clippy not installed\n              Suggestion: Add clippy to prerequisites\n```\n\n### `ms simulate --with-fixtures`\n\n```\nRun simulation with fixture files\n\nUSAGE:\n    ms simulate \u003cSKILL\u003e --with-fixtures \u003cDIR\u003e\n\nThe fixtures directory should contain files that will be copied\nto the simulation workspace before execution.\n\nEXAMPLE STRUCTURE:\n    fixtures/\n    ├── Cargo.toml          # Project manifest\n    ├── src/\n    │   └── main.rs         # Sample source file\n    └── test_data/\n        └── input.json      # Test data\n\nEXAMPLE:\n    ms simulate rust-error-handling --with-fixtures ./fixtures/\n```\n\n### `ms simulate --record-transcript`\n\n```\nRecord detailed simulation transcript\n\nUSAGE:\n    ms simulate \u003cSKILL\u003e --record-transcript [OPTIONS]\n\nOPTIONS:\n    --output \u003cFILE\u003e     Output file [default: simulation-transcript.json]\n    --format \u003cFMT\u003e      Format: json, yaml, markdown [default: json]\n\nThe transcript includes:\n- All executed commands and their output\n- All code executions and results\n- File system changes\n- Timing information\n- Environment state\n\nEXAMPLE:\n    ms simulate rust-error-handling --record-transcript --output report.json\n    \n    # View as markdown\n    ms simulate rust-error-handling --record-transcript --format markdown \u003e report.md\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum SimulationError {\n    #[error(\"Skill not found: {0}\")]\n    SkillNotFound(String),\n    \n    #[error(\"Command blocked: {0}\")]\n    BlockedCommand(String),\n    \n    #[error(\"Execution timeout after {0:?}\")]\n    Timeout(Duration),\n    \n    #[error(\"Unsupported language: {0}\")]\n    UnsupportedLanguage(String),\n    \n    #[error(\"Sandbox creation failed: {0}\")]\n    SandboxError(String),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Parse error: {0}\")]\n    ParseError(String),\n}\n```\n\n---\n\n## Configuration File\n\nSimulation configuration in `~/.config/meta_skill/simulation.toml`:\n\n```toml\n[sandbox]\nallow_network = false\nallow_external_fs = false\ncommand_timeout_secs = 30\ntotal_timeout_secs = 300\nuse_container = false\n\n[limits]\nmax_memory_mb = 512\nmax_cpu_percent = 50\nmax_disk_mb = 100\nmax_processes = 10\n\n[mock_commands]\n# Commands to mock (replace with stubs)\nblocked = [\n    \"rm -rf /\",\n    \"sudo\",\n    \"docker run\",\n    \"kubectl delete\",\n]\n\n# Mock responses for specific commands\n[mock_responses]\n\"git --version\" = { exit_code = 0, stdout = \"git version 2.40.0\" }\n\"docker --version\" = { exit_code = 0, stdout = \"Docker version 24.0.0\" }\n```\n\n---\n\n## Dependencies\n\n- **Skill Tests (ms test)** (meta_skill-x7k): Test infrastructure this builds upon\n- `tempfile`: Temporary workspace management\n- `regex`: Content parsing\n- `sha2`: File content hashing\n- `serde`, `serde_json`, `serde_yaml`: Configuration and report serialization\n- `chrono`: Timestamps","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T23:04:03.74720972-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:07.081061577-05:00","labels":["phase-6","sandbox","simulation","validation"],"dependencies":[{"issue_id":"meta_skill-8gl","depends_on_id":"meta_skill-x7k","type":"blocks","created_at":"2026-01-13T23:04:16.53606976-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-8ti","title":"Cross-Project Learning","description":"# Cross-Project Learning\n\n**Phase 6 - Section 23**\n\nLearn from sessions across multiple projects to build comprehensive skills. This feature enables coverage gap analysis, universal pattern extraction, and knowledge synthesis across diverse codebases.\n\n---\n\n## Overview\n\nSkills become more valuable when they incorporate learnings from multiple projects. A single project may not exercise all aspects of a topic, but patterns observed across many projects reveal universal best practices. Cross-project learning:\n\n1. **Aggregates Sessions**: Collect CASS sessions from multiple projects\n2. **Finds Coverage Gaps**: Identify topics with insufficient skill coverage\n3. **Extracts Universal Patterns**: Find patterns that recur across projects\n4. **Builds Knowledge Graphs**: Connect related concepts across domains\n\n---\n\n## Core Data Structures\n\n### Cross-Project Analyzer\n\n```rust\nuse std::collections::{HashMap, HashSet};\nuse std::path::PathBuf;\n\n/// Analyzes patterns across multiple projects\npub struct CrossProjectAnalyzer {\n    /// CASS client for session access\n    cass: CassClient,\n    \n    /// Registered projects\n    projects: Vec\u003cProjectInfo\u003e,\n    \n    /// Pattern extractor\n    pattern_extractor: PatternExtractor,\n    \n    /// Knowledge graph builder\n    graph_builder: KnowledgeGraphBuilder,\n}\n\n/// Information about a project for cross-project analysis\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProjectInfo {\n    /// Unique project identifier\n    pub id: String,\n    \n    /// Human-readable project name\n    pub name: String,\n    \n    /// Path to project root\n    pub path: PathBuf,\n    \n    /// Path to CASS database\n    pub cass_path: PathBuf,\n    \n    /// Project metadata\n    pub metadata: ProjectMetadata,\n    \n    /// When project was registered\n    pub registered_at: DateTime\u003cUtc\u003e,\n    \n    /// Last analysis timestamp\n    pub last_analyzed: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProjectMetadata {\n    /// Primary language(s)\n    pub languages: Vec\u003cString\u003e,\n    \n    /// Frameworks/libraries used\n    pub frameworks: Vec\u003cString\u003e,\n    \n    /// Project type (web, cli, library, etc.)\n    pub project_type: ProjectType,\n    \n    /// Size estimate\n    pub size_estimate: ProjectSize,\n    \n    /// Custom tags\n    pub tags: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ProjectType {\n    WebBackend,\n    WebFrontend,\n    FullStack,\n    Cli,\n    Library,\n    MobileApp,\n    DataPipeline,\n    Infrastructure,\n    Other(String),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ProjectSize {\n    Small,      // \u003c 10k lines\n    Medium,     // 10k - 100k lines\n    Large,      // 100k - 1M lines\n    VeryLarge,  // \u003e 1M lines\n}\n\nimpl CrossProjectAnalyzer {\n    pub fn new(cass: CassClient) -\u003e Self {\n        Self {\n            cass,\n            projects: Vec::new(),\n            pattern_extractor: PatternExtractor::new(),\n            graph_builder: KnowledgeGraphBuilder::new(),\n        }\n    }\n    \n    /// Register a project for cross-project analysis\n    pub fn register_project(\u0026mut self, project: ProjectInfo) -\u003e Result\u003c(), AnalyzerError\u003e {\n        // Validate CASS database exists\n        if !project.cass_path.exists() {\n            return Err(AnalyzerError::CassNotFound(project.cass_path.clone()));\n        }\n        \n        // Check for duplicates\n        if self.projects.iter().any(|p| p.id == project.id) {\n            return Err(AnalyzerError::DuplicateProject(project.id));\n        }\n        \n        self.projects.push(project);\n        Ok(())\n    }\n    \n    /// Analyze all registered projects\n    pub fn analyze_all(\u0026mut self) -\u003e Result\u003cCrossProjectReport, AnalyzerError\u003e {\n        let mut report = CrossProjectReport::new();\n        \n        for project in \u0026self.projects {\n            let project_analysis = self.analyze_project(project)?;\n            report.add_project_analysis(project.id.clone(), project_analysis);\n        }\n        \n        // Find cross-project patterns\n        report.universal_patterns = self.find_universal_patterns(\u0026report.project_analyses)?;\n        \n        // Build knowledge graph\n        report.knowledge_graph = self.graph_builder.build(\u0026report)?;\n        \n        // Identify coverage gaps\n        report.coverage_gaps = self.identify_coverage_gaps(\u0026report)?;\n        \n        Ok(report)\n    }\n    \n    /// Analyze a single project\n    fn analyze_project(\u0026self, project: \u0026ProjectInfo) -\u003e Result\u003cProjectAnalysis, AnalyzerError\u003e {\n        // Connect to project's CASS database\n        let cass = CassClient::connect(\u0026project.cass_path)?;\n        \n        // Get all sessions\n        let sessions = cass.list_sessions()?;\n        \n        let mut analysis = ProjectAnalysis {\n            project_id: project.id.clone(),\n            session_count: sessions.len(),\n            patterns: Vec::new(),\n            topics: Vec::new(),\n            tool_usage: HashMap::new(),\n            error_types: HashMap::new(),\n        };\n        \n        // Extract patterns from each session\n        for session in sessions {\n            let session_patterns = self.pattern_extractor.extract(\u0026session)?;\n            analysis.patterns.extend(session_patterns);\n            \n            // Track topics discussed\n            for topic in self.extract_topics(\u0026session) {\n                if !analysis.topics.contains(\u0026topic) {\n                    analysis.topics.push(topic);\n                }\n            }\n            \n            // Track tool usage\n            for tool in \u0026session.tools_used {\n                *analysis.tool_usage.entry(tool.clone()).or_insert(0) += 1;\n            }\n            \n            // Track error types encountered\n            for error in \u0026session.errors {\n                let error_type = self.categorize_error(error);\n                *analysis.error_types.entry(error_type).or_insert(0) += 1;\n            }\n        }\n        \n        Ok(analysis)\n    }\n    \n    /// Find patterns that appear across multiple projects\n    fn find_universal_patterns(\n        \u0026self,\n        analyses: \u0026HashMap\u003cString, ProjectAnalysis\u003e,\n    ) -\u003e Result\u003cVec\u003cUniversalPattern\u003e, AnalyzerError\u003e {\n        let mut pattern_occurrences: HashMap\u003cPatternSignature, Vec\u003c(String, ExtractedPattern)\u003e\u003e = HashMap::new();\n        \n        // Group patterns by signature\n        for (project_id, analysis) in analyses {\n            for pattern in \u0026analysis.patterns {\n                let signature = pattern.signature();\n                pattern_occurrences\n                    .entry(signature)\n                    .or_default()\n                    .push((project_id.clone(), pattern.clone()));\n            }\n        }\n        \n        // Filter to patterns appearing in multiple projects\n        let min_projects = 2;\n        let universal: Vec\u003cUniversalPattern\u003e = pattern_occurrences\n            .into_iter()\n            .filter(|(_, occurrences)| {\n                let unique_projects: HashSet\u003c_\u003e = occurrences.iter().map(|(p, _)| p).collect();\n                unique_projects.len() \u003e= min_projects\n            })\n            .map(|(signature, occurrences)| {\n                let projects: Vec\u003c_\u003e = occurrences.iter().map(|(p, _)| p.clone()).collect();\n                let examples: Vec\u003c_\u003e = occurrences.into_iter().map(|(_, p)| p).collect();\n                \n                UniversalPattern {\n                    signature,\n                    projects,\n                    occurrence_count: examples.len(),\n                    examples,\n                    confidence: self.calculate_pattern_confidence(\u0026signature, \u0026examples),\n                }\n            })\n            .collect();\n        \n        Ok(universal)\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProjectAnalysis {\n    pub project_id: String,\n    pub session_count: usize,\n    pub patterns: Vec\u003cExtractedPattern\u003e,\n    pub topics: Vec\u003cTopic\u003e,\n    pub tool_usage: HashMap\u003cString, u32\u003e,\n    pub error_types: HashMap\u003cErrorCategory, u32\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UniversalPattern {\n    /// Pattern signature (for deduplication)\n    pub signature: PatternSignature,\n    \n    /// Projects where this pattern was observed\n    pub projects: Vec\u003cString\u003e,\n    \n    /// Total occurrences across all projects\n    pub occurrence_count: usize,\n    \n    /// Example instances\n    pub examples: Vec\u003cExtractedPattern\u003e,\n    \n    /// Confidence in pattern universality\n    pub confidence: f64,\n}\n\n#[derive(Debug, Clone, Hash, Eq, PartialEq, Serialize, Deserialize)]\npub struct PatternSignature {\n    /// Pattern type\n    pub pattern_type: PatternType,\n    \n    /// Key concept or topic\n    pub concept: String,\n    \n    /// Language(s) involved\n    pub languages: Vec\u003cString\u003e,\n}\n```\n\n### Coverage Analyzer\n\n```rust\n/// Analyzes coverage gaps in the skill library\npub struct CoverageAnalyzer {\n    /// CASS client for session data\n    cass: CassClient,\n    \n    /// Skill registry for existing skills\n    skill_registry: Registry,\n    \n    /// Hybrid searcher for skill matching\n    search: HybridSearcher,\n}\n\nimpl CoverageAnalyzer {\n    pub fn new(cass: CassClient, skill_registry: Registry, search: HybridSearcher) -\u003e Self {\n        Self { cass, skill_registry, search }\n    }\n    \n    /// Analyze coverage across all sessions\n    pub fn analyze_coverage(\u0026self) -\u003e Result\u003cCoverageReport, CoverageError\u003e {\n        let sessions = self.cass.list_sessions()?;\n        let existing_skills = self.skill_registry.list_all()?;\n        \n        let mut report = CoverageReport::new();\n        let mut topic_occurrences: HashMap\u003cTopic, TopicStats\u003e = HashMap::new();\n        \n        for session in sessions {\n            // Extract topics from session\n            let topics = self.extract_session_topics(\u0026session)?;\n            \n            for topic in topics {\n                let stats = topic_occurrences.entry(topic.clone()).or_default();\n                stats.occurrence_count += 1;\n                stats.sessions.push(session.id.clone());\n                \n                // Check if any skill covers this topic\n                let coverage = self.find_skill_coverage(\u0026topic, \u0026existing_skills)?;\n                \n                match coverage {\n                    SkillCoverage::Full(skill_id) =\u003e {\n                        stats.covered_by.push(skill_id);\n                    }\n                    SkillCoverage::Partial { skill_id, gap } =\u003e {\n                        stats.partially_covered_by.push((skill_id, gap));\n                    }\n                    SkillCoverage::None =\u003e {\n                        stats.uncovered = true;\n                    }\n                }\n            }\n        }\n        \n        // Build gaps list\n        for (topic, stats) in topic_occurrences {\n            if stats.uncovered \u0026\u0026 stats.occurrence_count \u003e= 3 {\n                report.gaps.push(CoverageGap {\n                    topic,\n                    occurrence_count: stats.occurrence_count,\n                    example_sessions: stats.sessions.into_iter().take(5).collect(),\n                    suggested_skill: self.suggest_skill(\u0026stats)?,\n                });\n            } else if !stats.partially_covered_by.is_empty() {\n                report.partial_gaps.push(PartialCoverageGap {\n                    topic,\n                    existing_skill: stats.partially_covered_by[0].0.clone(),\n                    missing_aspects: stats.partially_covered_by.iter()\n                        .map(|(_, gap)| gap.clone())\n                        .collect(),\n                });\n            }\n        }\n        \n        // Sort by occurrence count (most important gaps first)\n        report.gaps.sort_by(|a, b| b.occurrence_count.cmp(\u0026a.occurrence_count));\n        \n        Ok(report)\n    }\n    \n    /// Find skill coverage for a topic\n    fn find_skill_coverage(\n        \u0026self,\n        topic: \u0026Topic,\n        skills: \u0026[Skill],\n    ) -\u003e Result\u003cSkillCoverage, CoverageError\u003e {\n        // Search for matching skills\n        let query = topic.to_search_query();\n        let results = self.search.search(\u0026query, 5)?;\n        \n        if results.is_empty() {\n            return Ok(SkillCoverage::None);\n        }\n        \n        let best_match = \u0026results[0];\n        \n        // Check coverage depth\n        let coverage_score = self.calculate_coverage_score(topic, \u0026best_match.skill)?;\n        \n        if coverage_score \u003e= 0.8 {\n            Ok(SkillCoverage::Full(best_match.skill.id.clone()))\n        } else if coverage_score \u003e= 0.4 {\n            let gap = self.identify_gap(topic, \u0026best_match.skill)?;\n            Ok(SkillCoverage::Partial {\n                skill_id: best_match.skill.id.clone(),\n                gap,\n            })\n        } else {\n            Ok(SkillCoverage::None)\n        }\n    }\n    \n    /// Calculate how well a skill covers a topic\n    fn calculate_coverage_score(\u0026self, topic: \u0026Topic, skill: \u0026Skill) -\u003e Result\u003cf64, CoverageError\u003e {\n        let mut score = 0.0;\n        let mut total_weight = 0.0;\n        \n        // Check concept coverage\n        for concept in \u0026topic.concepts {\n            let weight = concept.importance;\n            total_weight += weight;\n            \n            if skill.mentions_concept(\u0026concept.name) {\n                score += weight * 1.0;\n            } else if skill.mentions_related_concept(\u0026concept.name) {\n                score += weight * 0.5;\n            }\n        }\n        \n        if total_weight == 0.0 {\n            return Ok(0.0);\n        }\n        \n        Ok(score / total_weight)\n    }\n    \n    /// Identify what's missing in skill coverage\n    fn identify_gap(\u0026self, topic: \u0026Topic, skill: \u0026Skill) -\u003e Result\u003cString, CoverageError\u003e {\n        let mut missing = Vec::new();\n        \n        for concept in \u0026topic.concepts {\n            if !skill.mentions_concept(\u0026concept.name) \u0026\u0026 !skill.mentions_related_concept(\u0026concept.name) {\n                missing.push(concept.name.clone());\n            }\n        }\n        \n        Ok(missing.join(\", \"))\n    }\n    \n    /// Suggest a skill to fill the gap\n    fn suggest_skill(\u0026self, stats: \u0026TopicStats) -\u003e Result\u003cSkillSuggestion, CoverageError\u003e {\n        // TODO: Use LLM to generate skill suggestion based on session context\n        Ok(SkillSuggestion {\n            suggested_name: format!(\"{}-skill\", stats.topic.name.to_lowercase().replace(' ', \"-\")),\n            suggested_sections: vec![\"overview\", \"best-practices\", \"examples\"]\n                .into_iter()\n                .map(String::from)\n                .collect(),\n            source_sessions: stats.sessions.clone(),\n        })\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CoverageReport {\n    /// Topics with no skill coverage\n    pub gaps: Vec\u003cCoverageGap\u003e,\n    \n    /// Topics with partial skill coverage\n    pub partial_gaps: Vec\u003cPartialCoverageGap\u003e,\n    \n    /// Overall coverage statistics\n    pub stats: CoverageStats,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CoverageGap {\n    /// Topic not covered\n    pub topic: Topic,\n    \n    /// How often this topic appears\n    pub occurrence_count: u32,\n    \n    /// Example sessions where topic appeared\n    pub example_sessions: Vec\u003cSessionId\u003e,\n    \n    /// Suggested skill to fill gap\n    pub suggested_skill: SkillSuggestion,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialCoverageGap {\n    pub topic: Topic,\n    pub existing_skill: SkillId,\n    pub missing_aspects: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CoverageStats {\n    pub total_topics: u32,\n    pub fully_covered: u32,\n    pub partially_covered: u32,\n    pub uncovered: u32,\n    pub coverage_percentage: f64,\n}\n\n#[derive(Debug)]\npub enum SkillCoverage {\n    Full(SkillId),\n    Partial { skill_id: SkillId, gap: String },\n    None,\n}\n```\n\n### Knowledge Graph\n\n```rust\n/// Knowledge graph connecting concepts across projects and skills\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KnowledgeGraph {\n    /// All nodes in the graph\n    pub nodes: Vec\u003cGraphNode\u003e,\n    \n    /// All edges in the graph\n    pub edges: Vec\u003cGraphEdge\u003e,\n    \n    /// Index for fast lookup\n    #[serde(skip)]\n    node_index: HashMap\u003cNodeId, usize\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GraphNode {\n    /// Unique node identifier\n    pub id: NodeId,\n    \n    /// Node type\n    pub node_type: NodeType,\n    \n    /// Node label/name\n    pub label: String,\n    \n    /// Node properties\n    pub properties: HashMap\u003cString, String\u003e,\n    \n    /// Embedding for semantic search\n    pub embedding: Option\u003cVec\u003cf32\u003e\u003e,\n}\n\n#[derive(Debug, Clone, Hash, Eq, PartialEq, Serialize, Deserialize)]\npub struct NodeId(pub String);\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum NodeType {\n    /// A concept (e.g., \"error handling\", \"async/await\")\n    Concept,\n    \n    /// A skill\n    Skill,\n    \n    /// A project\n    Project,\n    \n    /// A technology (language, framework, tool)\n    Technology,\n    \n    /// A pattern\n    Pattern,\n    \n    /// An error category\n    ErrorCategory,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GraphEdge {\n    /// Source node\n    pub from: NodeId,\n    \n    /// Target node\n    pub to: NodeId,\n    \n    /// Edge type\n    pub edge_type: EdgeType,\n    \n    /// Edge weight (strength of relationship)\n    pub weight: f64,\n    \n    /// Edge properties\n    pub properties: HashMap\u003cString, String\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum EdgeType {\n    /// Skill covers concept\n    Covers,\n    \n    /// Concept relates to concept\n    RelatedTo,\n    \n    /// Project uses technology\n    Uses,\n    \n    /// Pattern applies to concept\n    AppliesTo,\n    \n    /// Concept is prerequisite for another\n    PrerequisiteFor,\n    \n    /// Error relates to concept\n    ErrorRelatesTo,\n    \n    /// Concept is part of broader concept\n    PartOf,\n}\n\nimpl KnowledgeGraph {\n    pub fn new() -\u003e Self {\n        Self {\n            nodes: Vec::new(),\n            edges: Vec::new(),\n            node_index: HashMap::new(),\n        }\n    }\n    \n    /// Add a node to the graph\n    pub fn add_node(\u0026mut self, node: GraphNode) {\n        let index = self.nodes.len();\n        self.node_index.insert(node.id.clone(), index);\n        self.nodes.push(node);\n    }\n    \n    /// Add an edge to the graph\n    pub fn add_edge(\u0026mut self, edge: GraphEdge) {\n        self.edges.push(edge);\n    }\n    \n    /// Find nodes related to a concept\n    pub fn find_related(\u0026self, node_id: \u0026NodeId, max_hops: u32) -\u003e Vec\u003c\u0026GraphNode\u003e {\n        let mut visited = HashSet::new();\n        let mut result = Vec::new();\n        let mut queue = vec![(node_id.clone(), 0u32)];\n        \n        while let Some((current, hops)) = queue.pop() {\n            if visited.contains(\u0026current) || hops \u003e max_hops {\n                continue;\n            }\n            visited.insert(current.clone());\n            \n            if let Some(\u0026index) = self.node_index.get(\u0026current) {\n                result.push(\u0026self.nodes[index]);\n            }\n            \n            // Find connected nodes\n            for edge in \u0026self.edges {\n                if edge.from == current \u0026\u0026 !visited.contains(\u0026edge.to) {\n                    queue.push((edge.to.clone(), hops + 1));\n                }\n                if edge.to == current \u0026\u0026 !visited.contains(\u0026edge.from) {\n                    queue.push((edge.from.clone(), hops + 1));\n                }\n            }\n        }\n        \n        result\n    }\n    \n    /// Find skills that cover a concept\n    pub fn find_covering_skills(\u0026self, concept_id: \u0026NodeId) -\u003e Vec\u003c\u0026GraphNode\u003e {\n        self.edges\n            .iter()\n            .filter(|e| e.to == *concept_id \u0026\u0026 e.edge_type == EdgeType::Covers)\n            .filter_map(|e| self.node_index.get(\u0026e.from))\n            .map(|\u0026i| \u0026self.nodes[i])\n            .collect()\n    }\n    \n    /// Find concepts not covered by any skill\n    pub fn find_uncovered_concepts(\u0026self) -\u003e Vec\u003c\u0026GraphNode\u003e {\n        let covered: HashSet\u003c_\u003e = self.edges\n            .iter()\n            .filter(|e| matches!(e.edge_type, EdgeType::Covers))\n            .map(|e| \u0026e.to)\n            .collect();\n        \n        self.nodes\n            .iter()\n            .filter(|n| matches!(n.node_type, NodeType::Concept))\n            .filter(|n| !covered.contains(\u0026n.id))\n            .collect()\n    }\n    \n    /// Find shortest path between two nodes\n    pub fn shortest_path(\u0026self, from: \u0026NodeId, to: \u0026NodeId) -\u003e Option\u003cVec\u003cNodeId\u003e\u003e {\n        use std::collections::VecDeque;\n        \n        let mut visited = HashSet::new();\n        let mut queue = VecDeque::new();\n        let mut parent: HashMap\u003cNodeId, NodeId\u003e = HashMap::new();\n        \n        queue.push_back(from.clone());\n        visited.insert(from.clone());\n        \n        while let Some(current) = queue.pop_front() {\n            if current == *to {\n                // Reconstruct path\n                let mut path = vec![current.clone()];\n                let mut node = \u0026current;\n                while let Some(p) = parent.get(node) {\n                    path.push(p.clone());\n                    node = p;\n                }\n                path.reverse();\n                return Some(path);\n            }\n            \n            for edge in \u0026self.edges {\n                let neighbor = if edge.from == current {\n                    \u0026edge.to\n                } else if edge.to == current {\n                    \u0026edge.from\n                } else {\n                    continue\n                };\n                \n                if !visited.contains(neighbor) {\n                    visited.insert(neighbor.clone());\n                    parent.insert(neighbor.clone(), current.clone());\n                    queue.push_back(neighbor.clone());\n                }\n            }\n        }\n        \n        None\n    }\n    \n    /// Export to DOT format for visualization\n    pub fn to_dot(\u0026self) -\u003e String {\n        let mut dot = String::from(\"digraph KnowledgeGraph {\\n\");\n        dot.push_str(\"  rankdir=LR;\\n\");\n        dot.push_str(\"  node [shape=box];\\n\\n\");\n        \n        // Add nodes\n        for node in \u0026self.nodes {\n            let color = match node.node_type {\n                NodeType::Concept =\u003e \"lightblue\",\n                NodeType::Skill =\u003e \"lightgreen\",\n                NodeType::Project =\u003e \"lightyellow\",\n                NodeType::Technology =\u003e \"lightpink\",\n                NodeType::Pattern =\u003e \"lavender\",\n                NodeType::ErrorCategory =\u003e \"lightsalmon\",\n            };\n            dot.push_str(\u0026format!(\n                \"  \\\"{}\\\" [label=\\\"{}\\\" fillcolor={} style=filled];\\n\",\n                node.id.0, node.label, color\n            ));\n        }\n        \n        dot.push_str(\"\\n\");\n        \n        // Add edges\n        for edge in \u0026self.edges {\n            let label = match \u0026edge.edge_type {\n                EdgeType::Covers =\u003e \"covers\",\n                EdgeType::RelatedTo =\u003e \"related\",\n                EdgeType::Uses =\u003e \"uses\",\n                EdgeType::AppliesTo =\u003e \"applies\",\n                EdgeType::PrerequisiteFor =\u003e \"prereq\",\n                EdgeType::ErrorRelatesTo =\u003e \"error\",\n                EdgeType::PartOf =\u003e \"part_of\",\n            };\n            dot.push_str(\u0026format!(\n                \"  \\\"{}\\\" -\u003e \\\"{}\\\" [label=\\\"{}\\\"];\\n\",\n                edge.from.0, edge.to.0, label\n            ));\n        }\n        \n        dot.push_str(\"}\\n\");\n        dot\n    }\n}\n\n/// Builds knowledge graph from analysis results\npub struct KnowledgeGraphBuilder {\n    /// LLM client for concept extraction\n    llm: Option\u003cLlmClient\u003e,\n}\n\nimpl KnowledgeGraphBuilder {\n    pub fn new() -\u003e Self {\n        Self { llm: None }\n    }\n    \n    /// Build knowledge graph from cross-project report\n    pub fn build(\u0026self, report: \u0026CrossProjectReport) -\u003e Result\u003cKnowledgeGraph, GraphError\u003e {\n        let mut graph = KnowledgeGraph::new();\n        \n        // Add project nodes\n        for (project_id, analysis) in \u0026report.project_analyses {\n            graph.add_node(GraphNode {\n                id: NodeId(format!(\"project:{}\", project_id)),\n                node_type: NodeType::Project,\n                label: project_id.clone(),\n                properties: HashMap::new(),\n                embedding: None,\n            });\n            \n            // Add technology nodes and edges\n            for (tech, count) in \u0026analysis.tool_usage {\n                let tech_id = NodeId(format!(\"tech:{}\", tech));\n                if !graph.node_index.contains_key(\u0026tech_id) {\n                    graph.add_node(GraphNode {\n                        id: tech_id.clone(),\n                        node_type: NodeType::Technology,\n                        label: tech.clone(),\n                        properties: HashMap::new(),\n                        embedding: None,\n                    });\n                }\n                \n                graph.add_edge(GraphEdge {\n                    from: NodeId(format!(\"project:{}\", project_id)),\n                    to: tech_id,\n                    edge_type: EdgeType::Uses,\n                    weight: *count as f64,\n                    properties: HashMap::new(),\n                });\n            }\n            \n            // Add topic nodes\n            for topic in \u0026analysis.topics {\n                let topic_id = NodeId(format!(\"concept:{}\", topic.name.to_lowercase().replace(' ', \"_\")));\n                if !graph.node_index.contains_key(\u0026topic_id) {\n                    graph.add_node(GraphNode {\n                        id: topic_id.clone(),\n                        node_type: NodeType::Concept,\n                        label: topic.name.clone(),\n                        properties: HashMap::new(),\n                        embedding: None,\n                    });\n                }\n            }\n        }\n        \n        // Add universal pattern nodes\n        for pattern in \u0026report.universal_patterns {\n            let pattern_id = NodeId(format!(\"pattern:{}\", pattern.signature.concept));\n            graph.add_node(GraphNode {\n                id: pattern_id.clone(),\n                node_type: NodeType::Pattern,\n                label: pattern.signature.concept.clone(),\n                properties: HashMap::from([\n                    (\"occurrence_count\".to_string(), pattern.occurrence_count.to_string()),\n                    (\"confidence\".to_string(), pattern.confidence.to_string()),\n                ]),\n                embedding: None,\n            });\n            \n            // Connect pattern to concept\n            let concept_id = NodeId(format!(\"concept:{}\", pattern.signature.concept.to_lowercase().replace(' ', \"_\")));\n            graph.add_edge(GraphEdge {\n                from: pattern_id,\n                to: concept_id,\n                edge_type: EdgeType::AppliesTo,\n                weight: pattern.confidence,\n                properties: HashMap::new(),\n            });\n        }\n        \n        // Add concept relationships (using LLM if available)\n        self.add_concept_relationships(\u0026mut graph)?;\n        \n        Ok(graph)\n    }\n    \n    fn add_concept_relationships(\u0026self, graph: \u0026mut KnowledgeGraph) -\u003e Result\u003c(), GraphError\u003e {\n        // Get all concept nodes\n        let concepts: Vec\u003c_\u003e = graph.nodes\n            .iter()\n            .filter(|n| matches!(n.node_type, NodeType::Concept))\n            .map(|n| (n.id.clone(), n.label.clone()))\n            .collect();\n        \n        // Add known relationships (could be enhanced with LLM)\n        let known_relationships = vec![\n            (\"error handling\", \"result type\", EdgeType::PartOf),\n            (\"error handling\", \"panic\", EdgeType::PartOf),\n            (\"async\", \"futures\", EdgeType::PartOf),\n            (\"async\", \"tokio\", EdgeType::Uses),\n            (\"testing\", \"unit tests\", EdgeType::PartOf),\n            (\"testing\", \"integration tests\", EdgeType::PartOf),\n        ];\n        \n        for (from, to, edge_type) in known_relationships {\n            let from_id = NodeId(format!(\"concept:{}\", from.replace(' ', \"_\")));\n            let to_id = NodeId(format!(\"concept:{}\", to.replace(' ', \"_\")));\n            \n            if graph.node_index.contains_key(\u0026from_id) \u0026\u0026 graph.node_index.contains_key(\u0026to_id) {\n                graph.add_edge(GraphEdge {\n                    from: from_id,\n                    to: to_id,\n                    edge_type,\n                    weight: 1.0,\n                    properties: HashMap::new(),\n                });\n            }\n        }\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms coverage`\n\n```\nAnalyze skill coverage across sessions\n\nUSAGE:\n    ms coverage [OPTIONS]\n\nOPTIONS:\n    --project \u003cPATH\u003e        Analyze specific project (can specify multiple)\n    --all-projects          Analyze all registered projects\n    --min-occurrences \u003cN\u003e   Minimum topic occurrences to report [default: 3]\n    --format \u003cFMT\u003e          Output format: text, json, markdown [default: text]\n    -v, --verbose           Show detailed analysis\n\nOUTPUT EXAMPLE:\n    Skill Coverage Analysis\n    =======================\n    \n    Overall Coverage: 73.2%\n      Fully Covered:     89 topics\n      Partially Covered: 23 topics  \n      Uncovered:         31 topics\n    \n    Top Coverage Gaps (uncovered topics):\n    \n    1. \"GraphQL Schema Design\" (12 occurrences)\n       Sessions: proj-a#12, proj-b#45, proj-c#23, ...\n       Suggested: Create skill \"graphql-schema-design\"\n       \n    2. \"Kubernetes Networking\" (9 occurrences)\n       Sessions: infra#5, infra#12, webapp#89, ...\n       Suggested: Create skill \"k8s-networking\"\n       \n    3. \"React Performance Optimization\" (8 occurrences)\n       Sessions: frontend#34, frontend#56, mobile#12, ...\n       Suggested: Create skill \"react-performance\"\n    \n    Partial Coverage Gaps:\n    \n    1. \"Error Handling\" - skill: rust-error-handling\n       Missing: async error handling, error chain patterns\n       \n    2. \"Database Migrations\" - skill: database-basics\n       Missing: rollback strategies, zero-downtime migrations\n```\n\n### `ms coverage --show-gaps`\n\n```\nShow detailed gap analysis\n\nUSAGE:\n    ms coverage --show-gaps [OPTIONS]\n\nOPTIONS:\n    --gap \u003cTOPIC\u003e           Show details for specific gap\n    --generate-skill        Generate suggested skill for gap\n    --export \u003cFILE\u003e         Export gaps to file\n\nOUTPUT EXAMPLE:\n    Coverage Gap: \"GraphQL Schema Design\"\n    =====================================\n    \n    Occurrence Count: 12\n    Projects: proj-a (5), proj-b (4), proj-c (3)\n    \n    Example Sessions:\n    \n    1. proj-a#12 (2024-01-15)\n       Context: \"How do I design a GraphQL schema for...\"\n       Duration: 45 minutes\n       Outcome: Partial success\n       \n    2. proj-b#45 (2024-01-18)\n       Context: \"Best practices for GraphQL mutations...\"\n       Duration: 30 minutes\n       Outcome: Success\n    \n    Key Concepts Extracted:\n      - Schema-first design\n      - Type relationships\n      - Custom scalars\n      - Input types vs output types\n      - Resolver patterns\n    \n    Related Existing Skills:\n      - api-design (partial match: 23%)\n      - database-schema (partial match: 15%)\n    \n    Suggested Skill Structure:\n      Name: graphql-schema-design\n      Sections:\n        - overview: GraphQL schema fundamentals\n        - types: Defining types and relationships\n        - mutations: Mutation design patterns\n        - best-practices: Schema design principles\n        - examples: Real-world schema examples\n```\n\n### `ms analyze --cross-project`\n\n```\nRun cross-project analysis\n\nUSAGE:\n    ms analyze --cross-project [OPTIONS]\n\nOPTIONS:\n    --register \u003cPATH\u003e       Register project for analysis\n    --list-projects         List registered projects\n    --unregister \u003cID\u003e       Unregister a project\n    --full-report           Generate comprehensive report\n    --graph                 Generate knowledge graph\n    --graph-format \u003cFMT\u003e    Graph format: dot, json [default: dot]\n    --patterns              Focus on universal patterns\n    --export \u003cDIR\u003e          Export analysis results\n\nEXAMPLES:\n    # Register projects\n    ms analyze --cross-project --register ~/projects/webapp\n    ms analyze --cross-project --register ~/projects/api-server\n    \n    # Run analysis\n    ms analyze --cross-project --full-report\n    \n    # Generate knowledge graph\n    ms analyze --cross-project --graph --graph-format dot \u003e knowledge.dot\n    dot -Tsvg knowledge.dot -o knowledge.svg\n    \n    # Extract universal patterns\n    ms analyze --cross-project --patterns\n\nOUTPUT EXAMPLE (patterns):\n    Universal Patterns Across Projects\n    ==================================\n    \n    1. \"Result-based Error Handling\" (Rust)\n       Projects: api-server, cli-tool, library\n       Occurrences: 47\n       Confidence: 0.92\n       \n       Pattern:\n         - Use Result\u003cT, E\u003e for recoverable errors\n         - Create custom error types with thiserror\n         - Use ? operator for propagation\n         - Map errors at boundaries\n    \n    2. \"Repository Pattern\" (Multiple)\n       Projects: webapp, api-server, data-pipeline\n       Occurrences: 23\n       Confidence: 0.85\n       \n       Pattern:\n         - Abstract data access behind trait/interface\n         - Separate domain logic from persistence\n         - Use dependency injection for testing\n```\n\n---\n\n## Project Registration\n\n```rust\nimpl CrossProjectAnalyzer {\n    /// Register a project from path\n    pub fn register_from_path(\u0026mut self, path: \u0026Path) -\u003e Result\u003cProjectInfo, AnalyzerError\u003e {\n        // Find CASS database\n        let cass_path = self.find_cass_db(path)?;\n        \n        // Detect project metadata\n        let metadata = self.detect_metadata(path)?;\n        \n        let project = ProjectInfo {\n            id: self.generate_project_id(path),\n            name: path.file_name()\n                .map(|n| n.to_string_lossy().to_string())\n                .unwrap_or_else(|| \"unknown\".to_string()),\n            path: path.to_path_buf(),\n            cass_path,\n            metadata,\n            registered_at: Utc::now(),\n            last_analyzed: None,\n        };\n        \n        self.register_project(project.clone())?;\n        \n        Ok(project)\n    }\n    \n    /// Find CASS database for a project\n    fn find_cass_db(\u0026self, path: \u0026Path) -\u003e Result\u003cPathBuf, AnalyzerError\u003e {\n        // Check common locations\n        let candidates = vec![\n            path.join(\".cass\").join(\"sessions.db\"),\n            path.join(\".claude\").join(\"cass.db\"),\n            dirs::data_local_dir()\n                .unwrap_or_default()\n                .join(\"cass\")\n                .join(\"projects\")\n                .join(path.file_name().unwrap_or_default())\n                .join(\"sessions.db\"),\n        ];\n        \n        for candidate in candidates {\n            if candidate.exists() {\n                return Ok(candidate);\n            }\n        }\n        \n        Err(AnalyzerError::CassNotFound(path.to_path_buf()))\n    }\n    \n    /// Detect project metadata from files\n    fn detect_metadata(\u0026self, path: \u0026Path) -\u003e Result\u003cProjectMetadata, AnalyzerError\u003e {\n        let mut languages = Vec::new();\n        let mut frameworks = Vec::new();\n        \n        // Check for language indicators\n        if path.join(\"Cargo.toml\").exists() {\n            languages.push(\"rust\".to_string());\n        }\n        if path.join(\"package.json\").exists() {\n            languages.push(\"javascript\".to_string());\n            languages.push(\"typescript\".to_string());\n            \n            // Check for frameworks\n            if let Ok(content) = std::fs::read_to_string(path.join(\"package.json\")) {\n                if content.contains(\"\\\"react\\\"\") {\n                    frameworks.push(\"react\".to_string());\n                }\n                if content.contains(\"\\\"vue\\\"\") {\n                    frameworks.push(\"vue\".to_string());\n                }\n                if content.contains(\"\\\"express\\\"\") {\n                    frameworks.push(\"express\".to_string());\n                }\n            }\n        }\n        if path.join(\"requirements.txt\").exists() || path.join(\"pyproject.toml\").exists() {\n            languages.push(\"python\".to_string());\n        }\n        if path.join(\"go.mod\").exists() {\n            languages.push(\"go\".to_string());\n        }\n        \n        // Detect project type\n        let project_type = self.detect_project_type(path, \u0026languages, \u0026frameworks)?;\n        \n        // Estimate size\n        let size_estimate = self.estimate_project_size(path)?;\n        \n        Ok(ProjectMetadata {\n            languages,\n            frameworks,\n            project_type,\n            size_estimate,\n            tags: Vec::new(),\n        })\n    }\n    \n    fn detect_project_type(\n        \u0026self,\n        path: \u0026Path,\n        languages: \u0026[String],\n        frameworks: \u0026[String],\n    ) -\u003e Result\u003cProjectType, AnalyzerError\u003e {\n        // Check for specific indicators\n        if frameworks.iter().any(|f| [\"react\", \"vue\", \"angular\"].contains(\u0026f.as_str())) {\n            if path.join(\"server\").exists() || path.join(\"api\").exists() {\n                return Ok(ProjectType::FullStack);\n            }\n            return Ok(ProjectType::WebFrontend);\n        }\n        \n        if frameworks.iter().any(|f| [\"express\", \"fastapi\", \"actix\", \"gin\"].contains(\u0026f.as_str())) {\n            return Ok(ProjectType::WebBackend);\n        }\n        \n        if path.join(\"src/main.rs\").exists() || path.join(\"src/main.go\").exists() {\n            // Check for web server indicators\n            if let Ok(content) = std::fs::read_to_string(path.join(\"Cargo.toml\")) {\n                if content.contains(\"actix-web\") || content.contains(\"axum\") || content.contains(\"rocket\") {\n                    return Ok(ProjectType::WebBackend);\n                }\n            }\n            return Ok(ProjectType::Cli);\n        }\n        \n        if path.join(\"lib.rs\").exists() || path.join(\"index.ts\").exists() {\n            return Ok(ProjectType::Library);\n        }\n        \n        Ok(ProjectType::Other(\"unknown\".to_string()))\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum AnalyzerError {\n    #[error(\"CASS database not found at {0}\")]\n    CassNotFound(PathBuf),\n    \n    #[error(\"Duplicate project: {0}\")]\n    DuplicateProject(String),\n    \n    #[error(\"Project not found: {0}\")]\n    ProjectNotFound(String),\n    \n    #[error(\"Database error: {0}\")]\n    DatabaseError(#[from] rusqlite::Error),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Analysis error: {0}\")]\n    AnalysisError(String),\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum CoverageError {\n    #[error(\"Search error: {0}\")]\n    SearchError(String),\n    \n    #[error(\"Registry error: {0}\")]\n    RegistryError(String),\n    \n    #[error(\"CASS error: {0}\")]\n    CassError(String),\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum GraphError {\n    #[error(\"Node not found: {0}\")]\n    NodeNotFound(String),\n    \n    #[error(\"Invalid edge: {0}\")]\n    InvalidEdge(String),\n    \n    #[error(\"Cycle detected\")]\n    CycleDetected,\n}\n```\n\n---\n\n## Dependencies\n\n- **CASS Client Integration** (meta_skill-hhu): Access to session data across projects\n- `rusqlite`: Database access\n- `serde`, `serde_json`: Serialization\n- `chrono`: Timestamps\n- `petgraph` (optional): Graph algorithms\n- `walkdir`: File system traversal","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:58:12.241581989-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:29:59.927233364-05:00","labels":["coverage","cross-project","learning","phase-6"],"dependencies":[{"issue_id":"meta_skill-8ti","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T23:04:14.72051745-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-93z","title":"[P2] RRF Score Fusion","description":"## RRF Score Fusion (Complete)\n\nReciprocal Rank Fusion (RRF) combines BM25 lexical search and vector similarity search into a single ranking. This hybrid approach captures both exact keyword matches and semantic similarity.\n\n### Algorithm\n\nRRF score for a document `d` across multiple rankings:\n\n```\nRRF(d) = Σ 1 / (k + rank_i(d))\n```\n\nWhere:\n- `k` is a constant (default: 60.0) that controls rank sensitivity\n- `rank_i(d)` is the position of document `d` in ranking `i` (1-indexed)\n- Higher `k` = more equal weighting across ranks\n- Lower `k` = top ranks dominate more\n\n### Implementation\n\n```rust\n/// Hybrid search combining BM25 and vector similarity\npub struct HybridSearcher {\n    tantivy_index: Index,\n    embedding_index: VectorIndex,\n    rrf_k: f32,  // Default: 60.0\n}\n\nimpl HybridSearcher {\n    /// Search with RRF fusion\n    pub async fn search(\n        \u0026self,\n        query: \u0026str,\n        filters: \u0026SearchFilters,\n        limit: usize,\n    ) -\u003e Result\u003cVec\u003cSearchResult\u003e\u003e {\n        // Run both searches in parallel (2x limit for fusion pool)\n        let (bm25_results, vector_results) = tokio::join\\!(\n            self.bm25_search(query, limit * 2),\n            self.vector_search(query, limit * 2),\n        );\n\n        // Compute RRF scores\n        let mut rrf_scores: HashMap\u003cString, f32\u003e = HashMap::new();\n\n        // Add BM25 ranking contributions\n        for (rank, result) in bm25_results?.iter().enumerate() {\n            let score = 1.0 / (self.rrf_k + rank as f32 + 1.0);\n            *rrf_scores.entry(result.skill_id.clone()).or_default() += score;\n        }\n\n        // Add vector ranking contributions\n        for (rank, result) in vector_results?.iter().enumerate() {\n            let score = 1.0 / (self.rrf_k + rank as f32 + 1.0);\n            *rrf_scores.entry(result.skill_id.clone()).or_default() += score;\n        }\n\n        // Apply filters and sort by fused score\n        let mut results: Vec\u003c_\u003e = rrf_scores\n            .into_iter()\n            .filter(|(id, _)| self.passes_filters(id, filters))\n            .map(|(id, score)| SearchResult { skill_id: id, score })\n            .collect();\n\n        results.sort_by(|a, b| b.score.partial_cmp(\u0026a.score).unwrap());\n        results.truncate(limit);\n\n        Ok(results)\n    }\n    \n    fn passes_filters(\u0026self, skill_id: \u0026str, filters: \u0026SearchFilters) -\u003e bool {\n        // Layer filter\n        if let Some(layer) = \u0026filters.layer {\n            if \\!self.skill_has_layer(skill_id, layer) {\n                return false;\n            }\n        }\n        \n        // Tag filter\n        if \\!filters.tags.is_empty() {\n            if \\!self.skill_has_any_tag(skill_id, \u0026filters.tags) {\n                return false;\n            }\n        }\n        \n        // Quality filter\n        if let Some(min_quality) = filters.min_quality {\n            if \\!self.skill_meets_quality(skill_id, min_quality) {\n                return false;\n            }\n        }\n        \n        // Deprecation filter (default: exclude deprecated)\n        if \\!filters.include_deprecated {\n            if self.skill_is_deprecated(skill_id) {\n                return false;\n            }\n        }\n        \n        true\n    }\n}\n```\n\n### Search Filters\n\n```rust\n#[derive(Default)]\npub struct SearchFilters {\n    pub layer: Option\u003cSkillLayer\u003e,\n    pub tags: Vec\u003cString\u003e,\n    pub min_quality: Option\u003cf32\u003e,\n    pub include_deprecated: bool,\n}\n```\n\n### RRF Breakdown (Explainability)\n\n```rust\n#[derive(Serialize)]\npub struct RrfBreakdown {\n    pub bm25_rank: Option\u003cusize\u003e,    // Position in BM25 results (None if not found)\n    pub vector_rank: Option\u003cusize\u003e,  // Position in vector results (None if not found)\n    pub rrf_score: f32,              // Final fused score\n}\n\n// Example breakdown:\n// skill \"git-workflow\":\n//   bm25_rank: 2\n//   vector_rank: 5\n//   rrf_score: 1/(60+3) + 1/(60+6) = 0.0159 + 0.0152 = 0.0311\n```\n\n### Configuration\n\n```toml\n# ms.toml\n[search]\nrrf_k = 60.0              # RRF fusion parameter\nbm25_weight = 1.0         # Weight for BM25 scores (future)\nvector_weight = 1.0       # Weight for vector scores (future)\ndefault_limit = 20        # Default result limit\n```\n\n### Why RRF?\n\n1. **Simple**: No learned weights or training required\n2. **Robust**: Works well across different query types\n3. **Explainable**: Easy to understand why results ranked\n4. **Proven**: Standard technique in hybrid search systems\n5. **Tunable**: Single `k` parameter for sensitivity control\n\n### CLI Usage\n\n```bash\n# Basic search (uses RRF internally)\nms search \"git workflow\"\n\n# With explain flag to see RRF breakdown\nms suggest --explain\n\n# Robot mode includes RRF components\nms search \"testing\" --robot | jq '.data.results[].rrf_breakdown'\n```","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:04.096281917-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:12:58.904509226-05:00","labels":["phase-2","ranking","search"],"dependencies":[{"issue_id":"meta_skill-93z","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T22:23:13.518848392-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-93z","depends_on_id":"meta_skill-ch6","type":"blocks","created_at":"2026-01-13T22:23:13.544929945-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-9ik","title":"[P3] Token Packer (Constrained Optimization)","description":"# Token Packer (Constrained Optimization)\n\n## Overview\n\nSelect an optimal subset of slices under a token budget, honoring dependencies, coverage quotas, mandatory policy slices, novelty penalties, and pack contracts.\n\n---\n\n## Tasks\n\n1. Implement constrained packer (seed coverage → greedy fill → swap improve).\n2. Enforce required slices + dependency closure.\n3. Apply novelty penalty against recently‑loaded slices.\n4. Support pack contracts (Debug/Refactor/Deploy).\n5. Provide `--explain-pack` output.\n\n---\n\n## Testing Requirements\n\n- Unit tests: budget compliance, dependency satisfaction.\n- Integration tests: pack contracts enforced.\n- Property tests: packer deterministic for same inputs.\n\n---\n\n## Acceptance Criteria\n\n- Budget never exceeded.\n- Mandatory policy slices always included.\n- Pack explanation lists inclusion/exclusion reasons.\n\n---\n\n## Dependencies\n\n- `meta_skill-0an` Micro‑Slicing Engine\n- `meta_skill-sqh` Disclosure Levels System","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:13.899489889-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:58:24.386579332-05:00","labels":["optimization","packing","phase-3"],"dependencies":[{"issue_id":"meta_skill-9ik","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T22:24:25.873226602-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-9ik","depends_on_id":"meta_skill-sqh","type":"blocks","created_at":"2026-01-13T22:24:25.900919392-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-9ok","title":"[Cross-Cutting] Testing Strategy","description":"# Testing Strategy (Cross‑Cutting)\n\n## Overview\n\nEstablish a **no‑mocks, real‑world testing philosophy** for ms. The system relies on on‑disk artifacts, SQLite, and Git archives; tests must exercise real code paths with real fixtures and detailed logging.\n\n---\n\n## Principles\n\n- **No mocks** for core logic: use real parsers, real DB, real file system.\n- **Determinism**: tests should be repeatable and environment‑stable.\n- **Observability**: every test logs inputs, outputs, timing, and errors.\n- **Coverage‑first**: all core modules must meet target coverage.\n\n---\n\n## Test Types \u0026 Ownership\n\n1. **Unit Tests** (`meta_skill-7t2`)\n   - Table‑driven + property tests (idempotence, determinism, safety).\n2. **Integration Tests** (`meta_skill-9pr`)\n   - Use temp dirs + on‑disk SQLite + Git archive.\n3. **E2E Tests** (`meta_skill-2kd`)\n   - Full CLI flows: init → index → search → load → build.\n4. **Snapshot Tests** (`meta_skill-wnk`)\n   - Stable output validation (JSON + human).\n5. **Benchmarks** (`meta_skill-ftb`)\n   - Performance for search, pack, suggest.\n6. **Skill Tests** (`meta_skill-x7k`)\n   - Validate skill content, packing, and coverage constraints.\n\n---\n\n## Required Logging Standard\n\nAll test suites must emit:\n- Test name + timestamp\n- Inputs + environment\n- Expected vs actual output\n- Timing per test\n- Failure context (stack trace + stderr capture)\n\n---\n\n## CI Integration Requirements\n\n- JUnit/TAP output for CI dashboards\n- Coverage report stored as build artifact\n- UBS gate enforced before merge\n- `cargo audit` for supply chain security\n\n---\n\n## Acceptance Criteria\n\n- All test suites defined and wired to CI.\n- Coverage ≥ 80% for core modules.\n- E2E scripts produce reproducible logs.\n- Failing tests block release.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:29:07.186502294-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:47:07.922505261-05:00","labels":["cross-cutting","quality","testing"]}
{"id":"meta_skill-9pr","title":"Integration Test Framework","description":"## Overview\n\nImplement comprehensive integration test framework for the meta_skill CLI that tests real filesystem operations, uses temp directories, and exercises full CLI workflows. This bead implements Section 18.3 of the Testing Strategy with real database state verification.\n\n## Requirements\n\n### 1. TestFixture Struct\n\nCreate `tests/integration/fixture.rs`:\n\n```rust\nuse std::path::PathBuf;\nuse std::process::Command;\nuse tempfile::TempDir;\nuse rusqlite::Connection;\n\n/// Integration test fixture providing complete test environment\npub struct TestFixture {\n    /// Root temp directory - all other paths are relative to this\n    pub temp_dir: TempDir,\n    \n    /// Config directory (~/.config/ms equivalent)\n    pub config_dir: PathBuf,\n    \n    /// Skills directory (~/.local/share/ms/skills equivalent)\n    pub skills_dir: PathBuf,\n    \n    /// Database connection for state verification\n    pub db: Option\u003cConnection\u003e,\n    \n    /// Search index path\n    pub index_path: PathBuf,\n    \n    /// Test start time for timing\n    start_time: std::time::Instant,\n    \n    /// Test name for logging\n    test_name: String,\n}\n\nimpl TestFixture {\n    /// Create a fresh test fixture\n    pub async fn new(test_name: \u0026str) -\u003e Self {\n        let start_time = std::time::Instant::now();\n        let temp_dir = TempDir::new().expect(\"Failed to create temp dir\");\n        let root = temp_dir.path();\n        \n        let config_dir = root.join(\".config/ms\");\n        let skills_dir = root.join(\".local/share/ms/skills\");\n        let index_path = root.join(\".local/share/ms/index\");\n        \n        std::fs::create_dir_all(\u0026config_dir).expect(\"Failed to create config dir\");\n        std::fs::create_dir_all(\u0026skills_dir).expect(\"Failed to create skills dir\");\n        std::fs::create_dir_all(\u0026index_path).expect(\"Failed to create index dir\");\n        \n        println!(\"\\n{'='.repeat(70)}\");\n        println!(\"[FIXTURE] Test: {}\", test_name);\n        println!(\"[FIXTURE] Root: {:?}\", root);\n        println!(\"[FIXTURE] Config: {:?}\", config_dir);\n        println!(\"[FIXTURE] Skills: {:?}\", skills_dir);\n        println!(\"[FIXTURE] Index: {:?}\", index_path);\n        println!(\"{'='.repeat(70)}\");\n        \n        Self {\n            temp_dir,\n            config_dir,\n            skills_dir,\n            index_path,\n            db: None,\n            start_time,\n            test_name: test_name.to_string(),\n        }\n    }\n    \n    /// Create fixture with pre-indexed skills\n    pub async fn with_indexed_skills(test_name: \u0026str, skills: \u0026[TestSkill]) -\u003e Self {\n        let mut fixture = Self::new(test_name).await;\n        \n        for skill in skills {\n            fixture.add_skill(skill);\n        }\n        \n        // Run ms index\n        let output = fixture.run_ms(\u0026[\"index\"]).await;\n        assert!(output.success, \"Failed to index skills: {}\", output.stderr);\n        \n        // Open database connection for verification\n        let db_path = fixture.config_dir.join(\"ms.db\");\n        if db_path.exists() {\n            fixture.db = Some(Connection::open(\u0026db_path).expect(\"Failed to open db\"));\n            println!(\"[FIXTURE] Database opened: {:?}\", db_path);\n        }\n        \n        fixture\n    }\n    \n    /// Create fixture with mock CASS integration\n    pub async fn with_mock_cass(test_name: \u0026str) -\u003e Self {\n        let fixture = Self::new(test_name).await;\n        \n        // Create mock CASS response files\n        let cass_dir = fixture.temp_dir.path().join(\"mock_cass\");\n        std::fs::create_dir_all(\u0026cass_dir).expect(\"Failed to create mock CASS dir\");\n        \n        // Create mock extraction response\n        let extraction = r#\"{\n            \"skill_name\": \"test-skill\",\n            \"description\": \"A test skill for integration testing\",\n            \"patterns\": [\"pattern1\", \"pattern2\"],\n            \"confidence\": 0.85\n        }\"#;\n        std::fs::write(cass_dir.join(\"extraction.json\"), extraction)\n            .expect(\"Failed to write mock extraction\");\n        \n        println!(\"[FIXTURE] Mock CASS configured at: {:?}\", cass_dir);\n        \n        fixture\n    }\n    \n    /// Add a skill to the test environment\n    pub fn add_skill(\u0026self, skill: \u0026TestSkill) {\n        let skill_dir = self.skills_dir.join(\u0026skill.name);\n        std::fs::create_dir_all(\u0026skill_dir).expect(\"Failed to create skill dir\");\n        \n        let skill_file = skill_dir.join(\"SKILL.md\");\n        std::fs::write(\u0026skill_file, \u0026skill.content).expect(\"Failed to write skill\");\n        \n        println!(\"[FIXTURE] Added skill: {} ({} bytes)\", skill.name, skill.content.len());\n    }\n    \n    /// Run ms CLI command and capture output\n    pub async fn run_ms(\u0026self, args: \u0026[\u0026str]) -\u003e CommandOutput {\n        let start = std::time::Instant::now();\n        \n        println!(\"\\n[CMD] ms {}\", args.join(\" \"));\n        \n        let output = Command::new(env!(\"CARGO_BIN_EXE_ms\"))\n            .args(args)\n            .env(\"MS_CONFIG_DIR\", \u0026self.config_dir)\n            .env(\"MS_DATA_DIR\", self.temp_dir.path().join(\".local/share/ms\"))\n            .env(\"HOME\", self.temp_dir.path())\n            .current_dir(self.temp_dir.path())\n            .output()\n            .expect(\"Failed to execute ms command\");\n        \n        let elapsed = start.elapsed();\n        let stdout = String::from_utf8_lossy(\u0026output.stdout).to_string();\n        let stderr = String::from_utf8_lossy(\u0026output.stderr).to_string();\n        \n        println!(\"[CMD] Exit code: {}\", output.status.code().unwrap_or(-1));\n        println!(\"[CMD] Timing: {:?}\", elapsed);\n        if !stdout.is_empty() {\n            println!(\"[STDOUT]\\n{}\", stdout);\n        }\n        if !stderr.is_empty() {\n            println!(\"[STDERR]\\n{}\", stderr);\n        }\n        \n        CommandOutput {\n            success: output.status.success(),\n            exit_code: output.status.code().unwrap_or(-1),\n            stdout,\n            stderr,\n            elapsed,\n        }\n    }\n    \n    /// Verify database state\n    pub fn verify_db_state(\u0026self, check: impl FnOnce(\u0026Connection) -\u003e bool, description: \u0026str) {\n        if let Some(ref db) = self.db {\n            let db_state_before = self.dump_db_state(db);\n            println!(\"[DB STATE BEFORE] {}\", db_state_before);\n            \n            let result = check(db);\n            assert!(result, \"Database state check failed: {}\", description);\n            \n            println!(\"[DB CHECK] {} - PASSED\", description);\n        } else {\n            println!(\"[DB CHECK] Skipped (no database connection): {}\", description);\n        }\n    }\n    \n    /// Dump database state for logging\n    fn dump_db_state(\u0026self, db: \u0026Connection) -\u003e String {\n        let mut state = String::new();\n        \n        // Count skills\n        if let Ok(count) = db.query_row::\u003ci64, _, _\u003e(\n            \"SELECT COUNT(*) FROM skills\", [], |r| r.get(0)\n        ) {\n            state.push_str(\u0026format!(\"skills={} \", count));\n        }\n        \n        // Count indexes\n        if let Ok(count) = db.query_row::\u003ci64, _, _\u003e(\n            \"SELECT COUNT(*) FROM search_index\", [], |r| r.get(0)\n        ) {\n            state.push_str(\u0026format!(\"indexed={} \", count));\n        }\n        \n        state\n    }\n}\n\nimpl Drop for TestFixture {\n    fn drop(\u0026mut self) {\n        let elapsed = self.start_time.elapsed();\n        println!(\"\\n{'='.repeat(70)}\");\n        println!(\"[FIXTURE] Test complete: {}\", self.test_name);\n        println!(\"[FIXTURE] Total time: {:?}\", elapsed);\n        println!(\"[FIXTURE] Cleaning up: {:?}\", self.temp_dir.path());\n        println!(\"{'='.repeat(70)}\\n\");\n    }\n}\n\n/// Test skill definition\npub struct TestSkill {\n    pub name: String,\n    pub content: String,\n}\n\nimpl TestSkill {\n    pub fn new(name: \u0026str, description: \u0026str) -\u003e Self {\n        let content = format!(r#\"---\nname: {}\ndescription: {}\ntags: [test]\n---\n\n# {}\n\n{}\n\"#, name, description, name, description);\n        \n        Self {\n            name: name.to_string(),\n            content,\n        }\n    }\n    \n    pub fn with_content(name: \u0026str, content: \u0026str) -\u003e Self {\n        Self {\n            name: name.to_string(),\n            content: content.to_string(),\n        }\n    }\n}\n\n/// Command output structure\npub struct CommandOutput {\n    pub success: bool,\n    pub exit_code: i32,\n    pub stdout: String,\n    pub stderr: String,\n    pub elapsed: std::time::Duration,\n}\n```\n\n### 2. CLI Command Tests\n\nCreate `tests/integration/cli_tests.rs`:\n\n```rust\nuse crate::fixture::{TestFixture, TestSkill};\n\n#[tokio::test]\nasync fn test_init_creates_config() {\n    let fixture = TestFixture::new(\"test_init_creates_config\").await;\n    \n    let output = fixture.run_ms(\u0026[\"init\"]).await;\n    \n    assert!(output.success, \"init command failed\");\n    assert!(fixture.config_dir.join(\"config.toml\").exists(), \"config.toml not created\");\n    \n    // Verify config content\n    let config_content = std::fs::read_to_string(fixture.config_dir.join(\"config.toml\"))\n        .expect(\"Failed to read config\");\n    assert!(config_content.contains(\"[general]\"), \"config missing [general] section\");\n}\n\n#[tokio::test]\nasync fn test_init_idempotent() {\n    let fixture = TestFixture::new(\"test_init_idempotent\").await;\n    \n    // Run init twice\n    let output1 = fixture.run_ms(\u0026[\"init\"]).await;\n    let output2 = fixture.run_ms(\u0026[\"init\"]).await;\n    \n    assert!(output1.success, \"first init failed\");\n    assert!(output2.success, \"second init failed\");\n    \n    // Should not error, config should exist\n    assert!(fixture.config_dir.join(\"config.toml\").exists());\n}\n\n#[tokio::test]\nasync fn test_index_empty_directory() {\n    let fixture = TestFixture::new(\"test_index_empty_directory\").await;\n    \n    let output = fixture.run_ms(\u0026[\"index\"]).await;\n    \n    // Should succeed but report 0 skills\n    assert!(output.success, \"index command failed\");\n    assert!(output.stdout.contains(\"0\") || output.stdout.contains(\"no skills\"));\n}\n\n#[tokio::test]\nasync fn test_index_with_skills() {\n    let skills = vec![\n        TestSkill::new(\"rust-error-handling\", \"Best practices for error handling in Rust\"),\n        TestSkill::new(\"git-workflow\", \"Standard git branching and merging workflow\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_index_with_skills\", \u0026skills).await;\n    \n    // Verify database has skills\n    fixture.verify_db_state(|db| {\n        let count: i64 = db.query_row(\"SELECT COUNT(*) FROM skills\", [], |r| r.get(0))\n            .unwrap_or(0);\n        count == 2\n    }, \"Should have 2 skills indexed\");\n}\n\n#[tokio::test]\nasync fn test_list_shows_indexed_skills() {\n    let skills = vec![\n        TestSkill::new(\"test-skill-1\", \"First test skill\"),\n        TestSkill::new(\"test-skill-2\", \"Second test skill\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_list_shows_indexed_skills\", \u0026skills).await;\n    \n    let output = fixture.run_ms(\u0026[\"list\"]).await;\n    \n    assert!(output.success, \"list command failed\");\n    assert!(output.stdout.contains(\"test-skill-1\"), \"Missing skill-1 in output\");\n    assert!(output.stdout.contains(\"test-skill-2\"), \"Missing skill-2 in output\");\n}\n\n#[tokio::test]\nasync fn test_show_skill_details() {\n    let skills = vec![\n        TestSkill::new(\"detailed-skill\", \"A skill with detailed information\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_show_skill_details\", \u0026skills).await;\n    \n    let output = fixture.run_ms(\u0026[\"show\", \"detailed-skill\"]).await;\n    \n    assert!(output.success, \"show command failed\");\n    assert!(output.stdout.contains(\"detailed-skill\"));\n    assert!(output.stdout.contains(\"detailed information\"));\n}\n\n#[tokio::test]\nasync fn test_show_nonexistent_skill() {\n    let fixture = TestFixture::new(\"test_show_nonexistent_skill\").await;\n    \n    let output = fixture.run_ms(\u0026[\"show\", \"nonexistent-skill\"]).await;\n    \n    assert!(!output.success, \"show should fail for nonexistent skill\");\n    assert!(output.stderr.contains(\"not found\") || output.exit_code != 0);\n}\n\n#[tokio::test]\nasync fn test_search_finds_matching_skills() {\n    let skills = vec![\n        TestSkill::new(\"rust-async\", \"Asynchronous programming patterns in Rust\"),\n        TestSkill::new(\"python-async\", \"Async/await patterns in Python\"),\n        TestSkill::new(\"git-basics\", \"Basic git commands and workflow\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_search_finds_matching_skills\", \u0026skills).await;\n    \n    let output = fixture.run_ms(\u0026[\"search\", \"async\"]).await;\n    \n    assert!(output.success, \"search command failed\");\n    assert!(output.stdout.contains(\"rust-async\"), \"Missing rust-async in results\");\n    assert!(output.stdout.contains(\"python-async\"), \"Missing python-async in results\");\n    assert!(!output.stdout.contains(\"git-basics\"), \"git-basics should not match 'async'\");\n}\n```\n\n### 3. Database State Verification\n\n```rust\n/// Detailed database state checker\npub struct DbStateChecker\u003c'a\u003e {\n    db: \u0026'a Connection,\n}\n\nimpl\u003c'a\u003e DbStateChecker\u003c'a\u003e {\n    pub fn new(db: \u0026'a Connection) -\u003e Self {\n        Self { db }\n    }\n    \n    pub fn skill_count(\u0026self) -\u003e i64 {\n        self.db.query_row(\"SELECT COUNT(*) FROM skills\", [], |r| r.get(0))\n            .unwrap_or(0)\n    }\n    \n    pub fn skill_exists(\u0026self, name: \u0026str) -\u003e bool {\n        self.db.query_row(\n            \"SELECT 1 FROM skills WHERE name = ?\",\n            [name],\n            |_| Ok(true)\n        ).unwrap_or(false)\n    }\n    \n    pub fn skill_indexed(\u0026self, name: \u0026str) -\u003e bool {\n        self.db.query_row(\n            \"SELECT 1 FROM search_index WHERE skill_name = ?\",\n            [name],\n            |_| Ok(true)\n        ).unwrap_or(false)\n    }\n    \n    pub fn log_full_state(\u0026self) {\n        println!(\"\\n[DB FULL STATE]\");\n        println!(\"  Skills: {}\", self.skill_count());\n        \n        // List all skills\n        if let Ok(mut stmt) = self.db.prepare(\"SELECT name, description FROM skills\") {\n            if let Ok(rows) = stmt.query_map([], |row| {\n                Ok((row.get::\u003c_, String\u003e(0)?, row.get::\u003c_, String\u003e(1)?))\n            }) {\n                for row in rows.flatten() {\n                    println!(\"    - {}: {}\", row.0, row.1);\n                }\n            }\n        }\n    }\n}\n```\n\n### 4. Logging Requirements\n\nEvery integration test must log:\n- Command executed with full arguments\n- Exit code\n- stdout (separate from stderr)\n- stderr (separate from stdout)\n- Timing for each command\n- Database state before operation\n- Database state after operation\n\n### 5. Test Organization\n\n```\ntests/\n├── integration/\n│   ├── mod.rs\n│   ├── fixture.rs\n│   ├── cli_tests.rs\n│   │   ├── init_tests\n│   │   ├── index_tests\n│   │   ├── list_tests\n│   │   ├── show_tests\n│   │   └── search_tests\n│   ├── workflow_tests.rs\n│   │   ├── full_workflow_test\n│   │   └── error_recovery_test\n│   └── db_state_tests.rs\n```\n\n## Acceptance Criteria\n\n1. [ ] TestFixture struct implemented with all methods\n2. [ ] with_indexed_skills() creates pre-populated test environment\n3. [ ] with_mock_cass() configures CASS mock responses\n4. [ ] All CLI commands have integration tests (init, index, list, show, search)\n5. [ ] Database state verification after each operation\n6. [ ] Detailed logging for all commands and state changes\n7. [ ] Tests use real filesystem (no mocks)\n8. [ ] Tests properly clean up temp directories\n9. [ ] Tests are isolated and can run in parallel\n10. [ ] All tests pass in CI environment\n\n## Dependencies\n\n- meta_skill-14h (CLI Commands) - commands must exist to test","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T22:55:17.640206042-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.833121835-05:00","labels":["framework","integration-tests","testing"],"dependencies":[{"issue_id":"meta_skill-9pr","depends_on_id":"meta_skill-14h","type":"blocks","created_at":"2026-01-13T22:55:22.372589829-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-9pr","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.152207328-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-9r9","title":"[P4] Specific-to-General Transformation","description":"# Specific‑to‑General Transformation\n\n## Overview\n\nTransform extracted specific instances into reusable, generalized rules. This step must *avoid over‑generalization* and feed low‑confidence outputs into the Uncertainty Queue.\n\n---\n\n## Core Workflow\n\n1. **Collect instances** for a pattern cluster.\n2. **Extract invariants** (what must always hold).\n3. **Abstract variables** (project names, file paths, versions).\n4. **Generate rule** (clear, reusable).\n5. **Critique pass** (LLM‑assisted self‑review).\n6. **Validate** against held‑out examples.\n\n---\n\n## Tasks\n\n- Implement LLM‑assisted generalization (with heuristic fallback).\n- Add critique round to detect over‑generalization.\n- Emit confidence score + evidence count.\n- Push low‑confidence results into Uncertainty Queue.\n\n---\n\n## Testing Requirements\n\n- Unit tests for placeholder substitution + invariants.\n- Integration tests: cluster → generalized rule → validation.\n- Regression tests for known over‑generalization cases.\n- Determinism tests for heuristic fallback.\n\n---\n\n## Acceptance Criteria\n\n- Generalized rules preserve constraints from evidence.\n- Over‑generalization detected and quarantined.\n- All outputs carry confidence + evidence counts.\n\n---\n\n## Dependencies\n\n- `meta_skill-237` Pattern Extraction Pipeline\n- `meta_skill-4g1` Uncertainty Queue (for low‑confidence outputs)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:46.382724536-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:53:07.780437611-05:00","labels":["llm","phase-4","transformation"],"dependencies":[{"issue_id":"meta_skill-9r9","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:12.965845876-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-aku","title":"CASS Mining: Security Vulnerability Assessment","description":"Deep dive into security vulnerability assessment patterns, API secret exposure, MFA implementation review, authentication patterns. Extract actionable skill patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:16.142464402-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:30:17.927653105-05:00","closed_at":"2026-01-13T18:30:17.927653105-05:00","close_reason":"Section 32 added: Security Vulnerability Assessment Patterns (~1,450 lines covering OWASP categories, crypto security, input validation, authentication, rate limiting, secret management)","labels":["cass-mining"]}
{"id":"meta_skill-ans","title":"[P4] Redaction Pipeline","description":"# Redaction Pipeline\n\n## Overview\n\nStrip secrets/PII from sessions **before** any extraction or storage. Redaction must be deterministic, auditable, and resistant to re‑assembly leaks across multiple excerpts.\n\n---\n\n## Key Requirements\n\n- Detect secrets, tokens, emails, internal hostnames, filesystem paths.\n- Emit **taint labels** for unsafe sources.\n- Support **reassembly resistance**: partial redactions across evidence must not reconstruct the original secret.\n- Preserve minimal safe excerpts for audit.\n\n---\n\n## Tasks\n\n1. Define `RedactionRule` + `SecretType` taxonomy.\n2. Implement regex + entropy‑based detectors.\n3. Apply redaction to all session content prior to mining.\n4. Track taint propagation into evidence.\n5. Store redaction reports in SQLite for audit.\n\n---\n\n## Testing Requirements\n\n- Unit tests for regex detectors and entropy thresholds.\n- Property tests: redaction idempotence.\n- Integration tests: session → redacted output → no secret leakage.\n- Regression tests for known secret formats (GitHub, OpenAI, AWS, SSH keys).\n\n---\n\n## Acceptance Criteria\n\n- No secret/PII appears in skills or logs.\n- Redaction reports record what/where was removed.\n- Reassembly resistance holds across multiple excerpts.\n\n---\n\n## Dependencies\n\n- `meta_skill-fma` Prompt Injection Defense (shared safety filters)\n- `meta_skill-qs1` SQLite Database Layer (redaction reports)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:49.71630535-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:52:34.197150939-05:00","labels":["phase-4","redaction","security"],"dependencies":[{"issue_id":"meta_skill-ans","depends_on_id":"meta_skill-fma","type":"blocks","created_at":"2026-01-13T22:57:37.229150881-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ans","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:52:43.076550135-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-avs","title":"CASS Mining: Refactoring Patterns","description":"Deep dive into CLI refactoring patterns, clippy-driven improvements, code modernization workflows.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:39.674096363-05:00","created_by":"ubuntu","updated_at":"2026-01-13T21:39:59.283028347-05:00","closed_at":"2026-01-13T21:39:59.283028347-05:00","close_reason":"Completed Section 38: Refactoring Patterns. Covered clippy-driven refactoring, dead code removal, function extraction, code organization patterns, consistency improvements, defensive refactoring, and type system improvements. ~280 lines added to PLAN_TO_MAKE_METASKILL_CLI.md.","labels":["cass-mining"]}
{"id":"meta_skill-b98","title":"[P1] Git Archive Layer","description":"## Overview\n\nImplement the Git archive layer for human-readable persistence. This works in tandem with SQLite (dual persistence) to provide version history, audit trail, and git-based synchronization. Skills are stored as YAML metadata + markdown files in a git repository.\n\n## Background \u0026 Rationale\n\n### Why Git Archive\n\n1. **Human-Readable**: Skills visible as files, not opaque database\n2. **Version History**: Full history of all skill changes\n3. **Conflict Resolution**: Git's merge tooling for multi-machine sync\n4. **Audit Trail**: Every change is a commit with author/timestamp\n5. **Collaboration**: Standard git workflows (PR, review, merge)\n6. **Backup**: Clone = complete backup\n\n### Dual Persistence Strategy\n\n- **SQLite**: Fast queries, FTS5 search, embeddings, runtime cache\n- **Git Archive**: Source of truth, version history, human editing\n- **Two-Phase Commit**: Ensures both stores stay consistent (see meta_skill-fus)\n\n---\n\n## Git Archive Structure (from Plan Section 3.3)\n\nThis is the authoritative archive layout from the big plan.\n\n```\n~/.local/share/ms/archive/\n├── .git/\n├── skills/\n│   ├── by-id/\n│   │   ├── ntm/\n│   │   │   ├── metadata.yaml         # Skill metadata (YAML frontmatter)\n│   │   │   ├── skill.spec.json       # Deterministic source-of-truth\n│   │   │   ├── spec.lens.json        # Block ID → byte range mapping\n│   │   │   ├── SKILL.md              # Compiled markdown (rendered view)\n│   │   │   ├── evidence.json         # Rule-level evidence index\n│   │   │   ├── evidence/             # Expanded evidence files\n│   │   │   │   ├── rule-1.md\n│   │   │   │   └── rule-3.md\n│   │   │   ├── slices.json           # Pre-computed slice index\n│   │   │   ├── tests/                # Skill-specific tests\n│   │   │   │   └── basic.yaml\n│   │   │   └── usage-log.jsonl       # Usage tracking (append-only)\n│   │   └── planning-workflow/\n│   │       ├── metadata.yaml\n│   │       ├── skill.spec.json\n│   │       ├── spec.lens.json\n│   │       ├── SKILL.md\n│   │       ├── evidence.json\n│   │       ├── slices.json\n│   │       └── usage-log.jsonl\n│   └── by-source/\n│       └── agent_flywheel_clawdbot_skills_and_integrations/\n│           └── ... (symlinks or copies)\n├── builds/\n│   ├── session-abc123/\n│   │   ├── manifest.yaml             # Build session metadata\n│   │   ├── patterns.md               # Extracted patterns\n│   │   ├── evidence.json             # Evidence references\n│   │   ├── redaction-report.json     # What was redacted\n│   │   ├── skill.spec.json           # Generated spec\n│   │   ├── spec.lens.json            # Lens for editing\n│   │   ├── draft-v1.md               # Iteration drafts\n│   │   ├── draft-v2.md\n│   │   └── final.md                  # Published version\n│   └── session-def456/\n│       └── ...\n├── bundles/\n│   └── published/\n│       └── ...\n└── README.md\n```\n\n---\n\n## Key Data Structures\n\n### GitArchive Core\n\n```rust\nuse git2::{Repository, Commit, Oid, Signature};\nuse std::path::{Path, PathBuf};\nuse serde::{Serialize, Deserialize};\n\n/// Git-based skill archive\npub struct GitArchive {\n    /// Git repository handle\n    repo: Repository,\n    /// Archive root path\n    root: PathBuf,\n    /// Git author signature\n    signature: Signature\u003c'static\u003e,\n}\n\nimpl GitArchive {\n    /// Open existing archive or initialize new one\n    pub fn open(path: impl AsRef\u003cPath\u003e) -\u003e Result\u003cSelf\u003e {\n        let path = path.as_ref();\n        let repo = if path.join(\".git\").exists() {\n            Repository::open(path)?\n        } else {\n            Self::init(path)?\n        };\n        \n        let signature = Self::get_signature(\u0026repo)?;\n        \n        Ok(Self {\n            repo,\n            root: path.to_path_buf(),\n            signature,\n        })\n    }\n    \n    /// Initialize new archive with standard structure\n    fn init(path: \u0026Path) -\u003e Result\u003cRepository\u003e {\n        std::fs::create_dir_all(path)?;\n        let repo = Repository::init(path)?;\n        \n        // Create directory structure\n        for dir in \u0026[\"skills/by-id\", \"skills/by-source\", \"builds\", \"bundles/published\"] {\n            std::fs::create_dir_all(path.join(dir))?;\n        }\n        \n        // Create README\n        std::fs::write(\n            path.join(\"README.md\"),\n            \"# ms skill archive\\n\\nManaged by ms CLI. Do not edit directly.\\n\",\n        )?;\n        \n        // Initial commit\n        let mut index = repo.index()?;\n        index.add_path(Path::new(\"README.md\"))?;\n        index.write()?;\n        \n        let tree_id = index.write_tree()?;\n        let tree = repo.find_tree(tree_id)?;\n        let sig = repo.signature()?;\n        \n        repo.commit(Some(\"HEAD\"), \u0026sig, \u0026sig, \"Initialize ms archive\", \u0026tree, \u0026[])?;\n        \n        Ok(repo)\n    }\n    \n    /// Get or create git signature\n    fn get_signature(repo: \u0026Repository) -\u003e Result\u003cSignature\u003c'static\u003e\u003e {\n        // Try to get from git config, fall back to defaults\n        match repo.signature() {\n            Ok(sig) =\u003e Ok(Signature::now(\n                sig.name().unwrap_or(\"ms\"),\n                sig.email().unwrap_or(\"ms@local\"),\n            )?),\n            Err(_) =\u003e Ok(Signature::now(\"ms\", \"ms@local\")?),\n        }\n    }\n}\n```\n\n### Skill Commit Operations\n\n```rust\nimpl GitArchive {\n    /// Write skill files and commit\n    pub fn write_skill(\u0026self, spec: \u0026SkillSpec) -\u003e Result\u003cSkillCommit\u003e {\n        let skill_dir = self.root.join(\"skills/by-id\").join(\u0026spec.id);\n        std::fs::create_dir_all(\u0026skill_dir)?;\n        \n        // Write metadata.yaml\n        let metadata_path = skill_dir.join(\"metadata.yaml\");\n        let metadata_yaml = serde_yaml::to_string(\u0026spec.metadata)?;\n        std::fs::write(\u0026metadata_path, metadata_yaml)?;\n        \n        // Write skill.spec.json (source of truth)\n        let spec_path = skill_dir.join(\"skill.spec.json\");\n        let spec_json = serde_json::to_string_pretty(spec)?;\n        std::fs::write(\u0026spec_path, spec_json)?;\n        \n        // Compile and write SKILL.md\n        let skill_md = SkillCompiler::compile(spec, CompileTarget::Claude)?;\n        let md_path = skill_dir.join(\"SKILL.md\");\n        std::fs::write(\u0026md_path, \u0026skill_md)?;\n        \n        // Write spec.lens.json (byte range mapping)\n        let lens = SpecLens::from_compiled(\u0026skill_md, spec)?;\n        let lens_path = skill_dir.join(\"spec.lens.json\");\n        let lens_json = serde_json::to_string_pretty(\u0026lens)?;\n        std::fs::write(\u0026lens_path, lens_json)?;\n        \n        // Write evidence.json if present\n        if \\!spec.evidence.rules.is_empty() {\n            let evidence_path = skill_dir.join(\"evidence.json\");\n            let evidence_json = serde_json::to_string_pretty(\u0026spec.evidence)?;\n            std::fs::write(\u0026evidence_path, evidence_json)?;\n        }\n        \n        // Write slices.json\n        let slices = SkillSlicer::slice(spec)?;\n        let slices_path = skill_dir.join(\"slices.json\");\n        let slices_json = serde_json::to_string_pretty(\u0026slices)?;\n        std::fs::write(\u0026slices_path, slices_json)?;\n        \n        // Stage and commit\n        let commit = self.commit_skill(\u0026spec.id, \"Update skill\")?;\n        \n        Ok(commit)\n    }\n    \n    /// Commit staged skill changes\n    fn commit_skill(\u0026self, skill_id: \u0026str, message: \u0026str) -\u003e Result\u003cSkillCommit\u003e {\n        let mut index = self.repo.index()?;\n        \n        // Add all files in skill directory\n        let skill_dir = format\\!(\"skills/by-id/{}\", skill_id);\n        index.add_all([\u0026skill_dir], git2::IndexAddOption::DEFAULT, None)?;\n        index.write()?;\n        \n        let tree_id = index.write_tree()?;\n        let tree = self.repo.find_tree(tree_id)?;\n        \n        let parent = self.repo.head()?.peel_to_commit()?;\n        let full_message = format\\!(\"{}: {}\", skill_id, message);\n        \n        let oid = self.repo.commit(\n            Some(\"HEAD\"),\n            \u0026self.signature,\n            \u0026self.signature,\n            \u0026full_message,\n            \u0026tree,\n            \u0026[\u0026parent],\n        )?;\n        \n        Ok(SkillCommit {\n            oid: oid.to_string(),\n            skill_id: skill_id.to_string(),\n            message: full_message,\n            timestamp: chrono::Utc::now(),\n        })\n    }\n    \n    /// Read skill from archive\n    pub fn read_skill(\u0026self, skill_id: \u0026str) -\u003e Result\u003cSkillSpec\u003e {\n        let spec_path = self.root\n            .join(\"skills/by-id\")\n            .join(skill_id)\n            .join(\"skill.spec.json\");\n        \n        if \\!spec_path.exists() {\n            return Err(MsError::SkillNotFound(skill_id.to_string()));\n        }\n        \n        let spec_json = std::fs::read_to_string(\u0026spec_path)?;\n        let spec: SkillSpec = serde_json::from_str(\u0026spec_json)?;\n        \n        Ok(spec)\n    }\n    \n    /// Delete skill from archive\n    pub fn delete_skill(\u0026self, skill_id: \u0026str) -\u003e Result\u003cSkillCommit\u003e {\n        let skill_dir = self.root.join(\"skills/by-id\").join(skill_id);\n        \n        if \\!skill_dir.exists() {\n            return Err(MsError::SkillNotFound(skill_id.to_string()));\n        }\n        \n        // Remove directory\n        std::fs::remove_dir_all(\u0026skill_dir)?;\n        \n        // Stage deletion and commit\n        let mut index = self.repo.index()?;\n        let skill_pattern = format\\!(\"skills/by-id/{}/*\", skill_id);\n        index.remove_all([\u0026skill_pattern], None)?;\n        index.write()?;\n        \n        let tree_id = index.write_tree()?;\n        let tree = self.repo.find_tree(tree_id)?;\n        let parent = self.repo.head()?.peel_to_commit()?;\n        \n        let message = format\\!(\"{}: Delete skill\", skill_id);\n        let oid = self.repo.commit(\n            Some(\"HEAD\"),\n            \u0026self.signature,\n            \u0026self.signature,\n            \u0026message,\n            \u0026tree,\n            \u0026[\u0026parent],\n        )?;\n        \n        Ok(SkillCommit {\n            oid: oid.to_string(),\n            skill_id: skill_id.to_string(),\n            message,\n            timestamp: chrono::Utc::now(),\n        })\n    }\n}\n\n/// Commit record for skill operations\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillCommit {\n    pub oid: String,\n    pub skill_id: String,\n    pub message: String,\n    pub timestamp: DateTime\u003cUtc\u003e,\n}\n```\n\n### Build Session Archive\n\n```rust\nimpl GitArchive {\n    /// Write build session to archive\n    pub fn write_build_session(\u0026self, session: \u0026BuildSession) -\u003e Result\u003c()\u003e {\n        let session_dir = self.root.join(\"builds\").join(\u0026session.id);\n        std::fs::create_dir_all(\u0026session_dir)?;\n        \n        // Write manifest.yaml\n        let manifest = BuildManifest {\n            id: session.id.clone(),\n            name: session.name.clone(),\n            status: session.status.clone(),\n            created_at: session.created_at,\n            updated_at: session.updated_at,\n            cass_queries: session.cass_queries.clone(),\n        };\n        let manifest_yaml = serde_yaml::to_string(\u0026manifest)?;\n        std::fs::write(session_dir.join(\"manifest.yaml\"), manifest_yaml)?;\n        \n        // Write patterns.md (human-readable)\n        let patterns_md = format_patterns_as_markdown(\u0026session.patterns)?;\n        std::fs::write(session_dir.join(\"patterns.md\"), patterns_md)?;\n        \n        // Write evidence.json\n        if let Some(evidence) = \u0026session.evidence {\n            let evidence_json = serde_json::to_string_pretty(evidence)?;\n            std::fs::write(session_dir.join(\"evidence.json\"), evidence_json)?;\n        }\n        \n        // Write skill.spec.json if present\n        if let Some(spec) = \u0026session.skill_spec {\n            let spec_json = serde_json::to_string_pretty(spec)?;\n            std::fs::write(session_dir.join(\"skill.spec.json\"), spec_json)?;\n        }\n        \n        // Write draft if present\n        if let Some(draft) = \u0026session.draft_skill {\n            let draft_path = session_dir.join(format\\!(\"draft-v{}.md\", session.iteration_count));\n            std::fs::write(\u0026draft_path, draft)?;\n        }\n        \n        // Commit\n        self.commit_build_session(\u0026session.id)?;\n        \n        Ok(())\n    }\n    \n    fn commit_build_session(\u0026self, session_id: \u0026str) -\u003e Result\u003cOid\u003e {\n        let mut index = self.repo.index()?;\n        let session_dir = format\\!(\"builds/{}\", session_id);\n        index.add_all([\u0026session_dir], git2::IndexAddOption::DEFAULT, None)?;\n        index.write()?;\n        \n        let tree_id = index.write_tree()?;\n        let tree = self.repo.find_tree(tree_id)?;\n        let parent = self.repo.head()?.peel_to_commit()?;\n        \n        let message = format\\!(\"build/{}: Update session\", session_id);\n        let oid = self.repo.commit(\n            Some(\"HEAD\"),\n            \u0026self.signature,\n            \u0026self.signature,\n            \u0026message,\n            \u0026tree,\n            \u0026[\u0026parent],\n        )?;\n        \n        Ok(oid)\n    }\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct BuildManifest {\n    pub id: String,\n    pub name: String,\n    pub status: String,\n    pub created_at: DateTime\u003cUtc\u003e,\n    pub updated_at: DateTime\u003cUtc\u003e,\n    pub cass_queries: Vec\u003cString\u003e,\n}\n```\n\n### Sync Operations\n\n```rust\nimpl GitArchive {\n    /// Pull changes from remote\n    pub fn pull(\u0026self, remote: \u0026str) -\u003e Result\u003cPullResult\u003e {\n        let mut remote = self.repo.find_remote(remote)?;\n        remote.fetch(\u0026[\"main\"], None, None)?;\n        \n        let fetch_head = self.repo.find_reference(\"FETCH_HEAD\")?;\n        let fetch_commit = fetch_head.peel_to_commit()?;\n        \n        let head = self.repo.head()?.peel_to_commit()?;\n        \n        // Fast-forward if possible\n        let analysis = self.repo.merge_analysis(\u0026[\u0026fetch_commit.as_object().as_annotated_commit()])?;\n        \n        if analysis.0.is_fast_forward() {\n            let refname = \"refs/heads/main\";\n            let mut reference = self.repo.find_reference(refname)?;\n            reference.set_target(fetch_commit.id(), \"pull: fast-forward\")?;\n            self.repo.set_head(refname)?;\n            self.repo.checkout_head(Some(git2::build::CheckoutBuilder::default().force()))?;\n            \n            Ok(PullResult::FastForward)\n        } else if analysis.0.is_up_to_date() {\n            Ok(PullResult::UpToDate)\n        } else {\n            // Merge required\n            Ok(PullResult::MergeRequired)\n        }\n    }\n    \n    /// Push changes to remote\n    pub fn push(\u0026self, remote: \u0026str) -\u003e Result\u003c()\u003e {\n        let mut remote = self.repo.find_remote(remote)?;\n        remote.push(\u0026[\"refs/heads/main:refs/heads/main\"], None)?;\n        Ok(())\n    }\n    \n    /// List skills modified since commit\n    pub fn skills_modified_since(\u0026self, since_oid: \u0026str) -\u003e Result\u003cVec\u003cString\u003e\u003e {\n        let since = Oid::from_str(since_oid)?;\n        let since_commit = self.repo.find_commit(since)?;\n        let head_commit = self.repo.head()?.peel_to_commit()?;\n        \n        let mut skills = Vec::new();\n        let diff = self.repo.diff_tree_to_tree(\n            Some(\u0026since_commit.tree()?),\n            Some(\u0026head_commit.tree()?),\n            None,\n        )?;\n        \n        diff.foreach(\n            \u0026mut |delta, _| {\n                if let Some(path) = delta.new_file().path() {\n                    if let Some(skill_id) = extract_skill_id_from_path(path) {\n                        if \\!skills.contains(\u0026skill_id) {\n                            skills.push(skill_id);\n                        }\n                    }\n                }\n                true\n            },\n            None,\n            None,\n            None,\n        )?;\n        \n        Ok(skills)\n    }\n}\n\npub enum PullResult {\n    FastForward,\n    UpToDate,\n    MergeRequired,\n}\n\nfn extract_skill_id_from_path(path: \u0026Path) -\u003e Option\u003cString\u003e {\n    let components: Vec\u003c_\u003e = path.components().collect();\n    if components.len() \u003e= 3 \n        \u0026\u0026 components[0].as_os_str() == \"skills\" \n        \u0026\u0026 components[1].as_os_str() == \"by-id\" \n    {\n        Some(components[2].as_os_str().to_string_lossy().to_string())\n    } else {\n        None\n    }\n}\n```\n\n---\n\n## Tasks\n\n### Task 1: GitArchive Initialization\n- [ ] Create src/storage/git.rs module\n- [ ] Implement GitArchive::open()\n- [ ] Implement GitArchive::init() with directory structure\n- [ ] Handle missing .git directory gracefully\n- [ ] Create default README.md on init\n\n### Task 2: Signature Handling\n- [ ] Read from git config if available\n- [ ] Fall back to sensible defaults (\"ms\", \"ms@local\")\n- [ ] Support custom signature via config\n\n### Task 3: Skill Write Operations\n- [ ] Implement write_skill() with all files\n- [ ] Write metadata.yaml\n- [ ] Write skill.spec.json\n- [ ] Compile and write SKILL.md\n- [ ] Write spec.lens.json\n- [ ] Write evidence.json (if present)\n- [ ] Write slices.json\n\n### Task 4: Skill Read Operations\n- [ ] Implement read_skill() from spec.json\n- [ ] Implement list_skills() returning all skill IDs\n- [ ] Implement skill_exists() check\n- [ ] Handle missing files gracefully\n\n### Task 5: Skill Delete Operations\n- [ ] Implement delete_skill() with confirmation\n- [ ] Remove directory recursively\n- [ ] Stage deletion in git index\n- [ ] Commit with descriptive message\n\n### Task 6: Build Session Operations\n- [ ] Implement write_build_session()\n- [ ] Write manifest.yaml\n- [ ] Write patterns.md (human-readable)\n- [ ] Write draft versions\n- [ ] Implement read_build_session()\n\n### Task 7: Commit Operations\n- [ ] Implement commit_skill() with staging\n- [ ] Implement commit_build_session()\n- [ ] Use consistent commit message format\n- [ ] Return SkillCommit with metadata\n\n### Task 8: Sync Operations\n- [ ] Implement pull() with fast-forward detection\n- [ ] Implement push() to remote\n- [ ] Implement skills_modified_since() for incremental sync\n- [ ] Handle merge conflicts (return MergeRequired)\n\n### Task 9: Integration with 2PC\n- [ ] Prepare phase: Write files but don't commit\n- [ ] Commit phase: Git commit after SQLite success\n- [ ] Rollback: Revert file changes on failure\n\n---\n\n## Acceptance Criteria\n\n1. **Archive Initializes**: New archive created with correct structure\n2. **Skills Write**: skill.spec.json, SKILL.md, metadata.yaml all created\n3. **Git History**: Every write creates a commit\n4. **Skills Read**: Can read back written skills\n5. **Skills Delete**: Removal reflected in git history\n6. **Sync Works**: Pull and push operations succeed\n7. **Build Sessions**: Build artifacts stored correctly\n8. **Integration**: Works with TxManager for 2PC\n\n---\n\n## Testing Requirements\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n\n    #[test]\n    fn test_archive_init() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        assert\\!(dir.path().join(\".git\").exists());\n        assert\\!(dir.path().join(\"skills/by-id\").exists());\n        assert\\!(dir.path().join(\"builds\").exists());\n        assert\\!(dir.path().join(\"README.md\").exists());\n    }\n\n    #[test]\n    fn test_skill_write_read() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let spec = SkillSpec {\n            id: \"test-skill\".to_string(),\n            // ... populate fields\n        };\n        \n        archive.write_skill(\u0026spec).unwrap();\n        \n        let skill_dir = dir.path().join(\"skills/by-id/test-skill\");\n        assert\\!(skill_dir.join(\"skill.spec.json\").exists());\n        assert\\!(skill_dir.join(\"SKILL.md\").exists());\n        assert\\!(skill_dir.join(\"metadata.yaml\").exists());\n        \n        let read_spec = archive.read_skill(\"test-skill\").unwrap();\n        assert_eq\\!(read_spec.id, \"test-skill\");\n    }\n\n    #[test]\n    fn test_git_history() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let spec = SkillSpec { id: \"hist-skill\".to_string(), /* ... */ };\n        let commit = archive.write_skill(\u0026spec).unwrap();\n        \n        assert\\!(\\!commit.oid.is_empty());\n        assert\\!(commit.message.contains(\"hist-skill\"));\n    }\n}\n```\n\n---\n\n## Logging Requirements\n\nAll git operations must log:\n- **DEBUG**: File paths written, git staging operations\n- **INFO**: Commits created, push/pull operations\n- **WARN**: Merge conflicts detected, missing remotes\n- **ERROR**: Git failures, permission errors\n\n---\n\n## References\n\n- **Plan Section 3.3**: Git Archive Structure (Human-Readable Persistence)\n- **Plan Section 3.7**: Two-Phase Commit for Dual Persistence\n- **Depends on**: meta_skill-5s0 (Rust Project Scaffolding)\n- **Blocks**: meta_skill-fus (2PC), meta_skill-14h (CLI Commands)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:01.489461268-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:59:06.880795928-05:00","labels":["git","persistence","phase-1"],"dependencies":[{"issue_id":"meta_skill-b98","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.82314122-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-c98","title":"[P6] Skill Templates Library","description":"# Skill Templates Library\n\n## Overview\n\nProvide curated templates for rapid skill authoring (debugging, refactor, deploy, UI polish, etc.). Templates enforce best‑practice structure and token density.\n\n---\n\n## Tasks\n\n1. Define template schema (metadata + sections).\n2. Provide CLI `ms template list/show/apply`.\n3. Include common templates aligned with best‑practices.\n\n---\n\n## Testing Requirements\n\n- Unit tests for template parsing.\n- Integration tests: template → SkillSpec compile.\n\n---\n\n## Acceptance Criteria\n\n- Templates compile to valid SkillSpec.\n- Templates produce deterministic output.\n\n---\n\n## Dependencies\n\n- `meta_skill-ik6` SkillSpec Data Model","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:26.843243723-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:12:36.819017575-05:00","labels":["authoring","phase-6","templates"],"dependencies":[{"issue_id":"meta_skill-c98","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:28:37.174615516-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-cbx","title":"CASS Mining: Testing Patterns","description":"Deep dive into Vitest, Testing Library, unit test patterns, integration test methodologies, property-based testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:40.328802502-05:00","created_by":"ubuntu","updated_at":"2026-01-13T20:12:49.25841031-05:00","closed_at":"2026-01-13T20:12:49.25841031-05:00","close_reason":"Added Section 34: Testing Patterns and Methodology (~915 lines). Covers NO mocks philosophy, test organization patterns (JS/TS/Go), fixtures with temp directories, property-based testing with proptest, coverage analysis, snapshot testing, E2E with Playwright, BATS for shell testing, clipboard testing, test harness patterns, and CI integration.","labels":["cass-mining"]}
{"id":"meta_skill-ch6","title":"[P2] Hash Embeddings (xf-style)","description":"## Hash Embeddings (xf-style, Complete)\n\nHash-based embeddings provide 80-90% of ML embedding quality for skill matching, with zero operational complexity. No model files, no inference latency, deterministic results.\n\n### Algorithm\n\n```rust\n/// Generate hash-based embeddings (no ML model needed)\n/// Uses FNV-1a hash with dimension reduction\npub fn hash_embedding(text: \u0026str, dimensions: usize) -\u003e Vec\u003cf32\u003e {\n    let mut embedding = vec\\![0.0f32; dimensions];\n\n    // Tokenize: lowercase, split on non-alphanumeric, filter short tokens\n    let tokens: Vec\u003c\u0026str\u003e = text\n        .to_lowercase()\n        .split(|c: char| \\!c.is_alphanumeric())\n        .filter(|s| \\!s.is_empty() \u0026\u0026 s.len() \u003e 2)\n        .collect();\n\n    // Hash each token and accumulate into embedding dimensions\n    for token in \u0026tokens {\n        let hash = fnv1a_hash(token.as_bytes());\n\n        // Use hash to determine dimension and sign\n        for i in 0..dimensions {\n            let dim_hash = fnv1a_hash(\u0026[hash as u8, i as u8]);\n            let sign = if dim_hash \u0026 1 == 0 { 1.0 } else { -1.0 };\n            let dim = (dim_hash as usize \u003e\u003e 1) % dimensions;\n            embedding[dim] += sign;\n        }\n    }\n\n    // Also hash n-grams for context (bigrams with reduced weight)\n    for window in tokens.windows(2) {\n        let bigram = format\\!(\"{} {}\", window[0], window[1]);\n        let hash = fnv1a_hash(bigram.as_bytes());\n\n        for i in 0..dimensions {\n            let dim_hash = fnv1a_hash(\u0026[hash as u8, i as u8]);\n            let sign = if dim_hash \u0026 1 == 0 { 0.5 } else { -0.5 };\n            let dim = (dim_hash as usize \u003e\u003e 1) % dimensions;\n            embedding[dim] += sign;\n        }\n    }\n\n    // L2 normalize for cosine similarity\n    let norm: f32 = embedding.iter().map(|x| x * x).sum::\u003cf32\u003e().sqrt();\n    if norm \u003e 0.0 {\n        for x in \u0026mut embedding {\n            *x /= norm;\n        }\n    }\n\n    embedding\n}\n\n/// FNV-1a hash function (fast, good distribution)\nfn fnv1a_hash(data: \u0026[u8]) -\u003e u64 {\n    const FNV_OFFSET: u64 = 0xcbf29ce484222325;\n    const FNV_PRIME: u64 = 0x100000001b3;\n    \n    let mut hash = FNV_OFFSET;\n    for byte in data {\n        hash ^= *byte as u64;\n        hash = hash.wrapping_mul(FNV_PRIME);\n    }\n    hash\n}\n```\n\n### Embedder Trait (Pluggable Backends)\n\n```rust\npub trait Embedder {\n    fn embed(\u0026self, text: \u0026str) -\u003e Vec\u003cf32\u003e;\n    fn dims(\u0026self) -\u003e usize;\n}\n\n/// Default embedder: hash-based (fast, deterministic, zero deps)\npub struct HashEmbedder {\n    pub dims: usize,  // Default: 384\n}\n\nimpl Embedder for HashEmbedder {\n    fn embed(\u0026self, text: \u0026str) -\u003e Vec\u003cf32\u003e {\n        hash_embedding(text, self.dims)\n    }\n    fn dims(\u0026self) -\u003e usize { self.dims }\n}\n\n/// Optional: local ML model embedder (feature-gated)\n#[cfg(feature = \"ml-embeddings\")]\npub struct LocalMlEmbedder {\n    model: ort::Session,\n}\n\n#[cfg(feature = \"ml-embeddings\")]\nimpl Embedder for LocalMlEmbedder {\n    fn embed(\u0026self, text: \u0026str) -\u003e Vec\u003cf32\u003e {\n        // ONNX inference for higher semantic fidelity\n        unimplemented\\!()\n    }\n    fn dims(\u0026self) -\u003e usize { 384 }\n}\n```\n\n### Vector Index Storage\n\n```rust\npub struct VectorIndex {\n    embeddings: HashMap\u003cString, Vec\u003cf32\u003e\u003e,  // skill_id -\u003e embedding\n    dims: usize,\n}\n\nimpl VectorIndex {\n    pub fn insert(\u0026mut self, skill_id: \u0026str, embedding: Vec\u003cf32\u003e) {\n        self.embeddings.insert(skill_id.to_string(), embedding);\n    }\n    \n    /// Cosine similarity search\n    pub fn search(\u0026self, query_embedding: \u0026[f32], limit: usize) -\u003e Vec\u003c(String, f32)\u003e {\n        let mut scores: Vec\u003c_\u003e = self.embeddings\n            .iter()\n            .map(|(id, emb)| {\n                let sim = cosine_similarity(query_embedding, emb);\n                (id.clone(), sim)\n            })\n            .collect();\n        \n        scores.sort_by(|a, b| b.1.partial_cmp(\u0026a.1).unwrap());\n        scores.truncate(limit);\n        scores\n    }\n}\n\nfn cosine_similarity(a: \u0026[f32], b: \u0026[f32]) -\u003e f32 {\n    // Vectors are pre-normalized, so dot product = cosine similarity\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n```\n\n### SQLite Storage\n\n```sql\n-- Embeddings stored as BLOB (binary float32 array)\nCREATE TABLE skill_embeddings (\n    skill_id TEXT PRIMARY KEY REFERENCES skills(id),\n    embedding BLOB NOT NULL,\n    dims INTEGER NOT NULL DEFAULT 384,\n    embedder_type TEXT NOT NULL DEFAULT 'hash',\n    computed_at TEXT NOT NULL\n);\n\n-- Index for fast lookup\nCREATE INDEX idx_skill_embeddings_type ON skill_embeddings(embedder_type);\n```\n\n### Key Properties\n\n| Property | Value |\n|----------|-------|\n| Default dimensions | 384 |\n| Hash function | FNV-1a (64-bit) |\n| Normalization | L2 (unit vectors) |\n| N-gram support | Bigrams at 0.5x weight |\n| Token filter | Length \u003e 2, alphanumeric only |\n| Similarity metric | Cosine (via dot product) |\n\n### Why Hash Embeddings?\n\n1. **Zero dependencies**: No model files, no GPU, no inference framework\n2. **Deterministic**: Same input always produces same output\n3. **Fast**: ~1μs per embedding (vs ~10ms for ML models)\n4. **Good enough**: 80-90% of ML quality for skill matching use cases\n5. **xf-proven**: Battle-tested in production at scale","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:03.391012411-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:12:28.654261415-05:00","labels":["embeddings","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-ch6","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:23:13.492169072-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-cn4","title":"Block-Level Overlays","description":"## Overview\n\nImplement block-level overlays for meta_skill (Section 3.5 of PLAN_TO_MAKE_METASKILL_CLI.md). Overlay files patch specific block IDs without copying entire skills, enabling surgical policy additions and customizations.\n\n## Background \u0026 Rationale\n\n### The Problem with Full Skill Copies\nWhen users want to customize a skill, the naive approach is to copy the entire skill to a higher layer. This creates problems:\n- **Maintenance Burden**: When the base skill updates, the copy is outdated\n- **Merge Conflicts**: No clear way to incorporate upstream changes\n- **Bloat**: Duplicating large skills for small changes wastes space\n- **Unclear Intent**: Hard to see what actually changed\n\n### The Overlay Solution\nOverlays are small patch files that declare:\n- Which skill they modify\n- Which blocks to add, replace, remove, or append to\n- What the new content should be\n\nBenefits:\n- **Minimal Footprint**: Only specify what changes\n- **Clear Intent**: Overlay file shows exactly what's different\n- **Automatic Updates**: Base skill updates flow through automatically\n- **Composable**: Multiple overlays can stack\n\n### Example Use Case\nA company wants to add a security reminder to the \"api-design\" skill without maintaining a full copy:\n\n```toml\n# ~/.config/ms/overlays/api-design-security.overlay\nskill_id = \"api-design\"\nlayer = \"global\"\n\n[[operations]]\ntype = \"add\"\nblock_id = \"security-reminder\"\nafter = \"best-practices\"\ncontent = \"\"\"\n## Security Reminder\nAll API endpoints MUST:\n- Validate input parameters\n- Use authentication\n- Log access attempts\n\"\"\"\n```\n\n## Key Data Structures (from Plan Section 3.5)\n\n```rust\n/// An overlay that patches a specific skill\nstruct SkillOverlay {\n    /// The skill this overlay patches\n    skill_id: String,\n    /// The layer this overlay exists in\n    layer: SkillLayer,\n    /// Ordered list of patch operations\n    operations: Vec\u003cOverlayOp\u003e,\n    /// Optional: only apply if condition is met\n    condition: Option\u003cOverlayCondition\u003e,\n    /// Metadata about the overlay\n    metadata: OverlayMetadata,\n}\n\n/// A single overlay operation\nenum OverlayOp {\n    /// Add a new block to the skill\n    Add {\n        block_id: String,\n        content: String,\n        /// Where to insert: after this block ID\n        after: Option\u003cString\u003e,\n        /// Where to insert: before this block ID\n        before: Option\u003cString\u003e,\n        /// Block type (section, example, tip, etc.)\n        block_type: BlockType,\n    },\n    /// Replace an existing block entirely\n    Replace {\n        block_id: String,\n        content: String,\n    },\n    /// Remove a block from the skill\n    Remove {\n        block_id: String,\n    },\n    /// Append content to an existing block\n    AppendTo {\n        block_id: String,\n        /// Items to append (e.g., list items, paragraphs)\n        items: Vec\u003cString\u003e,\n        /// Separator between existing and new content\n        separator: Option\u003cString\u003e,\n    },\n    /// Prepend content to an existing block\n    PrependTo {\n        block_id: String,\n        items: Vec\u003cString\u003e,\n        separator: Option\u003cString\u003e,\n    },\n    /// Modify block metadata without changing content\n    UpdateMetadata {\n        block_id: String,\n        updates: HashMap\u003cString, String\u003e,\n    },\n}\n\n/// Conditions for when to apply an overlay\nenum OverlayCondition {\n    /// Only apply in certain environments\n    Environment(String),\n    /// Only apply if a feature flag is set\n    FeatureFlag(String),\n    /// Only apply if another skill is loaded\n    SkillLoaded(String),\n    /// Combine conditions with AND\n    All(Vec\u003cOverlayCondition\u003e),\n    /// Combine conditions with OR\n    Any(Vec\u003cOverlayCondition\u003e),\n}\n\n/// Metadata about an overlay\nstruct OverlayMetadata {\n    /// Human-readable description of what this overlay does\n    description: String,\n    /// Who created this overlay\n    author: Option\u003cString\u003e,\n    /// Version of the overlay\n    version: Option\u003cVersion\u003e,\n    /// Minimum skill version this is compatible with\n    min_skill_version: Option\u003cVersion\u003e,\n}\n\n/// Result of applying an overlay\nstruct OverlayApplicationResult {\n    /// The skill ID that was modified\n    skill_id: String,\n    /// Operations that succeeded\n    applied: Vec\u003cOverlayOpResult\u003e,\n    /// Operations that failed (with reasons)\n    failed: Vec\u003cOverlayOpFailure\u003e,\n    /// Warnings (e.g., deprecated block IDs)\n    warnings: Vec\u003cString\u003e,\n}\n\n/// Result of a single operation\nstruct OverlayOpResult {\n    /// The operation that was applied\n    operation: OverlayOp,\n    /// Block IDs affected\n    affected_blocks: Vec\u003cString\u003e,\n}\n\n/// Failure information for an operation\nstruct OverlayOpFailure {\n    /// The operation that failed\n    operation: OverlayOp,\n    /// Why it failed\n    reason: OverlayError,\n}\n\n/// Possible overlay errors\nenum OverlayError {\n    /// Target block doesn't exist\n    BlockNotFound(String),\n    /// Target skill doesn't exist\n    SkillNotFound(String),\n    /// Block ID already exists (for Add)\n    BlockAlreadyExists(String),\n    /// Invalid content format\n    InvalidContent(String),\n    /// Condition not met\n    ConditionNotMet(OverlayCondition),\n    /// Version incompatibility\n    VersionMismatch { required: Version, actual: Version },\n}\n```\n\n## Tasks\n\n### Task 1: Define Overlay Types\n- [ ] Create `src/overlays/mod.rs` module\n- [ ] Define `SkillOverlay` struct\n- [ ] Define `OverlayOp` enum with all variants\n- [ ] Define `OverlayCondition` enum\n- [ ] Define `OverlayMetadata` struct\n- [ ] Define result and error types\n- [ ] Add comprehensive documentation\n\n### Task 2: Implement Overlay Parsing\n- [ ] Create TOML parser for `.overlay` files\n- [ ] Parse overlay header (skill_id, layer, metadata)\n- [ ] Parse operations array\n- [ ] Parse conditions\n- [ ] Validate overlay structure on parse\n- [ ] Return helpful parse errors with line numbers\n\n### Task 3: Implement Add Operation\n- [ ] Insert new block at specified position\n- [ ] Support `after` positioning (after specific block)\n- [ ] Support `before` positioning (before specific block)\n- [ ] Default to end if no position specified\n- [ ] Validate block_id doesn't already exist\n- [ ] Log block addition details\n\n### Task 4: Implement Replace Operation\n- [ ] Find target block by ID\n- [ ] Replace content entirely\n- [ ] Preserve block type unless explicitly changed\n- [ ] Error if block doesn't exist\n- [ ] Log original vs new content hash\n\n### Task 5: Implement Remove Operation\n- [ ] Find target block by ID\n- [ ] Remove block from skill\n- [ ] Handle references to removed block\n- [ ] Error if block doesn't exist\n- [ ] Log removed block details\n\n### Task 6: Implement AppendTo/PrependTo Operations\n- [ ] Find target block by ID\n- [ ] Append/prepend items to existing content\n- [ ] Support configurable separator\n- [ ] Handle list blocks specially (add list items)\n- [ ] Log appended/prepended content\n\n### Task 7: Implement Condition Evaluation\n- [ ] Implement `Environment` condition check\n- [ ] Implement `FeatureFlag` condition check\n- [ ] Implement `SkillLoaded` condition check\n- [ ] Implement `All` combinator (AND logic)\n- [ ] Implement `Any` combinator (OR logic)\n- [ ] Log condition evaluation results\n\n### Task 8: Implement Overlay Application Engine\n- [ ] Create `apply_overlay(skill, overlay)` function\n- [ ] Execute operations in order\n- [ ] Collect results for each operation\n- [ ] Continue on recoverable errors\n- [ ] Return comprehensive `OverlayApplicationResult`\n- [ ] Support dry-run mode for preview\n\n### Task 9: Implement Overlay Discovery\n- [ ] Scan overlay directories for `.overlay` files\n- [ ] Group overlays by target skill\n- [ ] Order overlays by layer priority\n- [ ] Handle overlay conflicts (same block in multiple overlays)\n- [ ] Log discovered overlays\n\n### Task 10: Integration with LayeredRegistry\n- [ ] Apply overlays during skill resolution\n- [ ] Apply overlays in layer order (system -\u003e session)\n- [ ] Cache applied overlay results\n- [ ] Support `--no-overlays` flag\n- [ ] Track which overlays are applied to each skill\n\n## Acceptance Criteria\n\n1. **Parsing**: Can parse all overlay file formats\n2. **Add Operation**: Adds blocks at correct positions\n3. **Replace Operation**: Replaces blocks correctly\n4. **Remove Operation**: Removes blocks cleanly\n5. **Append/Prepend**: Modifies existing blocks correctly\n6. **Conditions**: Evaluates all condition types\n7. **Error Handling**: Clear errors for invalid operations\n8. **Layer Order**: Overlays apply in correct layer order\n9. **Dry Run**: Can preview overlay effects\n10. **Performance**: Overlay application adds \u003c5ms per skill\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[test]\nfn test_parse_overlay_file() {\n    let overlay_toml = r#\"\n        skill_id = \"api-design\"\n        layer = \"global\"\n        \n        [metadata]\n        description = \"Add security section\"\n        \n        [[operations]]\n        type = \"add\"\n        block_id = \"security\"\n        after = \"best-practices\"\n        content = \"## Security\\nAlways validate input.\"\n    \"#;\n    \n    let overlay = SkillOverlay::from_toml(overlay_toml).unwrap();\n    assert_eq!(overlay.skill_id, \"api-design\");\n    assert_eq!(overlay.layer, SkillLayer::Global);\n    assert_eq!(overlay.operations.len(), 1);\n}\n\n#[test]\nfn test_add_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"intro\", \"Introduction content\");\n    skill.add_block(\"conclusion\", \"Conclusion content\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Add {\n                block_id: \"middle\".into(),\n                content: \"Middle content\".into(),\n                after: Some(\"intro\".into()),\n                before: None,\n                block_type: BlockType::Section,\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let result = apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    assert_eq!(result.applied.len(), 1);\n    \n    let blocks: Vec\u003c_\u003e = skill.blocks().map(|b| b.id()).collect();\n    assert_eq!(blocks, vec![\"intro\", \"middle\", \"conclusion\"]);\n}\n\n#[test]\nfn test_replace_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"target\", \"Original content\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"target\".into(),\n                content: \"Replaced content\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    assert_eq!(skill.get_block(\"target\").unwrap().content(), \"Replaced content\");\n}\n\n#[test]\nfn test_remove_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"keep\", \"Keep this\");\n    skill.add_block(\"remove\", \"Remove this\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Remove { block_id: \"remove\".into() }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    assert!(skill.get_block(\"remove\").is_none());\n    assert!(skill.get_block(\"keep\").is_some());\n}\n\n#[test]\nfn test_append_to_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"list\", \"- Item 1\\n- Item 2\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::AppendTo {\n                block_id: \"list\".into(),\n                items: vec![\"- Item 3\".into(), \"- Item 4\".into()],\n                separator: Some(\"\\n\".into()),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    let content = skill.get_block(\"list\").unwrap().content();\n    assert!(content.contains(\"Item 3\"));\n    assert!(content.contains(\"Item 4\"));\n}\n\n#[test]\nfn test_condition_evaluation() {\n    let condition = OverlayCondition::All(vec![\n        OverlayCondition::Environment(\"production\".into()),\n        OverlayCondition::SkillLoaded(\"base-skill\".into()),\n    ]);\n    \n    let mut ctx = EvalContext::new();\n    ctx.set_environment(\"production\");\n    ctx.set_loaded_skills(vec![\"base-skill\".into()]);\n    \n    assert!(condition.evaluate(\u0026ctx));\n    \n    ctx.set_environment(\"development\");\n    assert!(!condition.evaluate(\u0026ctx));\n}\n\n#[test]\nfn test_overlay_error_block_not_found() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"nonexistent\".into(),\n                content: \"New content\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let result = apply_overlay(\u0026mut skill, \u0026overlay);\n    assert!(result.is_err());\n    assert!(matches!(\n        result.unwrap_err(),\n        OverlayError::BlockNotFound(_)\n    ));\n}\n\n#[test]\nfn test_multiple_overlays_layer_order() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"content\", \"Original\");\n    \n    let system_overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::System,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"content\".into(),\n                content: \"System version\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let project_overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Project,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"content\".into(),\n                content: \"Project version\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    // Apply in layer order\n    apply_overlay(\u0026mut skill, \u0026system_overlay).unwrap();\n    apply_overlay(\u0026mut skill, \u0026project_overlay).unwrap();\n    \n    // Project (higher layer) should win\n    assert_eq!(skill.get_block(\"content\").unwrap().content(), \"Project version\");\n}\n\n#[test]\nfn test_dry_run_mode() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"content\", \"Original\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"content\".into(),\n                content: \"Modified\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let result = preview_overlay(\u0026skill, \u0026overlay).unwrap();\n    \n    // Original skill unchanged\n    assert_eq!(skill.get_block(\"content\").unwrap().content(), \"Original\");\n    \n    // Preview shows what would change\n    assert_eq!(result.would_modify.len(), 1);\n    assert_eq!(result.would_modify[0].block_id, \"content\");\n    assert_eq!(result.would_modify[0].new_content, \"Modified\");\n}\n```\n\n### Logging Requirements\nAll operations must log with appropriate levels:\n- `DEBUG`: Parse steps, operation execution details\n- `INFO`: Overlay application results, blocks modified\n- `WARN`: Deprecated block IDs, version mismatches\n- `ERROR`: Failed operations, invalid overlays\n\nExample log output:\n```\n[INFO] Discovering overlays in /home/user/.config/ms/overlays/\n[DEBUG] Found overlay: api-design-security.overlay\n[DEBUG] Parsing overlay for skill 'api-design'\n[INFO] Applying overlay 'api-design-security' to skill 'api-design'\n[DEBUG] Executing Add operation: block_id='security', after='best-practices'\n[DEBUG] Block 'security' inserted at position 3\n[INFO] Overlay applied successfully: 1 operation, 1 block affected\n[DEBUG] Overlay application took 2ms\n```\n\n## File Format Reference\n\n### Basic Overlay File\n```toml\n# skill-name.overlay\nskill_id = \"skill-name\"\nlayer = \"global\"\n\n[metadata]\ndescription = \"What this overlay does\"\nauthor = \"Your Name\"\nversion = \"1.0.0\"\n\n[[operations]]\ntype = \"add\"\nblock_id = \"new-section\"\nafter = \"existing-section\"\ncontent = \"\"\"\n## New Section\nContent here.\n\"\"\"\n\n[[operations]]\ntype = \"replace\"\nblock_id = \"old-section\"\ncontent = \"\"\"\n## Updated Section\nNew content.\n\"\"\"\n```\n\n### Conditional Overlay\n```toml\nskill_id = \"production-skill\"\nlayer = \"project\"\n\n[condition]\ntype = \"all\"\nconditions = [\n    { type = \"environment\", value = \"production\" },\n    { type = \"skill_loaded\", value = \"security-base\" }\n]\n\n[[operations]]\ntype = \"add\"\nblock_id = \"prod-warning\"\nbefore = \"intro\"\ncontent = \"\"\"\n\u003e **Production Environment**\n\u003e Extra care required.\n\"\"\"\n```\n\n## References\n\n- Plan Section 3.5: Layering \u0026 Conflict Resolution (overlay subsection)\n- Plan Section 3.1: Block structure and IDs\n- Depends on: meta_skill-225 (Skill Layering \u0026 Conflict Resolution)\n- Blocks: meta_skill-7va (ms load Command)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:53:53.125727762-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:53:53.125727762-05:00","labels":["layers","overlays","phase-1"],"dependencies":[{"issue_id":"meta_skill-cn4","depends_on_id":"meta_skill-225","type":"blocks","created_at":"2026-01-13T22:54:05.254837937-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-dag","title":"CASS Mining: Error Handling Patterns (anyhow/thiserror)","description":"Deep dive into anyhow::Result patterns, custom error types, robot-friendly structured output formats, error propagation best practices.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:27.956747962-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:42:47.226031489-05:00","closed_at":"2026-01-13T18:42:47.226031489-05:00","close_reason":"Completed Section 33: Error Handling Patterns and Methodology (~860 lines). Covers thiserror/anyhow dichotomy, structured CLI errors, error taxonomy, context chaining, retry with backoff, circuit breakers, panic vs Result guidelines, error boundaries, and logging best practices.","labels":["cass-mining"]}
{"id":"meta_skill-e5e","title":"Skill Quality Scoring Algorithm","description":"# Skill Quality Scoring Algorithm\n\n## Section Reference\nSection 7.4 - Skill Quality Scoring Algorithm\n\n## Overview\n\nQuality scoring determines which skills are most worth surfacing to agents. This implements a multi-factor scoring algorithm that combines structure analysis, content quality, provenance (evidence coverage and confidence), usage metrics, and toolchain compatibility.\n\n## Why Quality Scoring Matters\n\nNot all skills are equally useful:\n- Some may be outdated or stale\n- Some may lack evidence/provenance\n- Some may have low usage/adoption\n- Some may not match the current tech stack\n\nQuality scoring enables:\n- Prioritizing high-quality skills in search results\n- Filtering out low-quality skills from suggestions\n- Identifying skills that need improvement\n- Automated quality gates for publishing\n\n## Core Data Structures (from Plan)\n\n```rust\n/// Quality scoring system\nstruct QualityScorer {\n    weights: QualityWeights,\n    usage_tracker: UsageTracker,\n    toolchain_detector: ToolchainDetector,\n    project_path: Option\u003cPathBuf\u003e,\n}\n\n/// Configurable weights for quality factors\nstruct QualityWeights {\n    structure_weight: f32,      // Well-formed sections\n    content_weight: f32,        // Completeness and clarity\n    evidence_weight: f32,       // Provenance coverage\n    usage_weight: f32,          // Recent usage frequency\n    toolchain_weight: f32,      // Tech stack match\n    freshness_weight: f32,      // Last update recency\n}\n\nimpl Default for QualityWeights {\n    fn default() -\u003e Self {\n        Self {\n            structure_weight: 0.15,\n            content_weight: 0.25,\n            evidence_weight: 0.20,\n            usage_weight: 0.20,\n            toolchain_weight: 0.10,\n            freshness_weight: 0.10,\n        }\n    }\n}\n\n/// Quality assessment result\nstruct QualityScore {\n    overall: f32,               // 0.0 to 1.0\n    breakdown: QualityBreakdown,\n    issues: Vec\u003cQualityIssue\u003e,\n    suggestions: Vec\u003cString\u003e,\n}\n\nstruct QualityBreakdown {\n    structure: f32,\n    content: f32,\n    evidence: f32,\n    usage: f32,\n    toolchain: f32,\n    freshness: f32,\n}\n```\n\n## Quality Issue Types\n\n```rust\nenum QualityIssue {\n    MissingSection(String),      // Required section absent\n    ShortContent(String, usize), // Section too brief\n    NoExamples,                  // No code examples\n    StaleContent(DateTime),      // Not updated recently\n    LowEvidence(f32),            // Insufficient provenance\n    LowUsage(u32),               // Rarely used\n    ToolchainMismatch(String),   // Tech stack doesn't match\n    NoTags,                      // Missing categorization\n    PoorFormatting,              // Markdown issues\n}\n```\n\n## CLI Integration\n\n```bash\n# Show quality score for a skill\nms quality rust-error-handling\n\n# Show quality scores for all skills\nms quality --all\n\n# Filter search by minimum quality\nms search \"async\" --min-quality 0.7\n\n# Quality report for publishing readiness\nms quality rust-error-handling --report\n```\n\n## Robot Mode Output\n\n```json\n{\n  \"skill_id\": \"rust-error-handling\",\n  \"quality_score\": 0.85,\n  \"breakdown\": {\n    \"structure\": 0.95,\n    \"content\": 0.90,\n    \"evidence\": 0.80,\n    \"usage\": 0.75,\n    \"toolchain\": 1.0,\n    \"freshness\": 0.85\n  },\n  \"issues\": [\n    {\"type\": \"low_evidence\", \"details\": \"Only 3 provenance links\"}\n  ],\n  \"suggestions\": [\n    \"Add more examples\",\n    \"Link to additional evidence\"\n  ]\n}\n```\n\n## Acceptance Criteria\n\n1. [ ] QualityScorer struct with configurable weights\n2. [ ] Structure analysis (required sections present)\n3. [ ] Content analysis (completeness, examples)\n4. [ ] Evidence analysis (provenance coverage)\n5. [ ] Usage analysis (frequency tracking)\n6. [ ] Toolchain compatibility check\n7. [ ] Freshness check (last update time)\n8. [ ] CLI command: ms quality \u003cskill\u003e\n9. [ ] Integration with search filtering\n10. [ ] Robot mode JSON output\n\n## Dependencies\n\n- Depends on: meta_skill-o8o (Context-Aware Suggestions)\n- Depends on: meta_skill-qs1 (SQLite for usage tracking)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:32:59.834298977-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:32:59.834298977-05:00","labels":["phase-3","quality","search"],"dependencies":[{"issue_id":"meta_skill-e5e","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:33:30.949403194-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-e5e","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:33:30.9782025-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-f5n","title":"[P3] Suggestion Cooldowns","description":"# Suggestion Cooldowns\n\nPrevent suggestion spam via context fingerprints.\n\n## Tasks\n1. Define ContextFingerprint (hash of recent context)\n2. Track recently suggested skills per fingerprint\n3. Implement cooldown periods\n4. Decay old fingerprints\n\n## Cooldown Logic (from Section 7.3)\n- Skill suggested → record (skill_id, context_hash, timestamp)\n- Same context + same skill → cooldown 30 minutes\n- Different context → suggest again\n- Explicit dismiss → extended cooldown\n\n## Storage\n```sql\nCREATE TABLE suggestion_cooldowns (\n    skill_id TEXT,\n    context_hash TEXT,\n    suggested_at TIMESTAMP,\n    dismissed BOOLEAN DEFAULT FALSE,\n    PRIMARY KEY (skill_id, context_hash)\n);\n```\n\n## Fingerprint Components\n- Hash of: cwd + sorted(files) + project_type\n- Exclude volatile data (timestamps, etc.)\n\n## Acceptance Criteria\n- Same context doesn't spam suggestions\n- Context change resets cooldown\n- Dismissed skills get longer cooldown","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:24:17.144555618-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:41:58.72973145-05:00","closed_at":"2026-01-13T23:41:58.72973145-05:00","close_reason":"Duplicate of meta_skill-8df (Context Fingerprints \u0026 Suggestion Cooldowns)","labels":["cooldown","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-f5n","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T22:24:25.980432592-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-f8s","title":"CASS Mining: CI/CD Automation Patterns","description":"Deep dive into GitHub Actions workflows (ci.yml, deploy.yml, e2e.yml, dependabot.yml), release automation, pipeline optimization.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:28.647404029-05:00","created_by":"ubuntu","updated_at":"2026-01-13T20:33:12.38309515-05:00","closed_at":"2026-01-13T20:33:12.38309515-05:00","close_reason":"Added Section 35: CI/CD Automation Patterns (~925 lines). CASS mined: repo_updater, apr, jeffreysprompts_premium, flywheel_gateway, destructive_command_guard. Covered: 35.1 GitHub Actions Workflow Architecture (ci.yml 5-job pattern), 35.2 Job Dependencies and Ordering, 35.3 Release Automation (tag-triggered with checksums), 35.4 Version Management (dual storage, semantic comparison), 35.5 Matrix Testing Strategies (OS+runtime matrices), 35.6 Container Image Pipelines (multi-stage Dockerfile, Trivy, SBOM), 35.7 Artifact Management (upload/download/cache patterns), 35.8 Dependabot Configuration, 35.9 Pre-Commit Hook Integration, 35.10 Deployment Workflows (Vercel + smoke tests), 35.11 Quality Gates (lint/type/format/test/build), 35.12 Self-Update Mechanisms (SHA256 verification), 35.13 Application to meta_skill table, 35.14 CI/CD Checklist.","labels":["cass-mining"]}
{"id":"meta_skill-f97","title":"[P4] Anti-Pattern Mining","description":"# Anti-Pattern Mining\n\nExtract \"what NOT to do\" from sessions.\n\n## Tasks\n1. Identify failure sequences\n2. Extract error patterns\n3. Link to eventual solutions\n4. Generate Pitfall slices\n5. Weight by frequency\n\n## Anti-Pattern Structure (from Section 8.9)\n```yaml\npitfall:\n  symptom: \"Module not found error after npm install\"\n  wrong_approach: \"Deleting node_modules and reinstalling\"\n  why_wrong: \"Often masks dependency version conflicts\"\n  correct_approach: \"Check package-lock.json for version mismatches\"\n  evidence:\n    - session: cass-abc123\n      quote: \"Tried deleting node_modules three times...\"\n```\n\n## Detection Heuristics\n- Multiple retries of same command\n- Error followed by different approach followed by success\n- Explicit \"that didn't work\" statements\n- Tool/command switches mid-task\n\n## Counterexample Value\n- Pitfalls are often more valuable than rules\n- Prevent common mistakes\n- Save debugging time\n\n## Acceptance Criteria\n- Anti-patterns extracted\n- Linked to correct approaches\n- Formatted as Pitfall slices","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:26:03.441956729-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:41:50.80848741-05:00","closed_at":"2026-01-13T23:41:50.80848741-05:00","close_reason":"Duplicate of meta_skill-tun (Anti-Pattern Mining)","labels":["anti-patterns","phase-4","pitfalls"],"dependencies":[{"issue_id":"meta_skill-f97","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:13.099831725-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-fma","title":"Prompt Injection Defense","description":"## Section Reference\nSection 5.17 - Prompt Injection Defense\n\n## Overview\n\n**CRITICAL**: This feature integrates ACIP (Advanced Cognitive Inoculation Prompt) v1.3 from `/data/projects/acip` as the primary prompt injection defense layer. ACIP is a battle-tested, comprehensive framework specifically designed to protect against sophisticated prompt injection attacks.\n\nRather than building custom detection from scratch, ms leverages ACIP's Cognitive Integrity Framework (CIF) and adapts it for CASS session mining contexts.\n\n## Why ACIP (not custom implementation)\n\n| Aspect | Custom Implementation | ACIP v1.3 |\n|--------|----------------------|-----------|\n| **Maturity** | New, untested | Battle-tested framework |\n| **Attack coverage** | Limited | Comprehensive (direct, indirect, exfiltration, bypass) |\n| **False positive rate** | Unknown | Tuned and documented |\n| **Maintenance** | Must track evolving attacks | Community-maintained |\n| **Audit mode** | Must build | Built-in operator observability |\n| **Domain coverage** | Generic | 6 balanced high-risk domains |\n\n## ACIP Integration Architecture\n\n```rust\n/// ACIP-based injection analyzer for session mining\nstruct AcipSessionAnalyzer {\n    /// ACIP v1.3 config (loaded from /data/projects/acip)\n    acip_config: AcipConfig,\n    /// Local quarantine store\n    quarantine: QuarantineStore,\n    /// Audit mode enabled (maps to ACIP_AUDIT_MODE)\n    audit_mode: bool,\n}\n\n/// Maps ACIP concepts to session mining\nstruct AcipConfig {\n    /// Path to ACIP prompt text\n    acip_prompt_path: PathBuf,\n    /// Version (should be \"1.3\")\n    version: String,\n    /// Trust boundaries for session content\n    trust_boundaries: TrustBoundaryConfig,\n    /// Decision discipline config\n    decision_config: DecisionConfig,\n}\n\n/// Trust boundary configuration per ACIP Section 3\nstruct TrustBoundaryConfig {\n    /// User messages: instructions or data?\n    user_messages: TrustLevel,\n    /// Assistant responses: trusted or verify?\n    assistant_messages: TrustLevel,\n    /// Tool outputs: always untrusted per ACIP\n    tool_outputs: TrustLevel,\n    /// File contents: always untrusted\n    file_contents: TrustLevel,\n}\n\nenum TrustLevel {\n    /// Can be instructions\n    Trusted,\n    /// Data only, never execute\n    Untrusted,\n    /// Verify before trusting\n    VerifyRequired,\n}\n```\n\n## ACIP Threat Model (from v1.3)\n\nACIP defends against:\n1. **Direct prompt injection** — malicious instructions from user\n2. **Indirect prompt injection** — instructions in untrusted content (tool outputs, webpages, documents)\n3. **Data exfiltration** — attempts to extract secrets/policies\n4. **Policy bypass** — encoding, transformation, aggregation attacks\n\n**Session mining specific threats:**\n- Poisoned sessions with embedded injection attempts\n- Payload smuggling in code snippets\n- Recursive injection (instructions to inject into outputs)\n- Multi-turn capability aggregation across session messages\n\n## ACIP Decision Discipline Integration\n\nPer ACIP v1.3 Section \"Decision Discipline\":\n\n```rust\n/// Classification result per ACIP decision framework\nenum AcipClassification {\n    /// Safe to extract patterns from\n    Safe,\n    /// Allowed but needs defensive framing\n    SensitiveAllowed { constraints: Vec\u003cString\u003e },\n    /// Must not extract patterns from\n    Disallowed { category: String, action: String },\n}\n\n/// Decision engine following ACIP discipline\nstruct DecisionEngine {\n    /// Classification logic\n    classifier: Box\u003cdyn Classifier\u003e,\n}\n\nimpl DecisionEngine {\n    /// Step 1: Classification (internal, never disclosed per ACIP)\n    fn classify(\u0026self, content: \u0026SessionContent) -\u003e AcipClassification {\n        // Check for:\n        // - Priority manipulation\n        // - Secret requests\n        // - Exfiltration vectors\n        // - High-risk domain escalation\n        // - Multi-turn drift\n        // - Capability aggregation\n        // - Contextual risk amplification\n        unimplemented!()\n    }\n    \n    /// Step 2: Response construction\n    fn respond(\u0026self, classification: AcipClassification) -\u003e FilterAction {\n        match classification {\n            AcipClassification::Safe =\u003e FilterAction::Extract,\n            AcipClassification::SensitiveAllowed { constraints } =\u003e {\n                FilterAction::ExtractWithConstraints(constraints)\n            }\n            AcipClassification::Disallowed { .. } =\u003e {\n                FilterAction::Quarantine\n            }\n        }\n    }\n}\n```\n\n## Audit Mode (per ACIP v1.3)\n\nACIP v1.3 includes operator audit mode for observability without oracle leakage:\n\n```rust\n/// Audit tag format per ACIP spec\nstruct AcipAuditTag {\n    action: AuditAction,\n    category: AuditCategory,\n    source: AuditSource,\n    turn: usize,\n}\n\nenum AuditAction {\n    Denied,\n    Filtered,\n    Escalated,\n}\n\nenum AuditCategory {\n    Injection,\n    Exfiltration,\n    Bypass,\n    HighRisk,\n    Aggregation,\n    Drift,\n    CovertChannel,\n}\n\nenum AuditSource {\n    Direct,\n    Indirect,\n    Tool,\n    MultiTurn,\n}\n\n/// Enable audit mode for ms operations\nfn enable_audit_mode() {\n    // Set ACIP_AUDIT_MODE=ENABLED in context\n    // Audit tags appended to filter reports\n}\n```\n\n## Forensic Quarantine (enhanced with ACIP)\n\n```rust\n/// Quarantine store with ACIP metadata\nstruct QuarantineStore {\n    items: Vec\u003cQuarantinedItem\u003e,\n    by_session: HashMap\u003cSessionId, Vec\u003cQuarantineId\u003e\u003e,\n    path: PathBuf,\n}\n\nstruct QuarantinedItem {\n    id: QuarantineId,\n    /// Hash of original content\n    content_hash: ContentHash,\n    /// Safe excerpt (heavily redacted per ACIP oracle prevention)\n    safe_excerpt: String,\n    /// ACIP classification\n    acip_classification: AcipClassification,\n    /// ACIP audit tag if audit mode was on\n    audit_tag: Option\u003cAcipAuditTag\u003e,\n    /// Session reference\n    session_id: SessionId,\n    message_index: usize,\n    quarantined_at: DateTime\u003cUtc\u003e,\n    /// Replay command (requires explicit invocation)\n    replay_command: String,\n}\n```\n\n## Detection Rules (leveraging ACIP categories)\n\nInstead of custom rules, leverage ACIP's CIF rules:\n\n```rust\n/// ACIP-derived detection categories\nenum AcipDetectionCategory {\n    /// Per CIF Section 2: Anticipatory Threat Recognition\n    SemanticReframing,\n    IndirectTasking,\n    HypotheticalExtraction,\n    AuthorityLaundering,\n    UrgencyFraming,\n    MoralCoercion,\n    IndirectInjection,\n    ExfiltrationAttempt,\n    /// Per CIF Section 3: Instruction-Source Separation\n    InstructionDataConfusion,\n    /// Per CIF Section 4: Semantic Isolation\n    SynonymSubstitution,\n    NegationReversal,\n    ImplicitAssumption,\n    PhraseReordering,\n    /// Per ACIP v1.3 additions\n    CapabilityAggregation,\n    CovertChannel,\n    MultiTurnDrift,\n}\n```\n\n## CLI Commands\n\n```bash\n# Scan sessions for injection attempts (uses ACIP)\nms security scan\nms security scan --session \u003csession-id\u003e\nms security scan --audit-mode  # Enable ACIP audit tags\n\n# View quarantine\nms security quarantine list\nms security quarantine show \u003cquarantine-id\u003e\n\n# Review quarantined items\nms security quarantine review \u003cquarantine-id\u003e --confirm-injection\nms security quarantine review \u003cquarantine-id\u003e --false-positive --reason \"...\"\n\n# Replay (requires explicit permission per ACIP trust boundaries)\nms security quarantine replay \u003cquarantine-id\u003e --i-understand-the-risks\n\n# View ACIP config\nms security acip status\nms security acip config\nms security acip version\n\n# Test detection\nms security test --input \"ignore previous instructions...\"\n```\n\n## Tasks\n\n1. [ ] Load ACIP v1.3 prompt from /data/projects/acip/ACIP_v_1.3_Full_Text.md\n2. [ ] Implement TrustBoundaryConfig for session content types\n3. [ ] Implement DecisionEngine following ACIP Decision Discipline\n4. [ ] Implement AcipAuditTag generation when audit mode enabled\n5. [ ] Integrate ACIP categories into detection rules\n6. [ ] Build QuarantineStore with ACIP metadata\n7. [ ] Implement session pre-filter using ACIP classification\n8. [ ] Build CLI commands for security scanning\n9. [ ] Add audit mode toggle and output formatting\n\n## Testing Requirements\n\n- ACIP integration tests (load config, classify content)\n- Decision discipline correctness tests\n- Trust boundary enforcement tests\n- Audit tag generation tests\n- Quarantine storage tests\n- Pipeline integration tests\n- False positive rate validation against ACIP benchmarks\n\n## Acceptance Criteria\n\n- ACIP v1.3 loaded and integrated\n- All session content classified per ACIP trust boundaries\n- Audit mode produces valid ACIP audit tags\n- Quarantine preserves ACIP classification\n- No oracle leakage (safe excerpts only)\n- CLI commands functional\n\n## References\n\n- ACIP repository: /data/projects/acip\n- ACIP v1.3 full text: /data/projects/acip/ACIP_v_1.3_Full_Text.md\n- Plan Section 5.17\n\nLabels: [defense injection phase-4 security acip]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:54:22.557041146-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:06:45.590145183-05:00","labels":["defense","injection","phase-4","security"]}
{"id":"meta_skill-ftb","title":"Benchmark Tests","description":"## Overview\n\nImplement Criterion benchmark tests for performance-critical paths in the meta_skill CLI. This bead implements Section 18.6 of the Testing Strategy with specific performance targets and CI integration for regression detection.\n\n## Requirements\n\n### 1. Benchmark Configuration\n\nAdd to `Cargo.toml`:\n```toml\n[dev-dependencies]\ncriterion = { version = \"0.5\", features = [\"html_reports\"] }\n\n[[bench]]\nname = \"benchmarks\"\nharness = false\n```\n\nCreate `benches/benchmarks.rs`:\n```rust\nuse criterion::{\n    black_box, criterion_group, criterion_main,\n    Criterion, BenchmarkId, Throughput,\n};\n\nmod hash_embedding;\nmod search;\nmod rrf_fusion;\nmod indexing;\nmod loading;\nmod packing;\n\ncriterion_group!(\n    benches,\n    hash_embedding::benches,\n    search::benches,\n    rrf_fusion::benches,\n    indexing::benches,\n    loading::benches,\n    packing::benches,\n);\n\ncriterion_main!(benches);\n```\n\n### 2. Hash Embedding Benchmarks\n\nTarget: **\u003c 1μs per embedding**\n\nCreate `benches/hash_embedding.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId, Throughput};\nuse ms::search::hash_embed::{hash_embedding, HashEmbedding};\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"hash_embedding\");\n    \n    // Benchmark different input sizes\n    for size in [10, 100, 1000, 10000].iter() {\n        let input: String = \"a\".repeat(*size);\n        \n        group.throughput(Throughput::Bytes(*size as u64));\n        group.bench_with_input(\n            BenchmarkId::new(\"hash_embedding\", size),\n            \u0026input,\n            |b, input| {\n                b.iter(|| hash_embedding(black_box(input)))\n            },\n        );\n    }\n    \n    group.finish();\n    \n    // Benchmark batch processing\n    let mut batch_group = c.benchmark_group(\"hash_embedding_batch\");\n    let inputs: Vec\u003cString\u003e = (0..100).map(|i| format!(\"sample text {}\", i)).collect();\n    \n    batch_group.throughput(Throughput::Elements(100));\n    batch_group.bench_function(\"batch_100\", |b| {\n        b.iter(|| {\n            inputs.iter().map(|s| hash_embedding(black_box(s))).collect::\u003cVec\u003c_\u003e\u003e()\n        })\n    });\n    \n    batch_group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_hash_embedding_performance_target() {\n    use std::time::Instant;\n    \n    let iterations = 10000;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = hash_embedding(black_box(\"sample text for embedding\"));\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] hash_embedding: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_micros(1),\n        \"hash_embedding exceeded 1μs target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 3. Search Benchmarks\n\nTarget: **\u003c 50ms p99 for 1000 skills**\n\nCreate `benches/search.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId, Throughput};\nuse ms::search::{SearchEngine, SearchQuery};\nuse tempfile::TempDir;\n\npub fn benches(c: \u0026mut Criterion) {\n    // Setup: Create index with skills\n    let temp_dir = TempDir::new().unwrap();\n    let engine = setup_search_engine(\u0026temp_dir, 1000);\n    \n    let mut group = c.benchmark_group(\"search\");\n    group.sample_size(100);\n    \n    // Benchmark different query types\n    let queries = vec![\n        (\"simple\", \"rust\"),\n        (\"two_words\", \"error handling\"),\n        (\"phrase\", \"async await patterns\"),\n        (\"complex\", \"rust error handling async\"),\n    ];\n    \n    for (name, query) in queries {\n        group.bench_with_input(\n            BenchmarkId::new(\"query\", name),\n            \u0026query,\n            |b, query| {\n                b.iter(|| engine.search(black_box(*query), 10))\n            },\n        );\n    }\n    \n    group.finish();\n    \n    // Benchmark scaling\n    let mut scaling_group = c.benchmark_group(\"search_scaling\");\n    for skill_count in [100, 500, 1000, 5000].iter() {\n        let temp = TempDir::new().unwrap();\n        let engine = setup_search_engine(\u0026temp, *skill_count);\n        \n        scaling_group.throughput(Throughput::Elements(*skill_count as u64));\n        scaling_group.bench_with_input(\n            BenchmarkId::new(\"skills\", skill_count),\n            skill_count,\n            |b, _| {\n                b.iter(|| engine.search(black_box(\"test query\"), 10))\n            },\n        );\n    }\n    \n    scaling_group.finish();\n}\n\nfn setup_search_engine(temp_dir: \u0026TempDir, skill_count: usize) -\u003e SearchEngine {\n    let mut engine = SearchEngine::new(temp_dir.path()).unwrap();\n    \n    for i in 0..skill_count {\n        engine.index_skill(\u0026format!(\"skill-{}\", i), \u0026format!(\n            \"Description for skill {} with various keywords like rust, async, error handling\",\n            i\n        )).unwrap();\n    }\n    \n    engine\n}\n\n// Target assertion for CI\n#[test]\nfn test_search_performance_target() {\n    use std::time::Instant;\n    \n    let temp_dir = TempDir::new().unwrap();\n    let engine = setup_search_engine(\u0026temp_dir, 1000);\n    \n    let mut times = Vec::new();\n    let queries = [\"rust\", \"error\", \"async\", \"handling\", \"patterns\"];\n    \n    for _ in 0..100 {\n        for query in \u0026queries {\n            let start = Instant::now();\n            let _ = engine.search(black_box(*query), 10);\n            times.push(start.elapsed());\n        }\n    }\n    \n    times.sort();\n    let p99 = times[times.len() * 99 / 100];\n    \n    println!(\"[PERF] search p99: {:?}\", p99);\n    assert!(\n        p99 \u003c std::time::Duration::from_millis(50),\n        \"search p99 exceeded 50ms target: {:?}\",\n        p99\n    );\n}\n```\n\n### 4. RRF Fusion Benchmarks\n\nTarget: **\u003c 10ms for combining rankings**\n\nCreate `benches/rrf_fusion.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId};\nuse ms::search::rrf::{rrf_fusion, RankedList};\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"rrf_fusion\");\n    \n    // Generate test rankings\n    let rankings: Vec\u003cRankedList\u003e = (0..3).map(|i| {\n        RankedList {\n            source: format!(\"source_{}\", i),\n            results: (0..100).map(|j| (format!(\"skill-{}\", j + i * 10), 1.0 / (j as f64 + 1.0))).collect(),\n        }\n    }).collect();\n    \n    // Benchmark different ranking sizes\n    for size in [10, 50, 100, 500].iter() {\n        let rankings: Vec\u003cRankedList\u003e = (0..3).map(|i| {\n            RankedList {\n                source: format!(\"source_{}\", i),\n                results: (0..*size).map(|j| (format!(\"skill-{}\", j), 1.0 / (j as f64 + 1.0))).collect(),\n            }\n        }).collect();\n        \n        group.bench_with_input(\n            BenchmarkId::new(\"ranking_size\", size),\n            \u0026rankings,\n            |b, rankings| {\n                b.iter(|| rrf_fusion(black_box(rankings), 60))\n            },\n        );\n    }\n    \n    // Benchmark different k values\n    for k in [20, 40, 60, 80, 100].iter() {\n        group.bench_with_input(\n            BenchmarkId::new(\"k_value\", k),\n            k,\n            |b, k| {\n                b.iter(|| rrf_fusion(black_box(\u0026rankings), *k))\n            },\n        );\n    }\n    \n    group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_rrf_fusion_performance_target() {\n    use std::time::Instant;\n    \n    let rankings: Vec\u003cRankedList\u003e = (0..5).map(|i| {\n        RankedList {\n            source: format!(\"source_{}\", i),\n            results: (0..1000).map(|j| (format!(\"skill-{}\", j), 1.0 / (j as f64 + 1.0))).collect(),\n        }\n    }).collect();\n    \n    let iterations = 100;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = rrf_fusion(black_box(\u0026rankings), 60);\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] rrf_fusion: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_millis(10),\n        \"rrf_fusion exceeded 10ms target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 5. Indexing Benchmarks\n\nTarget: **1000 skills/second**\n\nCreate `benches/indexing.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId, Throughput};\nuse ms::indexing::Indexer;\nuse tempfile::TempDir;\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"indexing\");\n    \n    // Generate test skills\n    let skills: Vec\u003c(String, String)\u003e = (0..1000).map(|i| {\n        (\n            format!(\"skill-{}\", i),\n            format!(\n                \"This is the description for skill number {}. It contains various keywords \\\n                 like rust, async, error handling, patterns, testing, and performance.\",\n                i\n            ),\n        )\n    }).collect();\n    \n    // Benchmark batch indexing\n    for batch_size in [10, 50, 100, 500, 1000].iter() {\n        let batch: Vec\u003c_\u003e = skills.iter().take(*batch_size).collect();\n        \n        group.throughput(Throughput::Elements(*batch_size as u64));\n        group.bench_with_input(\n            BenchmarkId::new(\"batch\", batch_size),\n            \u0026batch,\n            |b, batch| {\n                let temp_dir = TempDir::new().unwrap();\n                let mut indexer = Indexer::new(temp_dir.path()).unwrap();\n                \n                b.iter(|| {\n                    for (name, desc) in batch.iter() {\n                        indexer.index(black_box(name), black_box(desc)).unwrap();\n                    }\n                })\n            },\n        );\n    }\n    \n    group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_indexing_performance_target() {\n    use std::time::Instant;\n    \n    let temp_dir = TempDir::new().unwrap();\n    let mut indexer = Indexer::new(temp_dir.path()).unwrap();\n    \n    let skills: Vec\u003c(String, String)\u003e = (0..1000).map(|i| {\n        (\n            format!(\"skill-{}\", i),\n            format!(\"Description for skill {} with keywords\", i),\n        )\n    }).collect();\n    \n    let start = Instant::now();\n    for (name, desc) in \u0026skills {\n        indexer.index(name, desc).unwrap();\n    }\n    let elapsed = start.elapsed();\n    let per_skill = elapsed / 1000;\n    let skills_per_second = 1000.0 / elapsed.as_secs_f64();\n    \n    println!(\"[PERF] indexing: {:?} per skill ({:.0} skills/sec)\", per_skill, skills_per_second);\n    assert!(\n        skills_per_second \u003e= 1000.0,\n        \"indexing below 1000 skills/sec target: {:.0}\",\n        skills_per_second\n    );\n}\n```\n\n### 6. Load Benchmarks\n\nTarget: **\u003c 100ms for skill with dependencies**\n\nCreate `benches/loading.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId};\nuse ms::loading::SkillLoader;\nuse tempfile::TempDir;\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"loading\");\n    \n    // Setup: Create skills with dependencies\n    let temp_dir = TempDir::new().unwrap();\n    let loader = setup_skill_loader(\u0026temp_dir, 10);  // 10 skills with deps\n    \n    // Benchmark loading single skill\n    group.bench_function(\"single_skill\", |b| {\n        b.iter(|| loader.load(black_box(\"skill-0\")))\n    });\n    \n    // Benchmark loading skill with dependencies\n    group.bench_function(\"skill_with_deps\", |b| {\n        b.iter(|| loader.load_with_deps(black_box(\"skill-0\")))\n    });\n    \n    // Benchmark loading all skills\n    group.bench_function(\"all_skills\", |b| {\n        b.iter(|| loader.load_all())\n    });\n    \n    group.finish();\n}\n\nfn setup_skill_loader(temp_dir: \u0026TempDir, count: usize) -\u003e SkillLoader {\n    let mut loader = SkillLoader::new(temp_dir.path()).unwrap();\n    \n    for i in 0..count {\n        let deps = if i \u003e 0 {\n            vec![format!(\"skill-{}\", i - 1)]\n        } else {\n            vec![]\n        };\n        loader.register_skill(\u0026format!(\"skill-{}\", i), \u0026deps).unwrap();\n    }\n    \n    loader\n}\n\n// Target assertion for CI\n#[test]\nfn test_loading_performance_target() {\n    use std::time::Instant;\n    \n    let temp_dir = TempDir::new().unwrap();\n    let loader = setup_skill_loader(\u0026temp_dir, 10);\n    \n    let iterations = 100;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = loader.load_with_deps(black_box(\"skill-9\"));  // Skill with most deps\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] load_with_deps: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_millis(100),\n        \"load_with_deps exceeded 100ms target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 7. Pack Benchmarks\n\nTarget: **\u003c 50ms for constrained optimization**\n\nCreate `benches/packing.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId};\nuse ms::packing::{Packer, PackConstraints, Skill};\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"packing\");\n    \n    // Generate test skills\n    let skills: Vec\u003cSkill\u003e = (0..100).map(|i| {\n        Skill {\n            name: format!(\"skill-{}\", i),\n            tokens: 100 + (i * 10) as usize,\n            priority: 1.0 - (i as f64 / 100.0),\n        }\n    }).collect();\n    \n    // Benchmark different constraint sizes\n    for max_tokens in [1000, 5000, 10000, 50000].iter() {\n        let constraints = PackConstraints {\n            max_tokens: *max_tokens,\n            max_skills: 50,\n        };\n        \n        group.bench_with_input(\n            BenchmarkId::new(\"max_tokens\", max_tokens),\n            \u0026constraints,\n            |b, constraints| {\n                b.iter(|| Packer::pack(black_box(\u0026skills), black_box(constraints)))\n            },\n        );\n    }\n    \n    // Benchmark different skill counts\n    for skill_count in [10, 50, 100, 500].iter() {\n        let skills: Vec\u003cSkill\u003e = (0..*skill_count).map(|i| {\n            Skill {\n                name: format!(\"skill-{}\", i),\n                tokens: 100 + (i * 10) as usize,\n                priority: 1.0 - (i as f64 / *skill_count as f64),\n            }\n        }).collect();\n        \n        let constraints = PackConstraints {\n            max_tokens: 10000,\n            max_skills: 50,\n        };\n        \n        group.bench_with_input(\n            BenchmarkId::new(\"skill_count\", skill_count),\n            \u0026skill_count,\n            |b, _| {\n                b.iter(|| Packer::pack(black_box(\u0026skills), black_box(\u0026constraints)))\n            },\n        );\n    }\n    \n    group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_packing_performance_target() {\n    use std::time::Instant;\n    \n    let skills: Vec\u003cSkill\u003e = (0..100).map(|i| {\n        Skill {\n            name: format!(\"skill-{}\", i),\n            tokens: 100 + (i * 10) as usize,\n            priority: 1.0 - (i as f64 / 100.0),\n        }\n    }).collect();\n    \n    let constraints = PackConstraints {\n        max_tokens: 10000,\n        max_skills: 50,\n    };\n    \n    let iterations = 100;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = Packer::pack(black_box(\u0026skills), black_box(\u0026constraints));\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] pack: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_millis(50),\n        \"pack exceeded 50ms target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 8. CI Integration\n\nAdd benchmark checks to CI:\n\n```yaml\nbenchmark-tests:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v4\n    \n    - name: Install Rust toolchain\n      uses: dtolnay/rust-action@stable\n    \n    - name: Run benchmarks\n      run: cargo bench --no-run\n    \n    - name: Run performance target tests\n      run: |\n        cargo test test_hash_embedding_performance_target -- --nocapture\n        cargo test test_search_performance_target -- --nocapture\n        cargo test test_rrf_fusion_performance_target -- --nocapture\n        cargo test test_indexing_performance_target -- --nocapture\n        cargo test test_loading_performance_target -- --nocapture\n        cargo test test_packing_performance_target -- --nocapture\n    \n    - name: Compare with baseline (on main branch)\n      if: github.ref == 'refs/heads/main'\n      run: |\n        cargo bench -- --save-baseline main\n    \n    - name: Check for regression (on PR)\n      if: github.event_name == 'pull_request'\n      run: |\n        cargo bench -- --baseline main\n```\n\n### 9. Performance Targets Summary\n\n| Operation | Target | Test |\n|-----------|--------|------|\n| hash_embedding | \u003c 1μs | test_hash_embedding_performance_target |\n| search (p99, 1000 skills) | \u003c 50ms | test_search_performance_target |\n| rrf_fusion | \u003c 10ms | test_rrf_fusion_performance_target |\n| indexing | 1000 skills/sec | test_indexing_performance_target |\n| load (with deps) | \u003c 100ms | test_loading_performance_target |\n| pack | \u003c 50ms | test_packing_performance_target |\n\n## Acceptance Criteria\n\n1. [ ] Criterion benchmark suite configured\n2. [ ] hash_embedding benchmark with \u003c 1μs target\n3. [ ] search benchmark with \u003c 50ms p99 target\n4. [ ] rrf_fusion benchmark with \u003c 10ms target\n5. [ ] indexing benchmark with 1000 skills/sec target\n6. [ ] load benchmark with \u003c 100ms target\n7. [ ] pack benchmark with \u003c 50ms target\n8. [ ] Performance target tests that fail on regression\n9. [ ] CI integration with baseline comparison\n10. [ ] HTML benchmark reports generated\n\n## Dependencies\n\n- meta_skill-5s0 (Rust Project Scaffolding) - provides project structure","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:57:39.72055569-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.885220875-05:00","labels":["benchmarks","performance","testing"],"dependencies":[{"issue_id":"meta_skill-ftb","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:57:44.88901878-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ftb","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.205437078-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ftj","title":"Tech Stack Detection","description":"# Tech Stack Detection\n\n## Overview\n\nDetect project language/framework/tooling to improve suggestion relevance, packing, and freshness scoring. This enables context‑aware suggestion without relying on brittle heuristics.\n\n---\n\n## Signals\n\n- Presence of `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`.\n- Framework identifiers (React, Next.js, Django, etc.).\n- Build tools (pnpm, yarn, poetry).\n\n---\n\n## Tasks\n\n1. Implement `TechStackDetector` that scans repo roots.\n2. Emit normalized stack labels (language + framework + build tool).\n3. Cache detection results per repo.\n4. Expose via `ms doctor --check=toolchain`.\n\n---\n\n## Testing Requirements\n\n- Unit tests for detection heuristics.\n- Integration tests with fixture repos.\n- Regression tests for false positives.\n\n---\n\n## Acceptance Criteria\n\n- Correctly detects common stacks (Rust, Go, JS/TS, Python).\n- Emits stable, normalized labels.\n- Caches results to avoid repeated scans.\n\n---\n\n## Dependencies\n\n- `meta_skill-qs1` SQLite Database Layer (cache storage)","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:59:50.156123903-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:57:22.880772257-05:00","labels":["detection","phase-3","techstack"],"dependencies":[{"issue_id":"meta_skill-ftj","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:57:31.804928294-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-fus","title":"[P1] Two-Phase Commit (2PC)","description":"## Overview\n\nImplement two-phase commit (2PC) for dual persistence to SQLite and Git. All writes that touch both stores are wrapped in a lightweight transaction protocol to prevent split-brain states where one store is updated but the other fails.\n\n## Background \u0026 Rationale\n\n### Why Two-Phase Commit\n\nWithout 2PC, failures can leave the system in inconsistent states:\n- SQLite updated but Git commit fails → data visible in queries but not persisted to archive\n- Git committed but SQLite fails → archive has data that queries can't find\n- Recovery unclear → which store is source of truth?\n\n2PC ensures atomic all-or-nothing semantics across both stores.\n\n### The Protocol\n\n1. **Prepare Phase**: Write intent to tx_log, stage changes in both stores\n2. **Commit Phase**: Finalize Git commit, mark SQLite complete\n3. **Complete Phase**: Clean up tx_log record\n4. **Recovery**: On startup, scan tx_log for incomplete transactions\n\n### Global File Locking\n\nConcurrent access (parallel agents, IDE indexer + CLI) requires coordination.\nWe use advisory file locking on `.ms/ms.lock` to serialize writes.\n\n---\n\n## Key Data Structures (from Plan Section 3.7)\n\n### Transaction Manager\n\n```rust\nuse rusqlite::Connection;\nuse std::path::{Path, PathBuf};\nuse chrono::{DateTime, Utc};\nuse serde::{Serialize, Deserialize};\n\npub struct TxManager {\n    db: Connection,\n    git: GitArchive,\n    tx_dir: PathBuf, // .ms/tx/\n    ms_root: PathBuf,\n}\n\nimpl TxManager {\n    pub fn new(db: Connection, git: GitArchive, ms_root: PathBuf) -\u003e Self {\n        let tx_dir = ms_root.join(\"tx\");\n        std::fs::create_dir_all(\u0026tx_dir).ok();\n        Self { db, git, tx_dir, ms_root }\n    }\n    \n    /// Write a skill with 2PC guarantees\n    pub fn write_skill(\u0026self, skill: \u0026SkillSpec) -\u003e Result\u003c()\u003e {\n        // Create transaction record\n        let tx = TxRecord::prepare(\"skill\", \u0026skill.id, skill)?;\n        \n        // Phase 1: Prepare\n        self.write_tx_record(\u0026tx)?;\n        self.db_write_pending(\u0026tx)?;\n        \n        // Phase 2: Commit\n        self.git_commit(\u0026tx)?;\n        self.db_mark_committed(\u0026tx)?;\n        \n        // Cleanup\n        self.cleanup_tx(\u0026tx)?;\n        \n        Ok(())\n    }\n    \n    /// Write transaction record to tx_log table and tx_dir\n    fn write_tx_record(\u0026self, tx: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        // Write to SQLite tx_log\n        self.db.execute(\n            \"INSERT INTO tx_log (id, entity_type, entity_id, phase, payload_json, created_at)\n             VALUES (?, ?, ?, ?, ?, ?)\",\n            params![\n                tx.id,\n                tx.entity_type,\n                tx.entity_id,\n                \"prepare\",\n                tx.payload_json,\n                tx.created_at.to_rfc3339(),\n            ],\n        )?;\n        \n        // Write to filesystem for crash recovery\n        let tx_path = self.tx_dir.join(format!(\"{}.json\", tx.id));\n        let tx_json = serde_json::to_string_pretty(tx)?;\n        std::fs::write(\u0026tx_path, tx_json)?;\n        \n        Ok(())\n    }\n    \n    /// Write to SQLite in pending state\n    fn db_write_pending(\u0026self, tx: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        // Deserialize payload and write to skills table\n        let skill: SkillSpec = serde_json::from_str(\u0026tx.payload_json)?;\n        \n        self.db.execute(\n            \"INSERT OR REPLACE INTO skills \n             (id, name, description, source_path, source_layer, content_hash, \n              body, metadata_json, assets_json, token_count, quality_score, \n              indexed_at, modified_at)\n             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'))\",\n            params![\n                skill.id,\n                skill.metadata.name,\n                skill.metadata.description,\n                \"pending\", // Will update after git commit\n                \"user\",\n                \"pending\",\n                \"\", // Body populated after compile\n                serde_json::to_string(\u0026skill.metadata)?,\n                serde_json::to_string(\u0026skill.assets)?,\n                0,\n                0.0,\n            ],\n        )?;\n        \n        // Update tx_log phase\n        self.db.execute(\n            \"UPDATE tx_log SET phase = 'pending' WHERE id = ?\",\n            [\u0026tx.id],\n        )?;\n        \n        Ok(())\n    }\n    \n    /// Commit to Git archive\n    fn git_commit(\u0026self, tx: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        let skill: SkillSpec = serde_json::from_str(\u0026tx.payload_json)?;\n        self.git.write_skill(\u0026skill)?;\n        \n        // Update tx_log phase\n        self.db.execute(\n            \"UPDATE tx_log SET phase = 'committed' WHERE id = ?\",\n            [\u0026tx.id],\n        )?;\n        \n        Ok(())\n    }\n    \n    /// Mark SQLite record as committed\n    fn db_mark_committed(\u0026self, tx: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        let skill: SkillSpec = serde_json::from_str(\u0026tx.payload_json)?;\n        \n        // Update with final values\n        self.db.execute(\n            \"UPDATE skills SET \n             source_path = ?,\n             content_hash = ?\n             WHERE id = ?\",\n            params![\n                self.git.skill_path(\u0026skill.id).to_string_lossy(),\n                compute_content_hash(\u0026skill)?,\n                skill.id,\n            ],\n        )?;\n        \n        // Update tx_log phase\n        self.db.execute(\n            \"UPDATE tx_log SET phase = 'complete' WHERE id = ?\",\n            [\u0026tx.id],\n        )?;\n        \n        Ok(())\n    }\n    \n    /// Clean up completed transaction\n    fn cleanup_tx(\u0026self, tx: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        // Remove from tx_log table\n        self.db.execute(\n            \"DELETE FROM tx_log WHERE id = ?\",\n            [\u0026tx.id],\n        )?;\n        \n        // Remove tx file\n        let tx_path = self.tx_dir.join(format!(\"{}.json\", tx.id));\n        std::fs::remove_file(\u0026tx_path).ok();\n        \n        Ok(())\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TxRecord {\n    pub id: String,\n    pub entity_type: String,\n    pub entity_id: String,\n    pub phase: String,\n    pub payload_json: String,\n    pub created_at: DateTime\u003cUtc\u003e,\n}\n\nimpl TxRecord {\n    pub fn prepare\u003cT: Serialize\u003e(entity_type: \u0026str, entity_id: \u0026str, payload: \u0026T) -\u003e Result\u003cSelf\u003e {\n        Ok(Self {\n            id: uuid::Uuid::new_v4().to_string(),\n            entity_type: entity_type.to_string(),\n            entity_id: entity_id.to_string(),\n            phase: \"prepare\".to_string(),\n            payload_json: serde_json::to_string(payload)?,\n            created_at: Utc::now(),\n        })\n    }\n}\n```\n\n### Global File Locking (Section 3.7.1)\n\n```rust\nuse std::fs::{File, OpenOptions};\nuse std::io;\nuse std::path::{Path, PathBuf};\nuse std::time::Duration;\n\n/// Advisory file lock for coordinating dual-persistence writes\npub struct GlobalLock {\n    lock_file: File,\n    lock_path: PathBuf,\n}\n\nimpl GlobalLock {\n    const LOCK_FILENAME: \u0026'static str = \".ms/ms.lock\";\n\n    /// Acquire exclusive lock (blocking)\n    pub fn acquire(ms_root: \u0026Path) -\u003e io::Result\u003cSelf\u003e {\n        let lock_path = ms_root.join(Self::LOCK_FILENAME);\n        std::fs::create_dir_all(lock_path.parent().unwrap())?;\n\n        let lock_file = OpenOptions::new()\n            .read(true)\n            .write(true)\n            .create(true)\n            .open(\u0026lock_path)?;\n\n        #[cfg(unix)]\n        {\n            use std::os::unix::io::AsRawFd;\n            let fd = lock_file.as_raw_fd();\n            // LOCK_EX = exclusive, blocks until acquired\n            unsafe { libc::flock(fd, libc::LOCK_EX) };\n        }\n\n        #[cfg(windows)]\n        {\n            use std::os::windows::io::AsRawHandle;\n            use winapi::um::fileapi::LockFileEx;\n            use winapi::um::minwinbase::LOCKFILE_EXCLUSIVE_LOCK;\n            let handle = lock_file.as_raw_handle();\n            unsafe {\n                let mut overlapped = std::mem::zeroed();\n                LockFileEx(\n                    handle as *mut _,\n                    LOCKFILE_EXCLUSIVE_LOCK,\n                    0,\n                    !0,\n                    !0,\n                    \u0026mut overlapped,\n                );\n            }\n        }\n\n        Ok(Self { lock_file, lock_path })\n    }\n\n    /// Try to acquire lock without blocking\n    pub fn try_acquire(ms_root: \u0026Path) -\u003e io::Result\u003cOption\u003cSelf\u003e\u003e {\n        let lock_path = ms_root.join(Self::LOCK_FILENAME);\n        std::fs::create_dir_all(lock_path.parent().unwrap())?;\n\n        let lock_file = OpenOptions::new()\n            .read(true)\n            .write(true)\n            .create(true)\n            .open(\u0026lock_path)?;\n\n        #[cfg(unix)]\n        {\n            use std::os::unix::io::AsRawFd;\n            let fd = lock_file.as_raw_fd();\n            // LOCK_NB = non-blocking\n            let result = unsafe { libc::flock(fd, libc::LOCK_EX | libc::LOCK_NB) };\n            if result != 0 {\n                return Ok(None); // Lock held by another process\n            }\n        }\n\n        Ok(Some(Self { lock_file, lock_path }))\n    }\n\n    /// Acquire with timeout (polling fallback for portability)\n    pub fn acquire_timeout(ms_root: \u0026Path, timeout: Duration) -\u003e io::Result\u003cOption\u003cSelf\u003e\u003e {\n        let start = std::time::Instant::now();\n        let poll_interval = Duration::from_millis(50);\n\n        while start.elapsed() \u003c timeout {\n            if let Some(lock) = Self::try_acquire(ms_root)? {\n                return Ok(Some(lock));\n            }\n            std::thread::sleep(poll_interval);\n        }\n\n        Ok(None)\n    }\n}\n\nimpl Drop for GlobalLock {\n    fn drop(\u0026mut self) {\n        #[cfg(unix)]\n        {\n            use std::os::unix::io::AsRawFd;\n            let fd = self.lock_file.as_raw_fd();\n            unsafe { libc::flock(fd, libc::LOCK_UN) };\n        }\n\n        #[cfg(windows)]\n        {\n            use std::os::windows::io::AsRawHandle;\n            use winapi::um::fileapi::UnlockFileEx;\n            let handle = self.lock_file.as_raw_handle();\n            unsafe {\n                let mut overlapped = std::mem::zeroed();\n                UnlockFileEx(handle as *mut _, 0, !0, !0, \u0026mut overlapped);\n            }\n        }\n    }\n}\n```\n\n### Locked TxManager\n\n```rust\nimpl TxManager {\n    /// Write skill with global lock coordination\n    pub fn write_skill_locked(\u0026self, skill: \u0026SkillSpec) -\u003e Result\u003c()\u003e {\n        let _lock = GlobalLock::acquire_timeout(\u0026self.ms_root, Duration::from_secs(30))\n            .map_err(|e| anyhow!(\"Failed to acquire lock: {}\", e))?\n            .ok_or_else(|| anyhow!(\"Timeout waiting for global lock\"))?;\n\n        self.write_skill(skill)\n        // Lock released on drop\n    }\n\n    /// Batch write with single lock acquisition\n    pub fn write_skills_batch(\u0026self, skills: \u0026[SkillSpec]) -\u003e Result\u003c()\u003e {\n        let _lock = GlobalLock::acquire(\u0026self.ms_root)?;\n\n        for skill in skills {\n            self.write_skill(skill)?;\n        }\n\n        Ok(())\n    }\n}\n```\n\n### Recovery on Startup\n\n```rust\nimpl TxManager {\n    /// Recover from incomplete transactions on startup\n    pub fn recover(\u0026self) -\u003e Result\u003cRecoveryReport\u003e {\n        let mut report = RecoveryReport::default();\n        \n        // Find incomplete transactions in tx_log\n        let mut stmt = self.db.prepare(\n            \"SELECT id, entity_type, entity_id, phase, payload_json, created_at \n             FROM tx_log WHERE phase != 'complete'\"\n        )?;\n        \n        let txs: Vec\u003cTxRecord\u003e = stmt.query_map([], |row| {\n            Ok(TxRecord {\n                id: row.get(0)?,\n                entity_type: row.get(1)?,\n                entity_id: row.get(2)?,\n                phase: row.get(3)?,\n                payload_json: row.get(4)?,\n                created_at: DateTime::parse_from_rfc3339(\u0026row.get::\u003c_, String\u003e(5)?)\n                    .unwrap()\n                    .with_timezone(\u0026Utc),\n            })\n        })?.collect::\u003cResult\u003cVec\u003c_\u003e, _\u003e\u003e()?;\n        \n        for tx in txs {\n            match tx.phase.as_str() {\n                \"prepare\" =\u003e {\n                    // Transaction never started - roll back\n                    tracing::info!(\"Rolling back prepare-only tx: {}\", tx.id);\n                    self.rollback_tx(\u0026tx)?;\n                    report.rolled_back += 1;\n                }\n                \"pending\" =\u003e {\n                    // SQLite written but Git not committed - roll back\n                    tracing::info!(\"Rolling back pending tx: {}\", tx.id);\n                    self.rollback_tx(\u0026tx)?;\n                    report.rolled_back += 1;\n                }\n                \"committed\" =\u003e {\n                    // Git committed but not marked complete - complete it\n                    tracing::info!(\"Completing committed tx: {}\", tx.id);\n                    self.db_mark_committed(\u0026tx)?;\n                    self.cleanup_tx(\u0026tx)?;\n                    report.completed += 1;\n                }\n                _ =\u003e {\n                    tracing::warn!(\"Unknown tx phase: {} for {}\", tx.phase, tx.id);\n                }\n            }\n        }\n        \n        // Also check tx_dir for orphaned tx files\n        if self.tx_dir.exists() {\n            for entry in std::fs::read_dir(\u0026self.tx_dir)? {\n                let entry = entry?;\n                if entry.path().extension() == Some(std::ffi::OsStr::new(\"json\")) {\n                    let tx_json = std::fs::read_to_string(entry.path())?;\n                    let tx: TxRecord = serde_json::from_str(\u0026tx_json)?;\n                    \n                    // Check if in database\n                    let in_db: bool = self.db.query_row(\n                        \"SELECT EXISTS(SELECT 1 FROM tx_log WHERE id = ?)\",\n                        [\u0026tx.id],\n                        |row| row.get(0),\n                    )?;\n                    \n                    if !in_db {\n                        tracing::warn!(\"Orphaned tx file: {}\", tx.id);\n                        std::fs::remove_file(entry.path())?;\n                        report.orphaned_files += 1;\n                    }\n                }\n            }\n        }\n        \n        Ok(report)\n    }\n    \n    /// Roll back a transaction\n    fn rollback_tx(\u0026self, tx: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        // Remove from skills table\n        self.db.execute(\n            \"DELETE FROM skills WHERE id = ? AND source_path = 'pending'\",\n            [\u0026tx.entity_id],\n        )?;\n        \n        // Remove from tx_log\n        self.db.execute(\n            \"DELETE FROM tx_log WHERE id = ?\",\n            [\u0026tx.id],\n        )?;\n        \n        // Remove tx file\n        let tx_path = self.tx_dir.join(format!(\"{}.json\", tx.id));\n        std::fs::remove_file(\u0026tx_path).ok();\n        \n        Ok(())\n    }\n}\n\n#[derive(Debug, Default)]\npub struct RecoveryReport {\n    pub rolled_back: usize,\n    pub completed: usize,\n    pub orphaned_files: usize,\n}\n```\n\n---\n\n## Lock Behavior by Command\n\n| Command | Lock Type | Rationale |\n|---------|-----------|-----------|\n| `ms index` | Exclusive | Bulk writes to both stores |\n| `ms load` | None (read-only) | SQLite WAL handles read concurrency |\n| `ms search` | None (read-only) | FTS queries are read-only |\n| `ms suggest` | None (read-only) | Query-only operation |\n| `ms edit` | Exclusive | Modifies SkillSpec, re-renders SKILL.md, updates SQLite |\n| `ms mine` | Exclusive | Writes new skills |\n| `ms calibrate` | Exclusive | Updates rule strengths |\n| `ms doctor --fix` | Exclusive | May modify both stores |\n\n---\n\n## Diagnostics\n\n```bash\n# Check lock status\nms doctor --check-lock\n\n# Force break stale lock (with pid check)\nms doctor --break-lock\n\n# Show lock holder\nms lock status\n```\n\nThe lock file includes a JSON payload with holder PID and timestamp, enabling\nstale lock detection (process no longer running) and diagnostics.\n\n---\n\n## Tasks\n\n### Task 1: TxManager Core\n- [ ] Create src/storage/tx.rs module\n- [ ] Implement TxManager struct\n- [ ] Implement TxRecord with prepare()\n- [ ] Create tx_dir on initialization\n\n### Task 2: Prepare Phase\n- [ ] Implement write_tx_record()\n- [ ] Write to tx_log table\n- [ ] Write to tx_dir filesystem\n- [ ] Generate UUID transaction ID\n\n### Task 3: Pending Phase\n- [ ] Implement db_write_pending()\n- [ ] Insert skill with 'pending' source_path\n- [ ] Update tx_log phase\n\n### Task 4: Commit Phase\n- [ ] Implement git_commit()\n- [ ] Write to Git archive\n- [ ] Update tx_log phase to 'committed'\n\n### Task 5: Complete Phase\n- [ ] Implement db_mark_committed()\n- [ ] Update skill with final values\n- [ ] Implement cleanup_tx()\n- [ ] Remove from tx_log and tx_dir\n\n### Task 6: Global Locking\n- [ ] Implement GlobalLock struct\n- [ ] Implement acquire() with flock\n- [ ] Implement try_acquire() non-blocking\n- [ ] Implement acquire_timeout() polling\n- [ ] Cross-platform support (Unix/Windows)\n\n### Task 7: Locked Operations\n- [ ] Implement write_skill_locked()\n- [ ] Implement write_skills_batch()\n- [ ] 30-second default timeout\n- [ ] Proper lock release on drop\n\n### Task 8: Recovery\n- [ ] Implement recover() on startup\n- [ ] Handle 'prepare' phase: rollback\n- [ ] Handle 'pending' phase: rollback\n- [ ] Handle 'committed' phase: complete\n- [ ] Clean orphaned tx files\n\n### Task 9: Diagnostics\n- [ ] Implement --check-lock for doctor\n- [ ] Implement --break-lock for doctor\n- [ ] Implement ms lock status command\n- [ ] Write PID/timestamp to lock file\n\n---\n\n## Acceptance Criteria\n\n1. **Atomic Writes**: Either both stores updated or neither\n2. **Recovery Works**: Startup recovers from any failure point\n3. **Lock Coordination**: Concurrent processes don't corrupt data\n4. **Timeout Handles**: Stale locks can be broken\n5. **Clean State**: No orphaned tx records after normal operation\n\n---\n\n## Testing Requirements\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n\n    #[test]\n    fn test_successful_2pc() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        let git = GitArchive::open(dir.path().join(\"archive\")).unwrap();\n        let tx = TxManager::new(db.conn, git, dir.path().to_path_buf());\n        \n        let skill = SkillSpec { id: \"test\".to_string(), /* ... */ };\n        tx.write_skill(\u0026skill).unwrap();\n        \n        // Verify in both stores\n        assert!(dir.path().join(\"archive/skills/by-id/test\").exists());\n        // Verify tx_log is empty\n        let count: i32 = db.conn.query_row(\n            \"SELECT COUNT(*) FROM tx_log\",\n            [],\n            |row| row.get(0),\n        ).unwrap();\n        assert_eq!(count, 0);\n    }\n\n    #[test]\n    fn test_recovery_from_pending() {\n        let dir = tempdir().unwrap();\n        // Simulate crash after pending phase\n        // ... setup incomplete tx in tx_log\n        \n        let tx = TxManager::new(/* ... */);\n        let report = tx.recover().unwrap();\n        \n        assert_eq!(report.rolled_back, 1);\n    }\n\n    #[test]\n    fn test_lock_acquisition() {\n        let dir = tempdir().unwrap();\n        let lock1 = GlobalLock::acquire(dir.path()).unwrap();\n        \n        // Second lock should fail with try_acquire\n        let lock2 = GlobalLock::try_acquire(dir.path()).unwrap();\n        assert!(lock2.is_none());\n        \n        // Release first lock\n        drop(lock1);\n        \n        // Now second should succeed\n        let lock3 = GlobalLock::try_acquire(dir.path()).unwrap();\n        assert!(lock3.is_some());\n    }\n}\n```\n\n---\n\n## Logging Requirements\n\n- **DEBUG**: Phase transitions, tx IDs, lock acquire/release\n- **INFO**: Transaction started/completed, recovery actions\n- **WARN**: Lock timeout, incomplete transactions found\n- **ERROR**: Recovery failures, lock acquisition failures\n\n---\n\n## References\n\n- **Plan Section 3.7**: Two-Phase Commit for Dual Persistence\n- **Plan Section 3.7.1**: Global File Locking\n- **Depends on**: meta_skill-qs1 (SQLite), meta_skill-b98 (Git Archive)\n- **Blocks**: All write operations in CLI","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:02.680560145-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:02:31.503611974-05:00","labels":["phase-1","safety","transaction"],"dependencies":[{"issue_id":"meta_skill-fus","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:22:14.902149258-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-fus","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:22:14.928783293-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-hax","title":"CASS Mining: Caching/Memoization Patterns","description":"Deep dive into topk heap-based collectors, lazy cached accessors (TriageContext), memoization patterns, LRU cache implementations.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:29.587581417-05:00","created_by":"ubuntu","updated_at":"2026-01-13T20:59:57.634708683-05:00","closed_at":"2026-01-13T20:59:57.634708683-05:00","close_reason":"Completed CASS mining for caching/memoization patterns. Added Section 36 (~1105 lines) covering: lazy initialization (OnceLock, sync.Once), TriageContext pattern for unified lazy caching, heap-based top-K collectors (Go generics + Rust BinaryHeap), LRU cache with disk persistence, in-memory cache with TTL, SIMD-optimized dot product, parallel k-NN search with thread-local heaps, cache-efficient SoA data layouts, hash-based content deduplication, and cache invalidation strategies. Sources: beads_viewer, xf, cass vector search implementations.","labels":["cass-mining"]}
{"id":"meta_skill-hhu","title":"[P4] CASS Client Integration","description":"# CASS Client Integration\n\n## Overview\n\nIntegrate CASS as the session source of truth. Provide a typed client that can list sessions, fetch transcripts, and run searches with consistent JSON output.\n\n---\n\n## Core Capabilities\n\n- `cass health` for readiness checks.\n- `cass search ... --robot` for targeted retrieval.\n- `cass view/expand` for evidence extraction.\n- Session metadata queries (project, agent, timestamp).\n\n---\n\n## Tasks\n\n1. Implement `CassClient` wrapper (exec + JSON decode).\n2. Add retry + error classification (not found vs transient).\n3. Provide search helpers for mining queries.\n4. Normalize session IDs and timestamps.\n\n---\n\n## Testing Requirements\n\n- Unit tests for JSON decoding and error handling.\n- Integration test with sample CASS fixture.\n- E2E: `ms doctor --check=cass` success/fail flows.\n\n---\n\n## Acceptance Criteria\n\n- CASS commands invoked reliably with `--robot`.\n- Errors are classified and actionable.\n- All session fetches return deterministic data.","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:45.444283967-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:53:22.137784957-05:00","labels":["cass","integration","phase-4"],"dependencies":[{"issue_id":"meta_skill-hhu","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:26:12.90516122-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-hzg","title":"CASS Mining: APR Iterative Refinement Patterns","description":"Deep dive into APR (Automated Plan Reviser Pro) sessions - iterative specification refinement, steady state convergence, robot mode JSON API for automation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:30.342465618-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:09:50.920724713-05:00","closed_at":"2026-01-13T18:09:50.920724713-05:00","close_reason":"Section 29 added to plan: APR iterative refinement patterns, convergence algorithm, grounded abstraction principle, reliability features, dual interface pattern","labels":["cass-mining"]}
{"id":"meta_skill-igx","title":"[P1] Global File Locking","description":"# Global File Locking\n\n## Overview\n\nPrevent concurrent write corruption across multiple ms processes by enforcing a global exclusive lock for write‑heavy commands (index, build, edit, doctor --fix). Read operations remain lock‑free.\n\n---\n\n## Tasks\n\n1. Implement `GlobalLock` using file locks (fs2 / platform APIs).\n2. Store lock metadata (PID, timestamp, command, host).\n3. Stale lock detection + safe break.\n4. Wire lock acquisition in write commands.\n5. Provide `ms lock status` and `ms doctor --check-lock`.\n\n---\n\n## Testing Requirements\n\n- Unit tests for lock acquire/release.\n- Integration test: two concurrent writers → one blocked.\n- Stale lock recovery test.\n\n---\n\n## Acceptance Criteria\n\n- No concurrent writes to SQLite/Git.\n- Stale locks detected and reported.\n- Diagnostics visible via doctor.\n\n---\n\n## Dependencies\n\n- `meta_skill-fus` Two‑Phase Commit\n- `meta_skill-q3l` Doctor Command","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:22:03.294997429-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:01:45.644768047-05:00","labels":["concurrency","locking","phase-1"],"dependencies":[{"issue_id":"meta_skill-igx","depends_on_id":"meta_skill-fus","type":"blocks","created_at":"2026-01-13T22:22:14.954734941-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-iim","title":"Skill Effectiveness Feedback Loop","description":"# Skill Effectiveness Feedback Loop\n\n**Phase 6 - Section 22**\n\nTrack whether skills actually help agents complete tasks successfully. This feature implements usage tracking, feedback collection, quality score updates, and A/B experimentation to continuously improve skill effectiveness.\n\n---\n\n## Overview\n\nNot all skills are equally helpful. Some may be outdated, too generic, or simply wrong. The effectiveness feedback loop measures real-world skill performance by:\n\n1. **Usage Tracking**: Record when skills are retrieved and used\n2. **Feedback Collection**: Gather explicit and implicit feedback on skill helpfulness\n3. **Quality Score Updates**: Adjust skill rankings based on evidence\n4. **A/B Experiments**: Test skill variations to find what works best\n\n---\n\n## Core Data Structures\n\n### Effectiveness Tracker\n\n```rust\nuse chrono::{DateTime, Utc};\nuse rusqlite::Connection;\nuse std::collections::HashMap;\n\n/// Main effectiveness tracking system\npub struct EffectivenessTracker {\n    /// SQLite database for persistent storage\n    db: Database,\n    \n    /// CASS client for session context\n    cass: CassClient,\n    \n    /// In-memory cache of recent events\n    event_cache: EventCache,\n    \n    /// Active experiments\n    experiments: HashMap\u003cString, SkillExperiment\u003e,\n}\n\n/// Database wrapper with effectiveness-specific operations\npub struct Database {\n    conn: Connection,\n}\n\nimpl Database {\n    /// Initialize effectiveness tracking tables\n    pub fn init_schema(\u0026self) -\u003e Result\u003c(), DbError\u003e {\n        self.conn.execute_batch(r#\"\n            -- Skill usage events\n            CREATE TABLE IF NOT EXISTS skill_usage (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                skill_id TEXT NOT NULL,\n                session_id TEXT NOT NULL,\n                timestamp TEXT NOT NULL,\n                context_type TEXT NOT NULL,\n                retrieval_rank INTEGER,\n                tokens_used INTEGER,\n                experiment_id TEXT,\n                variant_id TEXT\n            );\n            \n            -- Explicit feedback\n            CREATE TABLE IF NOT EXISTS skill_feedback (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                skill_id TEXT NOT NULL,\n                session_id TEXT NOT NULL,\n                timestamp TEXT NOT NULL,\n                feedback_type TEXT NOT NULL,\n                rating INTEGER,\n                comment TEXT,\n                section TEXT,\n                slice TEXT\n            );\n            \n            -- Session outcomes (implicit feedback)\n            CREATE TABLE IF NOT EXISTS session_outcomes (\n                session_id TEXT PRIMARY KEY,\n                skills_used TEXT NOT NULL,  -- JSON array\n                outcome TEXT NOT NULL,\n                duration_seconds INTEGER,\n                error_count INTEGER,\n                completion_signals TEXT  -- JSON\n            );\n            \n            -- Aggregated skill scores\n            CREATE TABLE IF NOT EXISTS skill_scores (\n                skill_id TEXT PRIMARY KEY,\n                usage_count INTEGER DEFAULT 0,\n                positive_feedback INTEGER DEFAULT 0,\n                negative_feedback INTEGER DEFAULT 0,\n                success_rate REAL DEFAULT 0.5,\n                avg_helpfulness REAL DEFAULT 0.5,\n                last_updated TEXT NOT NULL,\n                score_version INTEGER DEFAULT 1\n            );\n            \n            -- Experiments\n            CREATE TABLE IF NOT EXISTS experiments (\n                id TEXT PRIMARY KEY,\n                skill_id TEXT NOT NULL,\n                scope TEXT NOT NULL,\n                status TEXT NOT NULL,\n                created_at TEXT NOT NULL,\n                started_at TEXT,\n                ended_at TEXT,\n                config TEXT NOT NULL  -- JSON\n            );\n            \n            -- Experiment variants\n            CREATE TABLE IF NOT EXISTS experiment_variants (\n                id TEXT PRIMARY KEY,\n                experiment_id TEXT NOT NULL,\n                name TEXT NOT NULL,\n                content TEXT NOT NULL,\n                allocation_percent REAL NOT NULL,\n                usage_count INTEGER DEFAULT 0,\n                success_count INTEGER DEFAULT 0,\n                FOREIGN KEY (experiment_id) REFERENCES experiments(id)\n            );\n            \n            -- Indexes\n            CREATE INDEX IF NOT EXISTS idx_usage_skill ON skill_usage(skill_id);\n            CREATE INDEX IF NOT EXISTS idx_usage_session ON skill_usage(session_id);\n            CREATE INDEX IF NOT EXISTS idx_feedback_skill ON skill_feedback(skill_id);\n            CREATE INDEX IF NOT EXISTS idx_outcomes_session ON session_outcomes(session_id);\n        \"#)?;\n        Ok(())\n    }\n}\n\n/// Cache for recent events before batch persistence\npub struct EventCache {\n    usage_events: Vec\u003cUsageEvent\u003e,\n    feedback_events: Vec\u003cFeedbackEvent\u003e,\n    max_size: usize,\n    flush_interval: std::time::Duration,\n    last_flush: std::time::Instant,\n}\n\nimpl EventCache {\n    pub fn new(max_size: usize, flush_interval_secs: u64) -\u003e Self {\n        Self {\n            usage_events: Vec::new(),\n            feedback_events: Vec::new(),\n            max_size,\n            flush_interval: std::time::Duration::from_secs(flush_interval_secs),\n            last_flush: std::time::Instant::now(),\n        }\n    }\n    \n    pub fn should_flush(\u0026self) -\u003e bool {\n        self.usage_events.len() \u003e= self.max_size\n            || self.feedback_events.len() \u003e= self.max_size\n            || self.last_flush.elapsed() \u003e= self.flush_interval\n    }\n}\n```\n\n### Usage Events\n\n```rust\n/// Record of a skill being used\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UsageEvent {\n    /// Skill that was used\n    pub skill_id: SkillId,\n    \n    /// Session in which skill was used\n    pub session_id: SessionId,\n    \n    /// When the skill was retrieved\n    pub timestamp: DateTime\u003cUtc\u003e,\n    \n    /// Context that triggered retrieval\n    pub context: UsageContext,\n    \n    /// Position in retrieval results (1 = top result)\n    pub retrieval_rank: Option\u003cu32\u003e,\n    \n    /// Tokens consumed by this skill\n    pub tokens_used: u32,\n    \n    /// If part of an experiment\n    pub experiment_info: Option\u003cExperimentAssignment\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UsageContext {\n    /// Type of suggestion context\n    pub context_type: ContextType,\n    \n    /// Query that triggered retrieval (if any)\n    pub query: Option\u003cString\u003e,\n    \n    /// Files being worked on\n    pub active_files: Vec\u003cString\u003e,\n    \n    /// Project type\n    pub project_type: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ContextType {\n    /// Automatic suggestion based on context\n    Automatic,\n    \n    /// Explicit query via `ms search`\n    ExplicitSearch,\n    \n    /// Direct access via `ms show \u003cskill\u003e`\n    DirectAccess,\n    \n    /// MCP server suggestion\n    McpSuggestion,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentAssignment {\n    pub experiment_id: String,\n    pub variant_id: String,\n}\n```\n\n### Feedback Types\n\n```rust\n/// Explicit feedback on skill helpfulness\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FeedbackEvent {\n    /// Skill being rated\n    pub skill_id: SkillId,\n    \n    /// Session providing feedback\n    pub session_id: SessionId,\n    \n    /// When feedback was provided\n    pub timestamp: DateTime\u003cUtc\u003e,\n    \n    /// Type of feedback\n    pub feedback_type: FeedbackType,\n    \n    /// Specific section if applicable\n    pub section: Option\u003cString\u003e,\n    \n    /// Specific slice if applicable\n    pub slice: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum FeedbackType {\n    /// Explicit thumbs up\n    Positive { comment: Option\u003cString\u003e },\n    \n    /// Explicit thumbs down\n    Negative { reason: NegativeReason, comment: Option\u003cString\u003e },\n    \n    /// Numeric rating (1-5)\n    Rating { value: u8, comment: Option\u003cString\u003e },\n    \n    /// Specific correction suggested\n    Correction { original: String, suggested: String },\n    \n    /// Section marked as outdated\n    Outdated { section: String },\n    \n    /// Request for more detail\n    NeedsMore { topic: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum NegativeReason {\n    NotRelevant,\n    Outdated,\n    Incorrect,\n    TooGeneric,\n    TooVerbose,\n    MissingContext,\n    Other(String),\n}\n\n/// Session outcome for implicit feedback\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SessionOutcome {\n    pub session_id: SessionId,\n    \n    /// Skills that were used in this session\n    pub skills_used: Vec\u003cSkillId\u003e,\n    \n    /// Overall outcome\n    pub outcome: Outcome,\n    \n    /// Session duration\n    pub duration: std::time::Duration,\n    \n    /// Number of errors encountered\n    pub error_count: u32,\n    \n    /// Signals indicating completion\n    pub completion_signals: Vec\u003cCompletionSignal\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Outcome {\n    /// Task completed successfully\n    Success,\n    \n    /// Task completed with issues\n    PartialSuccess,\n    \n    /// Task abandoned or failed\n    Failure,\n    \n    /// Unknown (session still active or no signal)\n    Unknown,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum CompletionSignal {\n    /// Explicit success indicator (e.g., \"Thanks, that worked!\")\n    ExplicitSuccess(String),\n    \n    /// Tests passing\n    TestsPassed { count: u32 },\n    \n    /// Build succeeded\n    BuildSucceeded,\n    \n    /// Git commit made\n    CommitMade { message: String },\n    \n    /// Explicit failure indicator\n    ExplicitFailure(String),\n    \n    /// Session ended abruptly\n    Abandoned,\n}\n```\n\n### A/B Experiments\n\n```rust\n/// Skill experiment definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillExperiment {\n    /// Unique experiment identifier\n    pub id: String,\n    \n    /// Skill being experimented on\n    pub skill_id: SkillId,\n    \n    /// Scope of the experiment\n    pub scope: ExperimentScope,\n    \n    /// Experiment variants\n    pub variants: Vec\u003cExperimentVariant\u003e,\n    \n    /// Current status\n    pub status: ExperimentStatus,\n    \n    /// When experiment was created\n    pub created_at: DateTime\u003cUtc\u003e,\n    \n    /// When experiment started\n    pub started_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// When experiment ended\n    pub ended_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// Experiment configuration\n    pub config: ExperimentConfig,\n    \n    /// Results (populated when experiment ends)\n    pub results: Option\u003cExperimentResults\u003e,\n}\n\n/// What part of the skill to experiment with\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ExperimentScope {\n    /// Experiment with the entire skill\n    EntireSkill,\n    \n    /// Experiment with a specific section\n    Section(String),\n    \n    /// Experiment with a specific slice (finest granularity)\n    Slice(String),\n}\n\n/// A variant in an experiment\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentVariant {\n    /// Variant identifier\n    pub id: String,\n    \n    /// Human-readable name\n    pub name: String,\n    \n    /// Content for this variant\n    pub content: VariantContent,\n    \n    /// Traffic allocation (0.0 - 1.0)\n    pub allocation: f64,\n    \n    /// Collected metrics\n    pub metrics: VariantMetrics,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum VariantContent {\n    /// Full skill content\n    FullSkill(Skill),\n    \n    /// Section content\n    SectionContent(String),\n    \n    /// Slice content\n    SliceContent(String),\n    \n    /// Reference to existing content (control group)\n    Control,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct VariantMetrics {\n    pub usage_count: u32,\n    pub success_count: u32,\n    pub positive_feedback: u32,\n    pub negative_feedback: u32,\n    pub avg_helpfulness: f64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ExperimentStatus {\n    /// Experiment created but not started\n    Draft,\n    \n    /// Experiment is running\n    Running,\n    \n    /// Experiment paused\n    Paused,\n    \n    /// Experiment completed\n    Completed { winner: Option\u003cString\u003e },\n    \n    /// Experiment cancelled\n    Cancelled { reason: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentConfig {\n    /// Minimum sample size before declaring winner\n    pub min_sample_size: u32,\n    \n    /// Statistical significance threshold (e.g., 0.95)\n    pub significance_threshold: f64,\n    \n    /// Maximum duration before auto-ending\n    pub max_duration_days: u32,\n    \n    /// Primary metric to optimize\n    pub primary_metric: MetricType,\n    \n    /// Whether to auto-apply winner when significant\n    pub auto_apply_winner: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum MetricType {\n    SuccessRate,\n    PositiveFeedbackRate,\n    Helpfulness,\n    UsageRetention,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentResults {\n    /// Metrics per variant\n    pub variant_results: HashMap\u003cString, VariantMetrics\u003e,\n    \n    /// Statistical analysis\n    pub analysis: StatisticalAnalysis,\n    \n    /// Winning variant (if significant)\n    pub winner: Option\u003cString\u003e,\n    \n    /// Confidence in the winner\n    pub confidence: f64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StatisticalAnalysis {\n    /// P-value for the comparison\n    pub p_value: f64,\n    \n    /// Effect size\n    pub effect_size: f64,\n    \n    /// Confidence interval for effect\n    pub confidence_interval: (f64, f64),\n    \n    /// Whether result is statistically significant\n    pub is_significant: bool,\n}\n```\n\n---\n\n## Slice-Level Experiments\n\nSlice-level experiments enable fine-grained A/B testing by targeting individual slices while keeping the rest of the skill constant. This approach offers several advantages:\n\n### Benefits\n\n1. **Faster Convergence**: Smaller units require fewer samples to reach significance\n2. **Precise Attribution**: Know exactly which content change caused the improvement\n3. **Lower Risk**: Testing a single slice doesn't risk the entire skill\n4. **Incremental Improvement**: Optimize skills one slice at a time\n\n### Implementation\n\n```rust\nimpl SkillExperiment {\n    /// Create a slice-level experiment\n    pub fn create_slice_experiment(\n        skill: \u0026Skill,\n        slice_id: \u0026str,\n        variants: Vec\u003c(String, String)\u003e, // (name, content) pairs\n    ) -\u003e Result\u003cSelf, ExperimentError\u003e {\n        // Validate slice exists\n        let slice = skill.find_slice(slice_id)\n            .ok_or(ExperimentError::SliceNotFound(slice_id.to_string()))?;\n        \n        // Create control variant from existing content\n        let mut experiment_variants = vec![ExperimentVariant {\n            id: \"control\".to_string(),\n            name: \"Control (Current)\".to_string(),\n            content: VariantContent::Control,\n            allocation: 0.5 / variants.len() as f64,\n            metrics: VariantMetrics::default(),\n        }];\n        \n        // Add test variants\n        let allocation_per_variant = 0.5 / variants.len() as f64;\n        for (name, content) in variants {\n            experiment_variants.push(ExperimentVariant {\n                id: format!(\"variant-{}\", name.to_lowercase().replace(' ', \"-\")),\n                name,\n                content: VariantContent::SliceContent(content),\n                allocation: allocation_per_variant,\n                metrics: VariantMetrics::default(),\n            });\n        }\n        \n        Ok(Self {\n            id: format!(\"exp-{}-{}\", skill.id.0, Uuid::new_v4()),\n            skill_id: skill.id.clone(),\n            scope: ExperimentScope::Slice(slice_id.to_string()),\n            variants: experiment_variants,\n            status: ExperimentStatus::Draft,\n            created_at: Utc::now(),\n            started_at: None,\n            ended_at: None,\n            config: ExperimentConfig::default(),\n            results: None,\n        })\n    }\n    \n    /// Get content for a session (with experiment assignment)\n    pub fn get_content_for_session(\n        \u0026mut self,\n        skill: \u0026Skill,\n        session_id: \u0026SessionId,\n    ) -\u003e (String, ExperimentAssignment) {\n        // Deterministic variant assignment based on session ID\n        let variant = self.assign_variant(session_id);\n        \n        let content = match \u0026variant.content {\n            VariantContent::Control =\u003e {\n                match \u0026self.scope {\n                    ExperimentScope::Slice(slice_id) =\u003e {\n                        skill.find_slice(slice_id)\n                            .map(|s| s.content.clone())\n                            .unwrap_or_default()\n                    }\n                    ExperimentScope::Section(section_name) =\u003e {\n                        skill.sections.get(section_name)\n                            .map(|s| s.content.clone())\n                            .unwrap_or_default()\n                    }\n                    ExperimentScope::EntireSkill =\u003e skill.render_full(),\n                }\n            }\n            VariantContent::SliceContent(content) =\u003e content.clone(),\n            VariantContent::SectionContent(content) =\u003e content.clone(),\n            VariantContent::FullSkill(skill) =\u003e skill.render_full(),\n        };\n        \n        let assignment = ExperimentAssignment {\n            experiment_id: self.id.clone(),\n            variant_id: variant.id.clone(),\n        };\n        \n        (content, assignment)\n    }\n    \n    /// Assign variant based on session ID (deterministic)\n    fn assign_variant(\u0026self, session_id: \u0026SessionId) -\u003e \u0026ExperimentVariant {\n        use std::hash::{Hash, Hasher};\n        use std::collections::hash_map::DefaultHasher;\n        \n        let mut hasher = DefaultHasher::new();\n        session_id.0.hash(\u0026mut hasher);\n        self.id.hash(\u0026mut hasher);\n        let hash = hasher.finish();\n        \n        // Convert to 0.0-1.0 range\n        let bucket = (hash % 10000) as f64 / 10000.0;\n        \n        // Find variant based on allocation\n        let mut cumulative = 0.0;\n        for variant in \u0026self.variants {\n            cumulative += variant.allocation;\n            if bucket \u003c cumulative {\n                return variant;\n            }\n        }\n        \n        // Fallback to last variant\n        self.variants.last().unwrap()\n    }\n}\n```\n\n---\n\n## Quality Score Updates\n\n```rust\nimpl EffectivenessTracker {\n    /// Update skill quality score based on new evidence\n    pub fn update_score(\u0026mut self, skill_id: \u0026SkillId) -\u003e Result\u003cQualityScore, EffectivenessError\u003e {\n        // Fetch all relevant data\n        let usage_count = self.db.get_usage_count(skill_id)?;\n        let feedback = self.db.get_feedback_summary(skill_id)?;\n        let outcomes = self.db.get_outcome_summary(skill_id)?;\n        \n        // Calculate component scores\n        let feedback_score = self.calculate_feedback_score(\u0026feedback);\n        let success_rate = self.calculate_success_rate(\u0026outcomes);\n        let recency_factor = self.calculate_recency_factor(skill_id)?;\n        \n        // Weighted combination\n        let weights = ScoreWeights::default();\n        let overall_score = \n            weights.feedback * feedback_score +\n            weights.success * success_rate +\n            weights.recency * recency_factor;\n        \n        // Apply Bayesian smoothing for low sample sizes\n        let smoothed_score = self.bayesian_smooth(overall_score, usage_count);\n        \n        // Update database\n        let score = QualityScore {\n            skill_id: skill_id.clone(),\n            overall: smoothed_score,\n            components: ScoreComponents {\n                feedback_score,\n                success_rate,\n                recency_factor,\n            },\n            confidence: self.calculate_confidence(usage_count),\n            sample_size: usage_count,\n            last_updated: Utc::now(),\n        };\n        \n        self.db.upsert_score(\u0026score)?;\n        \n        Ok(score)\n    }\n    \n    /// Bayesian smoothing to handle low sample sizes\n    fn bayesian_smooth(\u0026self, score: f64, sample_size: u32) -\u003e f64 {\n        // Prior: assume average skill (0.5)\n        let prior_mean = 0.5;\n        let prior_strength = 10.0; // Equivalent to 10 observations\n        \n        let smoothed = (prior_strength * prior_mean + sample_size as f64 * score) \n            / (prior_strength + sample_size as f64);\n        \n        smoothed\n    }\n    \n    /// Calculate confidence based on sample size\n    fn calculate_confidence(\u0026self, sample_size: u32) -\u003e f64 {\n        // Confidence grows with sample size, asymptotic to 1.0\n        let max_samples = 1000.0;\n        1.0 - (-(sample_size as f64) / max_samples).exp()\n    }\n    \n    fn calculate_feedback_score(\u0026self, feedback: \u0026FeedbackSummary) -\u003e f64 {\n        let total = feedback.positive + feedback.negative;\n        if total == 0 {\n            return 0.5; // No feedback, neutral score\n        }\n        \n        feedback.positive as f64 / total as f64\n    }\n    \n    fn calculate_success_rate(\u0026self, outcomes: \u0026OutcomeSummary) -\u003e f64 {\n        let total = outcomes.successes + outcomes.failures;\n        if total == 0 {\n            return 0.5;\n        }\n        \n        // Weight partial successes at 0.5\n        let effective_successes = outcomes.successes as f64 \n            + 0.5 * outcomes.partial_successes as f64;\n        \n        effective_successes / total as f64\n    }\n    \n    fn calculate_recency_factor(\u0026self, skill_id: \u0026SkillId) -\u003e Result\u003cf64, EffectivenessError\u003e {\n        let last_positive = self.db.get_last_positive_feedback(skill_id)?;\n        \n        match last_positive {\n            Some(timestamp) =\u003e {\n                let days_ago = (Utc::now() - timestamp).num_days() as f64;\n                // Decay factor: halve every 30 days\n                let decay = 0.5_f64.powf(days_ago / 30.0);\n                Ok(0.5 + 0.5 * decay) // Range: 0.5 to 1.0\n            }\n            None =\u003e Ok(0.5), // No recent positive feedback\n        }\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct QualityScore {\n    pub skill_id: SkillId,\n    pub overall: f64,\n    pub components: ScoreComponents,\n    pub confidence: f64,\n    pub sample_size: u32,\n    pub last_updated: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ScoreComponents {\n    pub feedback_score: f64,\n    pub success_rate: f64,\n    pub recency_factor: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ScoreWeights {\n    pub feedback: f64,\n    pub success: f64,\n    pub recency: f64,\n}\n\nimpl Default for ScoreWeights {\n    fn default() -\u003e Self {\n        Self {\n            feedback: 0.4,\n            success: 0.4,\n            recency: 0.2,\n        }\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms effectiveness report`\n\n```\nGenerate effectiveness report for a skill\n\nUSAGE:\n    ms effectiveness report \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --period \u003cDAYS\u003e     Analysis period in days [default: 30]\n    --format \u003cFMT\u003e      Output format: text, json, markdown [default: text]\n    --detailed          Include per-section breakdown\n    --compare \u003cSKILL\u003e   Compare with another skill\n\nOUTPUT EXAMPLE:\n    Effectiveness Report: rust-error-handling\n    ==========================================\n    \n    Overall Score: 0.78 (High) [Confidence: 0.92]\n    \n    Components:\n      Feedback Score:    0.85 (42 positive, 8 negative)\n      Success Rate:      0.72 (38 successes, 15 failures)\n      Recency Factor:    0.91 (last positive: 2 days ago)\n    \n    Usage Statistics (last 30 days):\n      Total Uses:        53\n      Unique Sessions:   41\n      Avg Tokens:        1,247\n      Top Context:       Automatic (67%)\n    \n    Section Breakdown:\n      overview           0.82  (12 uses)\n      error-types        0.79  (28 uses)\n      best-practices     0.74  (18 uses)\n      examples           0.88  (31 uses)\n    \n    Trends:\n      Week 1:  0.71 -\u003e Week 4: 0.78 (+9.8%)\n    \n    Recommendations:\n      - Section \"best-practices\" has lower score, consider updating\n      - High usage of \"examples\" - consider expanding\n```\n\n### `ms feedback add`\n\n```\nAdd feedback for a skill\n\nUSAGE:\n    ms feedback add \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --positive          Mark as helpful (thumbs up)\n    --negative          Mark as not helpful (thumbs down)\n    --rating \u003c1-5\u003e      Provide numeric rating\n    --reason \u003cREASON\u003e   Reason for negative feedback\n    --comment \u003cTEXT\u003e    Additional comment\n    --section \u003cNAME\u003e    Feedback for specific section\n    --slice \u003cID\u003e        Feedback for specific slice\n    --outdated          Mark section as outdated\n    --needs-more \u003cTOPIC\u003e  Request more detail on topic\n\nEXAMPLES:\n    ms feedback add rust-error-handling --positive\n    ms feedback add rust-error-handling --negative --reason outdated\n    ms feedback add rust-error-handling --rating 4 --comment \"Good examples\"\n    ms feedback add rust-error-handling --section overview --outdated\n    ms feedback add rust-error-handling --needs-more \"async error handling\"\n\nNEGATIVE REASONS:\n    not-relevant, outdated, incorrect, too-generic, too-verbose, missing-context\n```\n\n### `ms experiment create`\n\n```\nCreate a skill experiment\n\nUSAGE:\n    ms experiment create \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --scope \u003cSCOPE\u003e         Experiment scope: entire, section:\u003cname\u003e, slice:\u003cid\u003e\n    --variant \u003cNAME:FILE\u003e   Add variant from file (can specify multiple)\n    --variant-inline \u003cNAME:CONTENT\u003e  Add variant with inline content\n    --min-samples \u003cN\u003e       Minimum samples before declaring winner [default: 100]\n    --significance \u003cF\u003e      Statistical significance threshold [default: 0.95]\n    --max-days \u003cN\u003e          Maximum experiment duration [default: 30]\n    --auto-apply            Auto-apply winner when significant\n    --start                 Start experiment immediately\n\nEXAMPLES:\n    # Experiment with entire skill\n    ms experiment create rust-error-handling \\\n        --variant \"concise:variants/concise.md\" \\\n        --variant \"verbose:variants/verbose.md\"\n    \n    # Slice-level experiment\n    ms experiment create rust-error-handling \\\n        --scope slice:error-types/result-usage \\\n        --variant-inline \"shorter:Use Result\u003cT, E\u003e for recoverable errors.\" \\\n        --min-samples 50 \\\n        --auto-apply\n    \n    # Start immediately\n    ms experiment create rust-error-handling \\\n        --scope section:examples \\\n        --variant \"new-examples:new_examples.md\" \\\n        --start\n\nOTHER SUBCOMMANDS:\n    ms experiment list              List all experiments\n    ms experiment status \u003cID\u003e       Show experiment status\n    ms experiment start \u003cID\u003e        Start a draft experiment\n    ms experiment stop \u003cID\u003e         Stop a running experiment\n    ms experiment results \u003cID\u003e      Show experiment results\n    ms experiment apply \u003cID\u003e        Apply winning variant\n```\n\n---\n\n## Event Collection\n\n```rust\nimpl EffectivenessTracker {\n    /// Record a skill usage event\n    pub fn record_usage(\u0026mut self, event: UsageEvent) -\u003e Result\u003c(), EffectivenessError\u003e {\n        // Add to cache\n        self.event_cache.usage_events.push(event.clone());\n        \n        // Update experiment metrics if applicable\n        if let Some(exp_info) = \u0026event.experiment_info {\n            self.update_experiment_usage(\u0026exp_info.experiment_id, \u0026exp_info.variant_id)?;\n        }\n        \n        // Flush if needed\n        if self.event_cache.should_flush() {\n            self.flush_cache()?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Record explicit feedback\n    pub fn record_feedback(\u0026mut self, event: FeedbackEvent) -\u003e Result\u003c(), EffectivenessError\u003e {\n        self.event_cache.feedback_events.push(event.clone());\n        \n        // Update score immediately for feedback (more signal)\n        self.update_score(\u0026event.skill_id)?;\n        \n        if self.event_cache.should_flush() {\n            self.flush_cache()?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Record session outcome (implicit feedback)\n    pub fn record_outcome(\u0026mut self, outcome: SessionOutcome) -\u003e Result\u003c(), EffectivenessError\u003e {\n        self.db.insert_outcome(\u0026outcome)?;\n        \n        // Update scores for all skills used in session\n        for skill_id in \u0026outcome.skills_used {\n            self.update_score(skill_id)?;\n        }\n        \n        // Update experiment metrics for skills in experiments\n        for skill_id in \u0026outcome.skills_used {\n            if let Some(experiment) = self.find_active_experiment(skill_id) {\n                let success = matches!(outcome.outcome, Outcome::Success | Outcome::PartialSuccess);\n                self.record_experiment_outcome(\u0026experiment.id, success)?;\n            }\n        }\n        \n        Ok(())\n    }\n    \n    /// Flush event cache to database\n    fn flush_cache(\u0026mut self) -\u003e Result\u003c(), EffectivenessError\u003e {\n        // Batch insert usage events\n        self.db.batch_insert_usage(\u0026self.event_cache.usage_events)?;\n        self.event_cache.usage_events.clear();\n        \n        // Batch insert feedback events\n        self.db.batch_insert_feedback(\u0026self.event_cache.feedback_events)?;\n        self.event_cache.feedback_events.clear();\n        \n        self.event_cache.last_flush = std::time::Instant::now();\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## Implicit Feedback Detection\n\n```rust\n/// Detect session outcomes from CASS session data\npub struct OutcomeDetector {\n    cass: CassClient,\n}\n\nimpl OutcomeDetector {\n    /// Analyze a completed session for implicit feedback signals\n    pub fn analyze_session(\u0026self, session_id: \u0026SessionId) -\u003e Result\u003cSessionOutcome, DetectorError\u003e {\n        let session = self.cass.get_session(session_id)?;\n        \n        let mut signals = Vec::new();\n        let mut error_count = 0;\n        \n        // Analyze conversation for signals\n        for message in \u0026session.messages {\n            // Check for explicit success signals\n            if self.is_success_message(\u0026message.content) {\n                signals.push(CompletionSignal::ExplicitSuccess(\n                    self.extract_signal_text(\u0026message.content)\n                ));\n            }\n            \n            // Check for explicit failure signals\n            if self.is_failure_message(\u0026message.content) {\n                signals.push(CompletionSignal::ExplicitFailure(\n                    self.extract_signal_text(\u0026message.content)\n                ));\n            }\n            \n            // Count errors in assistant responses\n            if message.role == Role::Assistant \u0026\u0026 self.contains_error(\u0026message.content) {\n                error_count += 1;\n            }\n        }\n        \n        // Check for tool use signals\n        if let Some(tool_results) = \u0026session.tool_results {\n            for result in tool_results {\n                match result {\n                    ToolResult::TestsPassed { count } =\u003e {\n                        signals.push(CompletionSignal::TestsPassed { count: *count });\n                    }\n                    ToolResult::BuildSucceeded =\u003e {\n                        signals.push(CompletionSignal::BuildSucceeded);\n                    }\n                    ToolResult::GitCommit { message } =\u003e {\n                        signals.push(CompletionSignal::CommitMade { \n                            message: message.clone() \n                        });\n                    }\n                    _ =\u003e {}\n                }\n            }\n        }\n        \n        // Determine overall outcome\n        let outcome = self.determine_outcome(\u0026signals, error_count);\n        \n        Ok(SessionOutcome {\n            session_id: session_id.clone(),\n            skills_used: session.skills_used.clone(),\n            outcome,\n            duration: session.duration(),\n            error_count,\n            completion_signals: signals,\n        })\n    }\n    \n    fn is_success_message(\u0026self, content: \u0026str) -\u003e bool {\n        let success_patterns = [\n            \"thanks\", \"thank you\", \"that worked\", \"perfect\", \"great\",\n            \"exactly what i needed\", \"solved\", \"fixed\", \"working now\",\n        ];\n        \n        let lower = content.to_lowercase();\n        success_patterns.iter().any(|p| lower.contains(p))\n    }\n    \n    fn is_failure_message(\u0026self, content: \u0026str) -\u003e bool {\n        let failure_patterns = [\n            \"doesn't work\", \"didn't work\", \"still broken\", \"not working\",\n            \"wrong\", \"incorrect\", \"that's not right\", \"failed\",\n        ];\n        \n        let lower = content.to_lowercase();\n        failure_patterns.iter().any(|p| lower.contains(p))\n    }\n    \n    fn determine_outcome(\u0026self, signals: \u0026[CompletionSignal], error_count: u32) -\u003e Outcome {\n        let has_success = signals.iter().any(|s| matches!(s, \n            CompletionSignal::ExplicitSuccess(_) |\n            CompletionSignal::TestsPassed { .. } |\n            CompletionSignal::BuildSucceeded |\n            CompletionSignal::CommitMade { .. }\n        ));\n        \n        let has_failure = signals.iter().any(|s| matches!(s,\n            CompletionSignal::ExplicitFailure(_) |\n            CompletionSignal::Abandoned\n        ));\n        \n        match (has_success, has_failure, error_count) {\n            (true, false, _) =\u003e Outcome::Success,\n            (true, true, _) =\u003e Outcome::PartialSuccess,\n            (false, true, _) =\u003e Outcome::Failure,\n            (false, false, e) if e \u003e 3 =\u003e Outcome::Failure,\n            _ =\u003e Outcome::Unknown,\n        }\n    }\n}\n```\n\n---\n\n## Dependencies\n\n- **SQLite Database Layer** (meta_skill-qs1): Persistent storage for effectiveness data\n- `rusqlite`: Database operations\n- `chrono`: Timestamps\n- `serde`, `serde_json`: Serialization\n- `uuid`: Experiment IDs\n- Statistical library for A/B testing (e.g., `statrs`)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:56:01.703772637-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:00.673499629-05:00","labels":["effectiveness","feedback","phase-6","tracking"],"dependencies":[{"issue_id":"meta_skill-iim","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:04:14.019616413-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-iim","depends_on_id":"meta_skill-7va","type":"blocks","created_at":"2026-01-13T23:43:47.424922484-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-iim","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:43:57.958376496-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ik6","title":"[P1] SkillSpec Data Model","description":"## SkillSpec Data Model (Complete)\n\nSkillSpec is the deterministic source-of-truth for skill content. The Skill struct is the runtime representation; SkillSpec is the canonical, serializable format.\n\n### Core Structures\n\n```rust\n/// A complete skill with all metadata and content\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Skill {\n    /// Unique identifier (derived from path or explicit id)\n    pub id: String,\n    /// YAML frontmatter metadata\n    pub metadata: SkillMetadata,\n    /// Main SKILL.md body content\n    pub body: String,\n    /// Associated files\n    pub assets: SkillAssets,\n    /// Source information\n    pub source: SkillSource,\n    /// Computed fields\n    pub computed: SkillComputed,\n    /// Rule-level evidence and provenance\n    pub evidence: SkillEvidenceIndex,\n}\n\n/// Deterministic source-of-truth for skill content\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSpec {\n    /// Spec format version (for migrations)\n    pub format_version: String,\n    /// Stable skill id\n    pub id: String,\n    /// Frontmatter metadata\n    pub metadata: SkillMetadata,\n    /// Structured sections and blocks\n    pub sections: Vec\u003cSkillSectionSpec\u003e,\n    /// Associated files\n    pub assets: SkillAssets,\n    /// Evidence index (rule provenance)\n    pub evidence: SkillEvidenceIndex,\n    /// When spec was generated or updated\n    pub generated_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSectionSpec {\n    pub title: String,\n    pub level: u8,\n    pub blocks: Vec\u003cSkillBlockSpec\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SkillBlockSpec {\n    Rule { id: String, text: String },\n    Command { command: String, description: Option\u003cString\u003e },\n    Example { language: String, code: String, description: Option\u003cString\u003e },\n    Checklist { items: Vec\u003cString\u003e },\n    Table { headers: Vec\u003cString\u003e, rows: Vec\u003cVec\u003cString\u003e\u003e },\n    Prompt { prompt: String },\n    Pitfall { bad: String, risk: String, fix: String },\n    Note { text: String },\n}\n```\n\n### SpecLens (Markdown-to-Spec Mapping)\n\n```rust\n/// Mapping from compiled markdown back to spec blocks\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SpecLens {\n    pub format_version: String,\n    pub blocks: Vec\u003cBlockLens\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BlockLens {\n    pub block_id: String,\n    pub section: String,\n    pub block_type: String,\n    pub byte_range: (u32, u32),\n}\n```\n\n### SkillMetadata\n\n```rust\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillMetadata {\n    pub name: String,\n    pub description: String,\n    #[serde(default)]\n    pub version: Option\u003cString\u003e,\n    #[serde(default)]\n    pub author: Option\u003cString\u003e,\n    #[serde(default)]\n    pub tags: Vec\u003cString\u003e,\n    #[serde(default)]\n    pub aliases: Vec\u003cString\u003e,        // Alternate names / legacy ids\n    #[serde(default)]\n    pub requires: Vec\u003cString\u003e,       // Dependencies on other skills\n    #[serde(default)]\n    pub provides: Vec\u003cString\u003e,       // Capabilities exposed by this skill\n    #[serde(default)]\n    pub triggers: Vec\u003cSkillTrigger\u003e, // When to suggest this skill\n    #[serde(default)]\n    pub priority: SkillPriority,\n    #[serde(default)]\n    pub deprecated: Option\u003cDeprecationInfo\u003e,\n    #[serde(default)]\n    pub toolchains: Vec\u003cToolchainConstraint\u003e,  // Compatibility constraints\n    #[serde(default)]\n    pub requirements: SkillRequirements,       // Tooling/OS/environment requirements\n    #[serde(default)]\n    pub fixes: Vec\u003cString\u003e,          // Error codes this skill addresses\n    #[serde(default)]\n    pub policies: Vec\u003cSkillPolicy\u003e,  // Machine-readable policy constraints\n}\n```\n\n### Triggers and Requirements\n\n```rust\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillTrigger {\n    /// Trigger type: \"command\", \"file_pattern\", \"keyword\", \"context\"\n    pub trigger_type: String,\n    /// Pattern to match\n    pub pattern: String,\n    /// Priority boost when triggered (0.0 - 1.0)\n    #[serde(default = \"default_boost\")]\n    pub boost: f32,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct SkillRequirements {\n    /// Supported platforms (empty = any)\n    pub platforms: Vec\u003cPlatform\u003e,\n    /// Required external tools (git, docker, gh, etc.)\n    pub tools: Vec\u003cToolRequirement\u003e,\n    /// Required environment variables (presence only)\n    pub env: Vec\u003cString\u003e,\n    /// Network requirement (offline/online)\n    pub network: NetworkRequirement,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ToolRequirement {\n    pub name: String,\n    pub min_version: Option\u003cString\u003e,\n    pub max_version: Option\u003cString\u003e,\n    #[serde(default = \"default_required\")]\n    pub required: bool,\n    pub notes: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]\npub enum Platform { Any, Linux, Macos, Windows }\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum NetworkRequirement { OfflineOk, Required, PreferOffline }\n```\n\n### SkillAssets and Source\n\n```rust\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillAssets {\n    pub scripts: Vec\u003cScriptFile\u003e,      // scripts/ directory\n    pub references: Vec\u003cReferenceFile\u003e, // references/ directory\n    pub tests: Vec\u003cTestFile\u003e,          // tests/ directory\n    pub assets: Vec\u003cAssetFile\u003e,        // Other assets\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSource {\n    pub path: PathBuf,\n    pub layer: SkillLayer,             // base, org, project, user\n    pub git_remote: Option\u003cString\u003e,\n    pub git_commit: Option\u003cString\u003e,\n    pub modified_at: DateTime\u003cUtc\u003e,\n    pub content_hash: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SkillLayer { Base, Org, Project, User }\n```\n\n### SkillSlice (Token Packing)\n\n```rust\n/// A sliceable unit of a skill for token-aware packing\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSlice {\n    pub id: String,                    // Stable slice id (rule-1, example-2)\n    pub slice_type: SliceType,\n    pub token_estimate: usize,\n    pub utility_score: f32,            // 0.0 - 1.0\n    pub coverage_group: Option\u003cString\u003e,\n    pub tags: Vec\u003cString\u003e,\n    pub requires: Vec\u003cString\u003e,         // Dependencies on other slices\n    pub condition: Option\u003cSlicePredicate\u003e,\n    pub content: String,               // Markdown payload\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SliceType {\n    Rule, Command, Example, Checklist, Pitfall, Overview, Reference,\n    Policy,   // Non-removable safety/policy invariants\n}\n\n/// Predicate for conditional slice inclusion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SlicePredicate {\n    pub expr: String,                  // \"package:next \u003e= 16.0.0\"\n    pub predicate_type: PredicateType,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum PredicateType {\n    PackageVersion { package: String, op: VersionOp, version: String },\n    EnvVar { var: String },\n    FileExists { pattern: String },\n    RustEdition { op: VersionOp, edition: String },\n    ToolVersion { tool: String, op: VersionOp, version: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum VersionOp { Eq, Ne, Lt, Le, Gt, Ge }\n```\n\n### Evidence and Provenance\n\n```rust\n/// Rule-level evidence index for provenance and auditing\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillEvidenceIndex {\n    pub rules: HashMap\u003cString, Vec\u003cEvidenceRef\u003e\u003e,\n    pub coverage: EvidenceCoverage,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct EvidenceRef {\n    pub session_id: String,\n    pub message_range: (u32, u32),\n    pub snippet_hash: String,\n    pub excerpt: Option\u003cString\u003e,\n    pub excerpt_path: Option\u003cPathBuf\u003e,\n    pub level: EvidenceLevel,\n    pub confidence: f32,\n    pub captured_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum EvidenceLevel {\n    Pointer,   // hash + message range only\n    Excerpt,   // minimal redacted excerpt\n    Expanded,  // full context available via CASS\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct EvidenceCoverage {\n    pub total_rules: usize,\n    pub rules_with_evidence: usize,\n    pub avg_confidence: f32,\n}\n```\n\n### Uncertainty Queue\n\n```rust\n/// Queue item for low-confidence generalizations\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UncertaintyItem {\n    pub id: String,\n    pub pattern_candidate: ExtractedPattern,\n    pub reason: String,\n    pub confidence: f32,\n    pub suggested_queries: Vec\u003cString\u003e,\n    pub auto_mine_attempts: u32,\n    pub last_mined_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub status: UncertaintyStatus,\n    pub created_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum UncertaintyStatus { Pending, Resolved, Discarded }\n```\n\n### SkillPack (Runtime Cache)\n\n```rust\n/// Precompiled runtime cache for low-latency load/suggest\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillPack {\n    pub skill_id: String,\n    pub pack_path: PathBuf,\n    pub spec_hash: String,\n    pub slices_hash: String,\n    pub embedding_hash: String,\n    pub predicate_index_hash: String,\n    pub generated_at: DateTime\u003cUtc\u003e,\n}\n```\n\n### PackContract (Minimum Guarantees)\n\n```rust\n/// Pack contracts define minimal guidance guarantees for specific tasks\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackContract {\n    pub id: String,                   // e.g., \"DebugContract\"\n    pub description: String,\n    pub required_groups: Vec\u003cString\u003e, // e.g., [\"critical-rules\", \"validation\"]\n    pub mandatory_slices: Vec\u003cString\u003e,\n    pub max_per_group: Option\u003cusize\u003e,\n}\n```","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:04.62909506-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:07:17.01804491-05:00","labels":["datamodel","phase-1","skill"],"dependencies":[{"issue_id":"meta_skill-ik6","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.849118748-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-jka","title":"Dependency Graph Resolution","description":"## Overview\n\nImplement dependency graph resolution for skills. Skills can declare dependencies (`requires`), capabilities (`provides`), and environment requirements. ms builds a dependency graph to resolve load order, detect cycles, and auto-load prerequisites with appropriate disclosure levels.\n\n## Background \u0026 Rationale\n\n### Why Dependency Resolution\n\n1. **Composability**: Skills can build on other skills\n2. **Auto-Loading**: Dependencies loaded automatically when skill requested\n3. **Cycle Detection**: Prevents infinite loops from circular dependencies\n4. **Environment Checks**: Validates platform/tool requirements before loading\n5. **Progressive Disclosure**: Dependencies can load at lower disclosure levels\n\n### Resolution Strategy\n\n- **BFS Expansion**: Breadth-first traversal with depth limit\n- **Tarjan/DFS**: Cycle detection via back-edge analysis\n- **Topological Sort**: Determines safe load order\n- **Disclosure Cascading**: Root at requested level, deps at overview/minimal\n\n---\n\n## Key Data Structures (from Plan Section 3.4)\n\n### Dependency Graph\n\n```rust\nuse std::collections::{HashSet, HashMap};\n\n#[derive(Debug, Clone)]\npub struct DependencyGraph {\n    /// All skill IDs in the graph\n    pub nodes: HashSet\u003cString\u003e,\n    /// Directed edges: skill -\u003e depends_on\n    pub edges: Vec\u003cDependencyEdge\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct DependencyEdge {\n    /// Skill that has the dependency\n    pub skill_id: String,\n    /// Skill that is depended upon\n    pub depends_on: String,\n}\n\nimpl DependencyGraph {\n    /// Build graph from skill registry\n    pub fn from_registry(registry: \u0026SkillRegistry) -\u003e Result\u003cSelf\u003e {\n        let mut nodes = HashSet::new();\n        let mut edges = Vec::new();\n        \n        for skill in registry.all_skills() {\n            nodes.insert(skill.id.clone());\n            \n            for dep in \u0026skill.metadata.requires {\n                edges.push(DependencyEdge {\n                    skill_id: skill.id.clone(),\n                    depends_on: dep.clone(),\n                });\n            }\n        }\n        \n        Ok(Self { nodes, edges })\n    }\n    \n    /// Get direct dependencies of a skill\n    pub fn direct_deps(\u0026self, skill_id: \u0026str) -\u003e Vec\u003c\u0026str\u003e {\n        self.edges\n            .iter()\n            .filter(|e| e.skill_id == skill_id)\n            .map(|e| e.depends_on.as_str())\n            .collect()\n    }\n    \n    /// Get skills that depend on this skill\n    pub fn dependents(\u0026self, skill_id: \u0026str) -\u003e Vec\u003c\u0026str\u003e {\n        self.edges\n            .iter()\n            .filter(|e| e.depends_on == skill_id)\n            .map(|e| e.skill_id.as_str())\n            .collect()\n    }\n}\n```\n\n### Resolution Plan\n\n```rust\n#[derive(Debug, Clone)]\npub struct ResolvedDependencyPlan {\n    /// Topologically sorted load order\n    pub ordered: Vec\u003cSkillLoadPlan\u003e,\n    /// Dependencies that are missing from registry\n    pub missing: Vec\u003cString\u003e,\n    /// Cycle groups (each Vec is a cycle)\n    pub cycles: Vec\u003cVec\u003cString\u003e\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct SkillLoadPlan {\n    /// Skill to load\n    pub skill_id: String,\n    /// How to disclose this skill\n    pub disclosure: DisclosurePlan,\n    /// Why this skill is in the plan\n    pub reason: LoadReason,\n}\n\n#[derive(Debug, Clone)]\npub enum LoadReason {\n    /// Explicitly requested by user\n    Requested,\n    /// Dependency of another skill\n    DependencyOf(String),\n    /// Provides a required capability\n    ProvidesCapability(String),\n}\n\n#[derive(Debug, Clone)]\npub struct DisclosurePlan {\n    /// Disclosure level for this skill\n    pub level: DisclosureLevel,\n    /// Token budget allocated\n    pub token_budget: Option\u003cusize\u003e,\n}\n```\n\n### Dependency Load Modes\n\n```rust\n#[derive(Debug, Clone, Copy, Default)]\npub enum DependencyLoadMode {\n    /// Don't load dependencies\n    Off,\n    /// Load dependencies at overview level\n    #[default]\n    Auto,\n    /// Load dependencies at full disclosure\n    Full,\n    /// Load dependencies at minimal level\n    Overview,\n}\n```\n\n### Dependency Resolver\n\n```rust\npub struct DependencyResolver {\n    /// Skill registry for lookups\n    registry: SkillRegistry,\n    /// Maximum dependency depth to traverse\n    max_depth: usize,\n}\n\nimpl DependencyResolver {\n    pub fn new(registry: SkillRegistry, max_depth: usize) -\u003e Self {\n        Self { registry, max_depth }\n    }\n    \n    /// Resolve all dependencies for a skill\n    pub fn resolve(\u0026self, root: \u0026str, mode: DependencyLoadMode) -\u003e Result\u003cResolvedDependencyPlan\u003e {\n        match mode {\n            DependencyLoadMode::Off =\u003e {\n                // Just the root skill\n                Ok(ResolvedDependencyPlan {\n                    ordered: vec\\![SkillLoadPlan {\n                        skill_id: root.to_string(),\n                        disclosure: DisclosurePlan { \n                            level: DisclosureLevel::Full, \n                            token_budget: None \n                        },\n                        reason: LoadReason::Requested,\n                    }],\n                    missing: vec\\![],\n                    cycles: vec\\![],\n                })\n            }\n            _ =\u003e self.resolve_with_deps(root, mode),\n        }\n    }\n    \n    fn resolve_with_deps(\u0026self, root: \u0026str, mode: DependencyLoadMode) -\u003e Result\u003cResolvedDependencyPlan\u003e {\n        // 1) Expand dependency closure (BFS with depth limit)\n        let (closure, missing) = self.expand_closure(root)?;\n        \n        // 2) Build graph for cycle detection\n        let graph = self.build_subgraph(\u0026closure)?;\n        \n        // 3) Detect cycles (Tarjan's algorithm)\n        let cycles = self.find_cycles(\u0026graph);\n        \n        // 4) Topologically sort (Kahn's algorithm)\n        let sorted = self.topological_sort(\u0026graph, \u0026cycles)?;\n        \n        // 5) Assign disclosure levels\n        let ordered = self.assign_disclosure(sorted, root, mode);\n        \n        Ok(ResolvedDependencyPlan {\n            ordered,\n            missing,\n            cycles,\n        })\n    }\n    \n    /// BFS expansion with depth limit\n    fn expand_closure(\u0026self, root: \u0026str) -\u003e Result\u003c(HashSet\u003cString\u003e, Vec\u003cString\u003e)\u003e {\n        let mut visited = HashSet::new();\n        let mut missing = Vec::new();\n        let mut queue = std::collections::VecDeque::new();\n        \n        queue.push_back((root.to_string(), 0usize));\n        \n        while let Some((skill_id, depth)) = queue.pop_front() {\n            if depth \u003e self.max_depth {\n                continue;\n            }\n            \n            if visited.contains(\u0026skill_id) {\n                continue;\n            }\n            visited.insert(skill_id.clone());\n            \n            match self.registry.get(\u0026skill_id) {\n                Ok(skill) =\u003e {\n                    for dep in \u0026skill.metadata.requires {\n                        if \\!visited.contains(dep) {\n                            queue.push_back((dep.clone(), depth + 1));\n                        }\n                    }\n                }\n                Err(_) =\u003e {\n                    missing.push(skill_id);\n                }\n            }\n        }\n        \n        Ok((visited, missing))\n    }\n    \n    /// Tarjan's SCC algorithm for cycle detection\n    fn find_cycles(\u0026self, graph: \u0026DependencyGraph) -\u003e Vec\u003cVec\u003cString\u003e\u003e {\n        let mut index = 0;\n        let mut stack = Vec::new();\n        let mut on_stack = HashSet::new();\n        let mut indices = HashMap::new();\n        let mut lowlinks = HashMap::new();\n        let mut sccs = Vec::new();\n        \n        fn strongconnect(\n            v: \u0026str,\n            graph: \u0026DependencyGraph,\n            index: \u0026mut usize,\n            stack: \u0026mut Vec\u003cString\u003e,\n            on_stack: \u0026mut HashSet\u003cString\u003e,\n            indices: \u0026mut HashMap\u003cString, usize\u003e,\n            lowlinks: \u0026mut HashMap\u003cString, usize\u003e,\n            sccs: \u0026mut Vec\u003cVec\u003cString\u003e\u003e,\n        ) {\n            indices.insert(v.to_string(), *index);\n            lowlinks.insert(v.to_string(), *index);\n            *index += 1;\n            stack.push(v.to_string());\n            on_stack.insert(v.to_string());\n            \n            for w in graph.direct_deps(v) {\n                if \\!indices.contains_key(w) {\n                    strongconnect(w, graph, index, stack, on_stack, indices, lowlinks, sccs);\n                    let v_low = lowlinks[v];\n                    let w_low = lowlinks[w];\n                    lowlinks.insert(v.to_string(), v_low.min(w_low));\n                } else if on_stack.contains(w) {\n                    let v_low = lowlinks[v];\n                    let w_idx = indices[w];\n                    lowlinks.insert(v.to_string(), v_low.min(w_idx));\n                }\n            }\n            \n            if lowlinks[v] == indices[v] {\n                let mut scc = Vec::new();\n                loop {\n                    let w = stack.pop().unwrap();\n                    on_stack.remove(\u0026w);\n                    scc.push(w.clone());\n                    if w == v {\n                        break;\n                    }\n                }\n                if scc.len() \u003e 1 {\n                    // Only report actual cycles (SCC with \u003e1 node)\n                    sccs.push(scc);\n                }\n            }\n        }\n        \n        for node in \u0026graph.nodes {\n            if \\!indices.contains_key(node) {\n                strongconnect(node, graph, \u0026mut index, \u0026mut stack, \u0026mut on_stack, \n                             \u0026mut indices, \u0026mut lowlinks, \u0026mut sccs);\n            }\n        }\n        \n        sccs\n    }\n    \n    /// Kahn's algorithm for topological sort\n    fn topological_sort(\u0026self, graph: \u0026DependencyGraph, cycles: \u0026[Vec\u003cString\u003e]) -\u003e Result\u003cVec\u003cString\u003e\u003e {\n        // Skip nodes in cycles\n        let cycle_nodes: HashSet\u003c_\u003e = cycles.iter().flatten().collect();\n        \n        let mut in_degree: HashMap\u003c\u0026str, usize\u003e = HashMap::new();\n        for node in \u0026graph.nodes {\n            if \\!cycle_nodes.contains(node) {\n                in_degree.insert(node, 0);\n            }\n        }\n        \n        for edge in \u0026graph.edges {\n            if \\!cycle_nodes.contains(\u0026edge.skill_id) \u0026\u0026 \\!cycle_nodes.contains(\u0026edge.depends_on) {\n                *in_degree.entry(\u0026edge.skill_id).or_default() += 1;\n            }\n        }\n        \n        let mut queue: Vec\u003c_\u003e = in_degree.iter()\n            .filter(|(_, \u0026deg)| deg == 0)\n            .map(|(\u0026node, _)| node.to_string())\n            .collect();\n        \n        let mut sorted = Vec::new();\n        \n        while let Some(node) = queue.pop() {\n            sorted.push(node.clone());\n            \n            for edge in \u0026graph.edges {\n                if edge.depends_on == node \u0026\u0026 \\!cycle_nodes.contains(\u0026edge.skill_id) {\n                    let deg = in_degree.get_mut(edge.skill_id.as_str()).unwrap();\n                    *deg -= 1;\n                    if *deg == 0 {\n                        queue.push(edge.skill_id.clone());\n                    }\n                }\n            }\n        }\n        \n        // Reverse for correct load order (dependencies first)\n        sorted.reverse();\n        Ok(sorted)\n    }\n    \n    /// Assign disclosure levels based on mode\n    fn assign_disclosure(\n        \u0026self,\n        sorted: Vec\u003cString\u003e,\n        root: \u0026str,\n        mode: DependencyLoadMode,\n    ) -\u003e Vec\u003cSkillLoadPlan\u003e {\n        sorted.into_iter().map(|skill_id| {\n            let is_root = skill_id == root;\n            let level = match (is_root, mode) {\n                (true, _) =\u003e DisclosureLevel::Full,\n                (false, DependencyLoadMode::Full) =\u003e DisclosureLevel::Full,\n                (false, DependencyLoadMode::Overview) =\u003e DisclosureLevel::Overview,\n                (false, DependencyLoadMode::Auto) =\u003e DisclosureLevel::Overview,\n                (false, DependencyLoadMode::Off) =\u003e unreachable\\!(),\n            };\n            \n            SkillLoadPlan {\n                skill_id: skill_id.clone(),\n                disclosure: DisclosurePlan { level, token_budget: None },\n                reason: if is_root {\n                    LoadReason::Requested\n                } else {\n                    LoadReason::DependencyOf(root.to_string())\n                },\n            }\n        }).collect()\n    }\n}\n```\n\n### Alias Resolution (Section 3.4.1)\n\n```rust\npub struct AliasResolver {\n    db: Connection,\n}\n\n#[derive(Debug, Clone)]\npub struct AliasResolution {\n    /// The canonical skill ID\n    pub canonical_id: String,\n    /// Type of alias\n    pub alias_type: AliasType,\n    /// Replacement skill if deprecated\n    pub replaced_by: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum AliasType {\n    Rename,\n    Deprecated,\n    Abbreviation,\n}\n\nimpl AliasResolver {\n    pub fn new(db: Connection) -\u003e Self {\n        Self { db }\n    }\n    \n    /// Resolve an ID or alias to canonical form\n    pub fn resolve(\u0026self, id_or_alias: \u0026str) -\u003e Result\u003cOption\u003cAliasResolution\u003e\u003e {\n        // 1) Check if it's already a canonical skill ID\n        let exists: bool = self.db.query_row(\n            \"SELECT EXISTS(SELECT 1 FROM skills WHERE id = ?)\",\n            [id_or_alias],\n            |row| row.get(0),\n        )?;\n        \n        if exists {\n            return Ok(Some(AliasResolution {\n                canonical_id: id_or_alias.to_string(),\n                alias_type: AliasType::Rename, // Not actually an alias\n                replaced_by: None,\n            }));\n        }\n        \n        // 2) Check alias table\n        let result = self.db.query_row(\n            \"SELECT canonical_id, alias_type, replaced_by FROM skill_aliases WHERE alias = ?\",\n            [id_or_alias],\n            |row| {\n                Ok(AliasResolution {\n                    canonical_id: row.get(0)?,\n                    alias_type: match row.get::\u003c_, String\u003e(1)?.as_str() {\n                        \"rename\" =\u003e AliasType::Rename,\n                        \"deprecated\" =\u003e AliasType::Deprecated,\n                        \"abbreviation\" =\u003e AliasType::Abbreviation,\n                        _ =\u003e AliasType::Rename,\n                    },\n                    replaced_by: row.get(2)?,\n                })\n            },\n        );\n        \n        match result {\n            Ok(resolution) =\u003e Ok(Some(resolution)),\n            Err(rusqlite::Error::QueryReturnedNoRows) =\u003e Ok(None),\n            Err(e) =\u003e Err(e.into()),\n        }\n    }\n}\n```\n\n---\n\n## Tasks\n\n### Task 1: DependencyGraph Implementation\n- [ ] Create src/core/dependency.rs module\n- [ ] Implement DependencyGraph struct\n- [ ] Implement from_registry() builder\n- [ ] Implement direct_deps() lookup\n- [ ] Implement dependents() reverse lookup\n\n### Task 2: Resolution Plan Types\n- [ ] Define ResolvedDependencyPlan struct\n- [ ] Define SkillLoadPlan struct\n- [ ] Define LoadReason enum\n- [ ] Define DisclosurePlan struct\n- [ ] Define DependencyLoadMode enum\n\n### Task 3: BFS Closure Expansion\n- [ ] Implement expand_closure()\n- [ ] Track visited nodes\n- [ ] Respect max_depth limit\n- [ ] Collect missing dependencies\n- [ ] Handle circular references gracefully\n\n### Task 4: Cycle Detection (Tarjan)\n- [ ] Implement Tarjan's SCC algorithm\n- [ ] Track indices and lowlinks\n- [ ] Maintain on-stack set\n- [ ] Return cycles with \u003e1 node\n- [ ] Handle multiple SCCs\n\n### Task 5: Topological Sort (Kahn)\n- [ ] Implement Kahn's algorithm\n- [ ] Compute in-degrees\n- [ ] Process nodes with zero in-degree\n- [ ] Skip cycle nodes\n- [ ] Reverse for correct order\n\n### Task 6: Disclosure Assignment\n- [ ] Root skill at requested level\n- [ ] Dependencies based on mode\n- [ ] Auto mode: deps at overview\n- [ ] Full mode: deps at full\n- [ ] Track load reasons\n\n### Task 7: Alias Resolution\n- [ ] Implement AliasResolver struct\n- [ ] Check canonical IDs first\n- [ ] Fall back to alias table\n- [ ] Return deprecation info\n- [ ] Emit warnings for deprecated aliases\n\n### Task 8: Integration\n- [ ] Wire into ms load command\n- [ ] Support --deps flag\n- [ ] Show resolution plan in robot mode\n- [ ] Handle missing dependencies gracefully\n\n---\n\n## Acceptance Criteria\n\n1. **Graph Builds**: Can construct graph from skill registry\n2. **Closure Expands**: BFS finds all transitive dependencies\n3. **Cycles Detected**: Tarjan's algorithm finds all cycles\n4. **Sort Works**: Topological sort produces valid order\n5. **Disclosure Cascades**: Dependencies get lower disclosure levels\n6. **Missing Handled**: Missing deps reported, not fatal\n7. **Aliases Resolve**: Legacy IDs map to canonical\n\n---\n\n## Testing Requirements\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_simple_dependency() {\n        let mut registry = MockRegistry::new();\n        registry.add_skill(\"A\", vec\\![\"B\"]);\n        registry.add_skill(\"B\", vec\\![]);\n        \n        let resolver = DependencyResolver::new(registry, 10);\n        let plan = resolver.resolve(\"A\", DependencyLoadMode::Auto).unwrap();\n        \n        assert_eq\\!(plan.ordered.len(), 2);\n        assert_eq\\!(plan.ordered[0].skill_id, \"B\"); // Dep first\n        assert_eq\\!(plan.ordered[1].skill_id, \"A\"); // Root last\n        assert\\!(plan.cycles.is_empty());\n    }\n\n    #[test]\n    fn test_cycle_detection() {\n        let mut registry = MockRegistry::new();\n        registry.add_skill(\"A\", vec\\![\"B\"]);\n        registry.add_skill(\"B\", vec\\![\"C\"]);\n        registry.add_skill(\"C\", vec\\![\"A\"]); // Cycle\\!\n        \n        let resolver = DependencyResolver::new(registry, 10);\n        let plan = resolver.resolve(\"A\", DependencyLoadMode::Auto).unwrap();\n        \n        assert_eq\\!(plan.cycles.len(), 1);\n        assert_eq\\!(plan.cycles[0].len(), 3);\n    }\n\n    #[test]\n    fn test_missing_dependency() {\n        let mut registry = MockRegistry::new();\n        registry.add_skill(\"A\", vec\\![\"B\", \"C\"]);\n        registry.add_skill(\"B\", vec\\![]);\n        // C is missing\n        \n        let resolver = DependencyResolver::new(registry, 10);\n        let plan = resolver.resolve(\"A\", DependencyLoadMode::Auto).unwrap();\n        \n        assert_eq\\!(plan.missing, vec\\![\"C\"]);\n        assert_eq\\!(plan.ordered.len(), 2); // A and B\n    }\n\n    #[test]\n    fn test_disclosure_cascade() {\n        let mut registry = MockRegistry::new();\n        registry.add_skill(\"A\", vec\\![\"B\"]);\n        registry.add_skill(\"B\", vec\\![]);\n        \n        let resolver = DependencyResolver::new(registry, 10);\n        let plan = resolver.resolve(\"A\", DependencyLoadMode::Auto).unwrap();\n        \n        // Root at Full, dependency at Overview\n        assert_eq\\!(plan.ordered[0].disclosure.level, DisclosureLevel::Overview);\n        assert_eq\\!(plan.ordered[1].disclosure.level, DisclosureLevel::Full);\n    }\n}\n```\n\n---\n\n## Logging Requirements\n\n- **DEBUG**: Graph construction, BFS steps, SCC processing\n- **INFO**: Resolution started/completed, cycles found\n- **WARN**: Missing dependencies, deprecated aliases used\n- **ERROR**: Resolution failures, invalid graph state\n\n---\n\n## References\n\n- **Plan Section 3.4**: Dependency Graph and Resolution\n- **Plan Section 3.4.1**: Skill Aliases and Deprecation\n- **Depends on**: meta_skill-qs1 (SQLite), meta_skill-ik6 (SkillSpec)\n- **Blocks**: meta_skill-7va (ms load)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:51:45.322323586-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:00:42.737860176-05:00","labels":["datamodel","dependencies","phase-1"],"dependencies":[{"issue_id":"meta_skill-jka","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:54:01.214027387-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-llm","title":"[P4] Session Quality Scoring","description":"# Session Quality Scoring\n\n## Overview\n\nScore CASS sessions for signal quality before mining. Low‑quality sessions should be ignored to prevent noisy or misleading patterns.\n\n---\n\n## Tasks\n\n1. Define `SessionQuality` signals (tests passed, resolution, code changes).\n2. Compute score with weighted signals + penalties.\n3. Persist quality scores for reuse.\n4. Expose thresholds in config.\n\n---\n\n## Testing Requirements\n\n- Unit tests for scoring logic.\n- Integration tests with sample sessions.\n- Regression tests for edge cases (abandoned sessions).\n\n---\n\n## Acceptance Criteria\n\n- Low‑quality sessions filtered out.\n- Quality score deterministic for same session.\n- Configurable thresholds respected.\n\n---\n\n## Dependencies\n\n- `meta_skill-hhu` CASS Client Integration\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:52.476357365-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:02:30.308036171-05:00","labels":["phase-4","quality","scoring"],"dependencies":[{"issue_id":"meta_skill-llm","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T22:26:13.074292574-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-llm","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:02:59.567791531-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-mc3","title":"CM (cass-memory) Integration","description":"## Section Reference\nIntegration with existing tooling - cass-memory (cm)\n\n## Overview\n\nIntegrate CM (cass-memory) from /data/projects/cass_memory_system as a complementary system for cross-agent learning. CM provides playbook rules with confidence tracking that can enrich skill mining and suggestion.\n\n## Why CM Integration\n\n| CM Feature | ms Application |\n|------------|----------------|\n| **Cross-agent learning** | Mine patterns from all agent types, not just Claude |\n| **Confidence decay** | Apply similar decay to skill effectiveness |\n| **Anti-pattern detection** | Link to skill pitfall sections |\n| **Scientific validation** | Validate skill rules against CASS evidence |\n| **Playbook rules** | Seed skill generation with existing rules |\n\n## Integration Architecture\n\n```rust\n/// CM client for querying playbook rules\nstruct CmClient {\n    /// Path to cm binary\n    cm_path: PathBuf,\n    /// Default flags\n    default_flags: Vec\u003cString\u003e,\n}\n\nimpl CmClient {\n    /// Get relevant context for skill mining\n    async fn get_context(\u0026self, task: \u0026str) -\u003e Result\u003cCmContext\u003e {\n        // Call: cm context \"\u003ctask\u003e\" --json\n    }\n    \n    /// Get playbook rules by category\n    async fn get_rules(\u0026self, category: \u0026str) -\u003e Result\u003cVec\u003cPlaybookRule\u003e\u003e {\n        // Call: cm playbook list --category \u003ccat\u003e --json\n    }\n    \n    /// Check if a rule already exists\n    async fn rule_exists(\u0026self, rule: \u0026str) -\u003e Result\u003cbool\u003e {\n        // Search playbook for similar rules\n    }\n}\n\n/// Context returned by cm\nstruct CmContext {\n    /// Rules that may help with the task\n    relevant_bullets: Vec\u003cPlaybookRule\u003e,\n    /// Pitfalls to avoid\n    anti_patterns: Vec\u003cAntiPattern\u003e,\n    /// Past sessions that solved similar problems\n    history_snippets: Vec\u003cHistorySnippet\u003e,\n    /// Suggested CASS queries for deeper investigation\n    suggested_cass_queries: Vec\u003cString\u003e,\n}\n\nstruct PlaybookRule {\n    id: String,\n    content: String,\n    category: String,\n    confidence: f32,\n    maturity: RuleMaturity,\n    helpful_count: u32,\n    harmful_count: u32,\n}\n\nenum RuleMaturity {\n    Candidate,\n    Established,\n    Proven,\n}\n```\n\n## Skill Mining Enhancements\n\n### 1. Pre-seed with Playbook Rules\n\nBefore mining CASS sessions, query CM for relevant rules:\n\n```rust\nimpl SkillBuilder {\n    async fn build_with_cm(\u0026self, topic: \u0026str) -\u003e Result\u003cSkillSpec\u003e {\n        // Get CM context first\n        let cm_context = self.cm_client.get_context(topic).await?;\n        \n        // Use relevant rules as seed patterns\n        let seed_patterns: Vec\u003cPattern\u003e = cm_context.relevant_bullets\n            .iter()\n            .map(|rule| Pattern::from_cm_rule(rule))\n            .collect();\n        \n        // Use anti-patterns for Pitfalls section\n        let pitfalls: Vec\u003cPitfall\u003e = cm_context.anti_patterns\n            .iter()\n            .map(|ap| Pitfall::from_cm_antipattern(ap))\n            .collect();\n        \n        // Mine CASS with enhanced queries\n        let cass_patterns = self.mine_cass(topic, \u0026cm_context.suggested_cass_queries).await?;\n        \n        // Merge and deduplicate\n        self.merge_patterns(seed_patterns, cass_patterns, pitfalls)\n    }\n}\n```\n\n### 2. Validate Extracted Patterns\n\nUse CM's scientific validation approach:\n\n```rust\nimpl PatternValidator {\n    /// Validate pattern against CASS evidence (CM-style)\n    async fn validate(\u0026self, pattern: \u0026ExtractedPattern) -\u003e ValidationResult {\n        // Search CASS for sessions where this pattern applied\n        let evidence = self.cass_client.search(\u0026pattern.evidence_query()).await?;\n        \n        if evidence.len() \u003c 3 {\n            return ValidationResult::InsufficientEvidence {\n                found: evidence.len(),\n                required: 3,\n            };\n        }\n        \n        // Check outcomes\n        let positive = evidence.iter().filter(|e| e.outcome.is_success()).count();\n        let negative = evidence.iter().filter(|e| e.outcome.is_failure()).count();\n        \n        // Apply CM's 4x harmful multiplier\n        let weighted_score = positive as f32 - (negative as f32 * 4.0);\n        \n        if weighted_score \u003e 0.0 {\n            ValidationResult::Validated { confidence: weighted_score / evidence.len() as f32 }\n        } else {\n            ValidationResult::Rejected { reason: \"More harmful than helpful\" }\n        }\n    }\n}\n```\n\n### 3. Bidirectional Sync\n\nSkills can generate CM playbook rules, and CM rules can seed skills:\n\n```rust\n/// Sync skills to CM playbook\nasync fn sync_skill_to_cm(skill: \u0026SkillSpec, cm: \u0026CmClient) -\u003e Result\u003c()\u003e {\n    for rule in skill.critical_rules() {\n        if !cm.rule_exists(\u0026rule.content).await? {\n            cm.add_rule(\u0026rule.content, \u0026skill.category()).await?;\n        }\n    }\n    Ok(())\n}\n\n/// Generate skill from CM rules\nasync fn skill_from_cm_rules(category: \u0026str, cm: \u0026CmClient) -\u003e Result\u003cSkillSpec\u003e {\n    let rules = cm.get_rules(category).await?;\n    // Convert to skill format\n    SkillSpec::from_cm_rules(rules)\n}\n```\n\n## CLI Commands\n\n```bash\n# Query CM before building\nms build --topic \"react auth\" --with-cm\n\n# Sync skill to CM playbook\nms sync-to-cm \u003cskill\u003e\n\n# Generate skill from CM rules\nms from-cm --category \"debugging\" --output debugging-workflow.skill.yaml\n\n# Show CM context for skill\nms context --cm \"react authentication\"\n```\n\n## Tasks\n\n1. [ ] Implement CmClient wrapper\n2. [ ] Add --with-cm flag to ms build\n3. [ ] Implement pattern seeding from CM rules\n4. [ ] Add validation using CM's evidence gate\n5. [ ] Implement bidirectional sync (skills \u003c-\u003e playbook)\n6. [ ] Add from-cm command for skill generation\n7. [ ] Handle graceful degradation when CM unavailable\n\n## Testing Requirements\n\n- CM integration tests (context, rules, sync)\n- Pattern seeding correctness\n- Validation with 4x harmful multiplier\n- Bidirectional sync tests\n- Graceful degradation when CM unavailable\n\n## Acceptance Criteria\n\n- CM detected and integrated\n- Build command can use CM context\n- Patterns validated against evidence\n- Skills can sync to CM playbook\n- Graceful fallback when CM unavailable\n\n## References\n\n- CM repository: /data/projects/cass_memory_system\n- CM README: /data/projects/cass_memory_system/README.md\n- CM SKILL.md: /data/projects/cass_memory_system/SKILL.md","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T23:09:26.577580416-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:09:26.577580416-05:00","labels":["phase-4 memory cross-agent learning"],"dependencies":[{"issue_id":"meta_skill-mc3","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T23:09:32.054827536-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-mh8","title":"[P2] Tantivy BM25 Full-Text Search","description":"## Tantivy BM25 Full-Text Search (Complete)\n\nTantivy is a Rust port of Apache Lucene, providing high-performance BM25 full-text search. ms uses Tantivy as one half of the hybrid search system.\n\n### Architecture\n\n```\n        ┌───────────────────────────────────────────────────────────────┐\n        │                     Skill Registry                            │\n        └───────────────────────────────────────────────────────────────┘\n                    │\n                    ▼\n        ┌───────────────────────┐             ┌───────────────────────┐\n        │    Full-Text Index    │             │    Vector Index       │\n        │   (Tantivy BM25)      │             │   (Hash Embeddings)   │\n        └───────────────────────┘             └───────────────────────┘\n                    │                                         │\n                    └─────────────────┬───────────────────────┘\n                                      ▼\n                        ┌────────────────────────┐\n                        │   Hybrid Search (RRF)  │\n                        └────────────────────────┘\n```\n\n### Tantivy Schema\n\n```rust\n/// Build the Tantivy schema for skill indexing\nfn build_schema() -\u003e Schema {\n    let mut builder = Schema::builder();\n    \n    // Skill identification\n    builder.add_text_field(\"id\", STRING | STORED);\n    builder.add_text_field(\"name\", TEXT | STORED);\n    \n    // Searchable content\n    builder.add_text_field(\"description\", TEXT);\n    builder.add_text_field(\"body\", TEXT);\n    builder.add_text_field(\"tags\", TEXT);\n    builder.add_text_field(\"aliases\", TEXT);\n    \n    // Metadata for filtering\n    builder.add_text_field(\"layer\", STRING | STORED);\n    builder.add_u64_field(\"quality_score\", FAST | STORED);\n    builder.add_bool_field(\"deprecated\", STORED);\n    \n    builder.build()\n}\n```\n\n### Indexing Flow\n\n```rust\npub struct TantivyIndexer {\n    index: Index,\n    writer: IndexWriter,\n}\n\nimpl TantivyIndexer {\n    pub fn new(index_path: \u0026Path) -\u003e Result\u003cSelf\u003e {\n        let schema = build_schema();\n        let index = Index::create_in_dir(index_path, schema)?;\n        let writer = index.writer(50_000_000)?; // 50MB buffer\n        Ok(Self { index, writer })\n    }\n    \n    pub fn index_skill(\u0026mut self, skill: \u0026Skill) -\u003e Result\u003c()\u003e {\n        let schema = self.index.schema();\n        let mut doc = Document::new();\n        \n        doc.add_text(schema.get_field(\"id\")?, \u0026skill.id);\n        doc.add_text(schema.get_field(\"name\")?, \u0026skill.metadata.name);\n        doc.add_text(schema.get_field(\"description\")?, \u0026skill.metadata.description);\n        doc.add_text(schema.get_field(\"body\")?, \u0026skill.body);\n        doc.add_text(schema.get_field(\"tags\")?, skill.metadata.tags.join(\" \"));\n        doc.add_text(schema.get_field(\"aliases\")?, skill.metadata.aliases.join(\" \"));\n        doc.add_text(schema.get_field(\"layer\")?, format!(\"{:?}\", skill.source.layer));\n        doc.add_u64(schema.get_field(\"quality_score\")?, \n            (skill.computed.quality_score * 100.0) as u64);\n        doc.add_bool(schema.get_field(\"deprecated\")?, \n            skill.metadata.deprecated.is_some());\n        \n        self.writer.add_document(doc)?;\n        Ok(())\n    }\n    \n    pub fn commit(\u0026mut self) -\u003e Result\u003c()\u003e {\n        self.writer.commit()?;\n        Ok(())\n    }\n}\n```\n\n### Search Implementation\n\n```rust\npub struct TantivySearcher {\n    index: Index,\n    reader: IndexReader,\n}\n\nimpl TantivySearcher {\n    pub fn bm25_search(\u0026self, query: \u0026str, limit: usize) -\u003e Result\u003cVec\u003cSearchResult\u003e\u003e {\n        let searcher = self.reader.searcher();\n        let schema = self.index.schema();\n        \n        // Multi-field query parser\n        let query_parser = QueryParser::for_index(\n            \u0026self.index,\n            vec![\n                schema.get_field(\"name\")?,\n                schema.get_field(\"description\")?,\n                schema.get_field(\"body\")?,\n                schema.get_field(\"tags\")?,\n            ],\n        );\n        \n        let query = query_parser.parse_query(query)?;\n        let top_docs = searcher.search(\u0026query, \u0026TopDocs::with_limit(limit))?;\n        \n        let mut results = Vec::new();\n        for (score, doc_address) in top_docs {\n            let doc = searcher.doc(doc_address)?;\n            let skill_id = doc.get_first(schema.get_field(\"id\")?)\n                .and_then(|v| v.as_text())\n                .unwrap_or_default()\n                .to_string();\n            \n            results.push(SearchResult {\n                skill_id,\n                score,\n                source: SearchSource::BM25,\n            });\n        }\n        \n        Ok(results)\n    }\n}\n```\n\n### Index Maintenance\n\n```bash\n# Index all skills\nms index\n\n# Force full re-index\nms index --all\n\n# Incremental index (new/changed only)\nms index --incremental\n\n# Watch mode (background daemon)\nms index --watch\n\n# Health check\nms doctor  # Reports: \"Tantivy index exists\", \"Index in sync with database\"\n```\n\n### File Location\n\n| Path | Purpose |\n|------|---------|\n| `~/.ms/index/` | Global Tantivy index directory |\n| `.ms/index/` | Project-local Tantivy index |\n| `XF_INDEX` env | Override index location |\n\n### Dependencies\n\n```toml\n[dependencies]\ntantivy = \"0.22\"\n```","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:02.169398245-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:11:55.818600502-05:00","labels":["phase-2","search","tantivy"],"dependencies":[{"issue_id":"meta_skill-mh8","depends_on_id":"meta_skill-14h","type":"blocks","created_at":"2026-01-13T22:23:13.459954471-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-n9r","title":"[Cross-Cutting] Security Hardening","description":"# Security Hardening (Cross‑Cutting)\n\n## Overview\n\nApply security best practices across **all** ms subsystems: input validation, secret handling, redaction, command execution, dependency auditing, and error‑message hygiene. This bead is the umbrella security checklist that ensures individual features don’t regress the global threat posture.\n\n---\n\n## Core Security Areas\n\n1. **Input Validation \u0026 Canonicalization**\n   - Normalize + validate all file paths (prevent traversal, symlink escapes).\n   - Reject unexpected path roots and disallow `..` unless explicitly allowed.\n2. **Secret Management**\n   - No secrets written to disk or logs.\n   - Only load secrets from env vars or secure store.\n3. **Command Execution Guarding**\n   - All commands pass through Safety Invariant Layer (DCG).\n4. **Redaction \u0026 Privacy**\n   - PII/secret redaction before storage or display.\n   - Taint propagation for untrusted sources.\n5. **Supply Chain Security**\n   - `cargo audit` + RUSTSEC on CI.\n   - Dependabot / Renovate update policy.\n6. **Error Hygiene**\n   - No error message should leak sensitive content.\n\n---\n\n## Implementation Tasks\n\n1. **Input Validation Utilities**\n   - Add `PathPolicy` utilities: `canonicalize_with_root`, `deny_symlink_escape`.\n2. **Secret Scanning**\n   - Regex + entropy detectors; integrate with redaction pipeline.\n3. **Command Safety Integration**\n   - Ensure all command paths route through DCG guard.\n4. **Redaction Enforcement**\n   - Enforce redaction on all evidence, logs, and skill outputs.\n5. **Dependency Auditing**\n   - Add `cargo audit` (CI) + dependency check policy.\n6. **Security Gate in Doctor**\n   - `ms doctor --check=security` verifies all safety invariants.\n\n---\n\n## Testing Requirements\n\n- Unit tests for path traversal + canonicalization.\n- Unit tests for secret detection + redaction.\n- Integration tests: ensure unsafe paths are rejected in CLI.\n- E2E: run `cargo audit` and ensure CI fails on known advisory.\n- Regression tests for error messages (no secret leakage).\n\n---\n\n## Acceptance Criteria\n\n- All user‑supplied paths are validated and canonicalized.\n- No secrets appear in persisted data or logs.\n- DCG gate enforced for all commands.\n- Redaction runs on all evidence and outputs.\n- CI enforces dependency scanning.\n\n---\n\n## Dependencies\n\n- `meta_skill-qox` Safety Invariant Layer (DCG)\n- `meta_skill-ans` Redaction Pipeline\n- `meta_skill-628` CI/CD Pipeline","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:29:10.040491573-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:45:03.362934676-05:00","labels":["cross-cutting","hardening","security"],"dependencies":[{"issue_id":"meta_skill-n9r","depends_on_id":"meta_skill-qox","type":"blocks","created_at":"2026-01-13T23:45:10.908403648-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-n9r","depends_on_id":"meta_skill-ans","type":"blocks","created_at":"2026-01-13T23:45:19.493750298-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-n9r","depends_on_id":"meta_skill-628","type":"blocks","created_at":"2026-01-13T23:45:28.093272495-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-nf3","title":"[P5] Backup System","description":"# Backup System\n\n## Overview\n\nEnsure durable backups for skills, bundles, and config. Backups must be automatic, versioned, and non‑destructive.\n\n---\n\n## Tasks\n\n1. Define backup schedule (local + optional remote).\n2. Implement snapshotting for `.ms/` and Git archive.\n3. Provide `ms backup list/restore` commands.\n4. Store manifest of backup contents.\n\n---\n\n## Testing Requirements\n\n- Integration tests: backup + restore round‑trip.\n- Failure tests: missing backups handled gracefully.\n\n---\n\n## Acceptance Criteria\n\n- Backups created on schedule.\n- Restore recovers skills without corruption.\n\n---\n\n## Dependencies\n\n- `meta_skill-b98` Git Archive Layer\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:27:07.224390191-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:10:48.33377111-05:00","labels":["backup","phase-5","safety"],"dependencies":[{"issue_id":"meta_skill-nf3","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.539017091-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-nf3","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-14T00:10:57.079283909-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-nf3","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:11:06.857219228-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-nht","title":"[P6] Auto-Update System","description":"# Auto‑Update System\n\n## Overview\n\nSelf‑update mechanism following xf pattern: check for new versions, download, verify signatures, and replace binaries safely.\n\n---\n\n## Tasks\n\n1. Implement update check (version + manifest).\n2. Download release assets securely.\n3. Verify signatures + checksums.\n4. Perform atomic binary swap.\n5. Expose `ms update` and `ms update --check`.\n\n---\n\n## Testing Requirements\n\n- Unit tests for version comparison.\n- Integration tests for download + verify flow.\n- Failure tests for invalid signatures.\n\n---\n\n## Acceptance Criteria\n\n- Updates only install when signature valid.\n- Rollback if update fails.\n- User informed of changes.\n\n---\n\n## Dependencies\n\n- `meta_skill-08m` GitHub Integration\n- `meta_skill-qox` Safety Invariant Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:23.234322584-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:08:11.45165181-05:00","labels":["distribution","phase-6","update"],"dependencies":[{"issue_id":"meta_skill-nht","depends_on_id":"meta_skill-08m","type":"blocks","created_at":"2026-01-13T22:28:37.006068433-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-nht","depends_on_id":"meta_skill-qox","type":"blocks","created_at":"2026-01-14T00:08:22.223684769-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-o8o","title":"[P3] Context-Aware Suggestions","description":"# Context‑Aware Suggestions\n\n## Overview\n\nSuggest relevant skills automatically based on repo state, active files, recent commands, and task context. This is the “recommendation engine” used by shell hooks and MCP.\n\n---\n\n## Core Signals\n\n- Repo root + language detection\n- Active files + file types\n- Recent commands (task intent)\n- Diff summary (what changed)\n- Skill metadata: tags, triggers, quality score\n\n---\n\n## Tasks\n\n1. Define `SuggestionContext` (files, diff, commands, repo).\n2. Score skills using hybrid search + triggers.\n3. Apply cooldowns and novelty penalties.\n4. Emit human + robot outputs.\n\n---\n\n## Testing Requirements\n\n- Unit tests for signal weighting.\n- Integration tests with fixture repos.\n- E2E tests with shell integration.\n\n---\n\n## Acceptance Criteria\n\n- Suggestions are relevant (\u003e80% accept rate in tests).\n- Cooldowns prevent spam.\n- Robot output is stable and deterministic.\n\n---\n\n## Dependencies\n\n- `meta_skill-0ki` ms search\n- `meta_skill-ftj` Tech Stack Detection\n- `meta_skill-8df` Context Fingerprints \u0026 Cooldowns\n- `meta_skill-q5x` Suggestion Signal Bandit","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:16.074120543-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:56:05.106557054-05:00","labels":["context","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-o8o","depends_on_id":"meta_skill-0ki","type":"blocks","created_at":"2026-01-13T22:24:25.953462594-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-o8o","depends_on_id":"meta_skill-ftj","type":"blocks","created_at":"2026-01-13T23:57:00.193896797-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-obj","title":"Brenner Method / ms mine --guided","description":"# Brenner Method / ms mine --guided\n\n## Overview\n\nGuided mining mode that enforces structured reasoning: identify invariants, variables, and generative grammar rather than summarizing transcripts. This produces higher‑quality skills and reduces over‑generalization.\n\n---\n\n## Tasks\n\n1. Implement guided prompts/checkpoints for each mining phase.\n2. Provide interactive accept/reject of generalizations.\n3. Log rationale and evidence per rule.\n4. Output low‑confidence items to Uncertainty Queue.\n\n---\n\n## Testing Requirements\n\n- Unit tests for guided state machine.\n- Integration: guided build produces same output as manual when fully accepted.\n- E2E: guided flow with explicit checkpoints.\n\n---\n\n## Acceptance Criteria\n\n- Guided mode enforces Brenner method steps.\n- Outputs include rationale + evidence summary.\n- Low‑confidence items go to uncertainty queue.\n\n---\n\n## Dependencies\n\n- `meta_skill-9r9` Specific‑to‑General Transformation\n- `meta_skill-4g1` Uncertainty Queue","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:56:54.26584897-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:04:19.645013099-05:00","labels":["brenner","guided","mining","phase-4"],"dependencies":[{"issue_id":"meta_skill-obj","depends_on_id":"meta_skill-9r9","type":"blocks","created_at":"2026-01-14T00:04:28.044400368-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-obj","depends_on_id":"meta_skill-4g1","type":"blocks","created_at":"2026-01-14T00:04:37.656823036-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-on7","title":"[P6] Error Recovery \u0026 Resilience","description":"# Error Recovery \u0026 Resilience\n\n## Overview\n\nEnsure ms is resilient to crashes, partial failures, and corrupted state. Recovery should be automatic and never destructive.\n\n---\n\n## Tasks\n\n1. Enumerate failure modes (DB, Git, index, cache).\n2. Implement recovery handlers for each.\n3. Provide `doctor --fix` paths with no deletes.\n4. Add retry logic with backoff.\n\n---\n\n## Testing Requirements\n\n- Crash simulation tests (2PC, index rebuild).\n- Integration tests: corrupted state recovery.\n\n---\n\n## Acceptance Criteria\n\n- ms can recover from interrupted writes.\n- No data loss on recovery.\n- Errors are actionable.\n\n---\n\n## Dependencies\n\n- `meta_skill-fus` Two‑Phase Commit\n- `meta_skill-q3l` Doctor Command","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:25.711087479-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:09:46.510682243-05:00","labels":["errors","phase-6","resilience"],"dependencies":[{"issue_id":"meta_skill-on7","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:28:37.146732879-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-on7","depends_on_id":"meta_skill-fus","type":"blocks","created_at":"2026-01-14T00:09:55.721022201-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-on7","depends_on_id":"meta_skill-q3l","type":"blocks","created_at":"2026-01-14T00:10:04.036241441-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-q3l","title":"[P6] Doctor Command","description":"# Doctor Command\n\n## Overview\n\n`ms doctor` is the preflight and recovery tool: checks registry health, lock state, cache validity, and can repair inconsistencies without deletions.\n\n---\n\n## Tasks\n\n1. Implement health checks: DB, Git, index, skillpack.\n2. Safety checks: lock state, pending 2PC transactions.\n3. `--fix` mode for safe repair (no deletes).\n4. Perf check: p50/p95/p99 latency output.\n\n---\n\n## Testing Requirements\n\n- Unit tests for each checker.\n- Integration tests: corrupt state → doctor detects.\n- E2E: doctor --fix resolves incomplete 2PC.\n\n---\n\n## Acceptance Criteria\n\n- Doctor detects all common failure modes.\n- Fix mode never deletes data.\n- Outputs usable in robot mode.\n\n---\n\n## Dependencies\n\n- `meta_skill-qs1` SQLite Database Layer\n- `meta_skill-b98` Git Archive Layer\n- `meta_skill-fus` Two‑Phase Commit","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:21.122225821-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:08:34.474092128-05:00","labels":["doctor","health","phase-6"],"dependencies":[{"issue_id":"meta_skill-q3l","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:28:36.860741281-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-q3l","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T22:28:36.89503469-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-q3l","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-14T00:08:44.856676625-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-q3l","depends_on_id":"meta_skill-fus","type":"blocks","created_at":"2026-01-14T00:08:53.245935981-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-q5x","title":"Suggestion Signal Bandit","description":"# Suggestion Signal Bandit\n\n## Overview\n\nThe Suggestion Signal Bandit is a contextual multi-armed bandit that learns per-project weightings over different suggestion signals. Instead of using fixed weights for BM25, embeddings, triggers, freshness, and project match scores, the bandit adaptively learns which signals are most predictive of useful suggestions for each project.\n\n**Why a Bandit?**\n\nDifferent projects have different characteristics:\n- A greenfield project might benefit more from freshness signals (new skills)\n- A mature codebase might benefit more from BM25 (established patterns)\n- A TypeScript project might weight tech stack matching higher\n- A mono-repo might weight project path triggers higher\n\nA contextual bandit learns these preferences from user feedback without requiring explicit configuration.\n\n## Background \u0026 Rationale\n\n### Section 7.2 Reference\n\nFrom the plan Section 7.2:\n\u003e \"A contextual bandit learns per-project weighting over signals (bm25, embeddings, triggers, freshness, project match) using usage/outcome rewards.\"\n\n### Multi-Armed Bandit Basics\n\nThe multi-armed bandit problem models exploration vs exploitation:\n- **Arms**: Different signal weighting strategies\n- **Rewards**: User acceptance/rejection of suggestions\n- **Goal**: Maximize cumulative reward (useful suggestions)\n\nWe use Thompson Sampling with Beta priors for:\n- Efficient exploration of uncertain arms\n- Natural handling of binary rewards (accept/reject)\n- Convergence to optimal arm selection\n\n### Contextual Extension\n\nOur bandit is \"contextual\" because arm selection depends on:\n- Project type (detected tech stack)\n- Task context (recent commands, open files)\n- Time of day / session patterns\n\n## Core Data Structures\n\n### SignalBandit Struct\n\n```rust\nuse std::collections::HashMap;\nuse rand::distributions::Distribution;\nuse rand_distr::Beta;\nuse serde::{Deserialize, Serialize};\n\n/// Type of signal used for suggestion scoring\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum SignalType {\n    /// BM25 text matching score\n    Bm25,\n    /// Semantic embedding similarity\n    Embedding,\n    /// Explicit trigger pattern match\n    Trigger,\n    /// How recently the skill was updated/used\n    Freshness,\n    /// Match with detected project tech stack\n    ProjectMatch,\n    /// Match with current file types being edited\n    FileTypeMatch,\n    /// Match with recent command patterns\n    CommandPattern,\n    /// User's historical acceptance rate for this skill\n    UserHistory,\n}\n\n/// A contextual bandit for learning signal weights\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SignalBandit {\n    /// Map from signal type to its bandit arm\n    pub arms: HashMap\u003cSignalType, BanditArm\u003e,\n    \n    /// Prior distribution for new arms (Beta distribution parameters)\n    pub prior: BetaDistribution,\n    \n    /// Context-specific arm adjustments\n    pub context_modifiers: HashMap\u003cContextKey, ContextModifier\u003e,\n    \n    /// Total number of selections made\n    pub total_selections: u64,\n    \n    /// Configuration for bandit behavior\n    pub config: BanditConfig,\n}\n\n/// A single arm in the multi-armed bandit\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BanditArm {\n    /// Signal type this arm represents\n    pub signal_type: SignalType,\n    \n    /// Number of successes (user accepted suggestion)\n    pub successes: u64,\n    \n    /// Number of failures (user rejected/ignored suggestion)\n    pub failures: u64,\n    \n    /// Current estimated probability of success\n    pub estimated_prob: f64,\n    \n    /// Upper confidence bound for exploration\n    pub ucb: f64,\n    \n    /// Last time this arm was selected\n    pub last_selected: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    \n    /// Decay factor for older observations\n    pub decay_factor: f64,\n}\n\n/// Beta distribution parameters for Thompson Sampling\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub struct BetaDistribution {\n    /// Alpha parameter (prior successes + 1)\n    pub alpha: f64,\n    /// Beta parameter (prior failures + 1)\n    pub beta: f64,\n}\n\nimpl Default for BetaDistribution {\n    fn default() -\u003e Self {\n        // Uniform prior: Beta(1, 1)\n        Self { alpha: 1.0, beta: 1.0 }\n    }\n}\n\nimpl BetaDistribution {\n    /// Create an optimistic prior (expects success)\n    pub fn optimistic() -\u003e Self {\n        Self { alpha: 2.0, beta: 1.0 }\n    }\n    \n    /// Create a pessimistic prior (expects failure)\n    pub fn pessimistic() -\u003e Self {\n        Self { alpha: 1.0, beta: 2.0 }\n    }\n    \n    /// Sample from the distribution\n    pub fn sample(\u0026self, rng: \u0026mut impl rand::Rng) -\u003e f64 {\n        let beta = Beta::new(self.alpha, self.beta).unwrap();\n        beta.sample(rng)\n    }\n}\n```\n\n### Context Modifiers\n\n```rust\n/// Key for context-specific modifications\n#[derive(Debug, Clone, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum ContextKey {\n    /// Tech stack (e.g., \"rust\", \"typescript\")\n    TechStack(String),\n    /// Time of day bucket (morning, afternoon, evening)\n    TimeOfDay(TimeOfDay),\n    /// Project size category\n    ProjectSize(ProjectSize),\n    /// Recent activity pattern\n    ActivityPattern(String),\n}\n\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum TimeOfDay {\n    Morning,    // 6am - 12pm\n    Afternoon,  // 12pm - 6pm\n    Evening,    // 6pm - 12am\n    Night,      // 12am - 6am\n}\n\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum ProjectSize {\n    Small,      // \u003c 1000 files\n    Medium,     // 1000 - 10000 files\n    Large,      // 10000 - 100000 files\n    Massive,    // \u003e 100000 files\n}\n\n/// Modifier applied to arm selection based on context\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ContextModifier {\n    /// Additive bonus to arm's estimated probability\n    pub probability_bonus: HashMap\u003cSignalType, f64\u003e,\n    \n    /// Multiplicative factor for arm's weight\n    pub weight_multiplier: HashMap\u003cSignalType, f64\u003e,\n    \n    /// Number of observations in this context\n    pub observation_count: u64,\n}\n```\n\n### Bandit Configuration\n\n```rust\n/// Configuration for bandit behavior\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BanditConfig {\n    /// Exploration factor (higher = more exploration)\n    pub exploration_factor: f64,\n    \n    /// Decay rate for older observations (0-1, 1 = no decay)\n    pub observation_decay: f64,\n    \n    /// Minimum observations before trusting an arm\n    pub min_observations: u64,\n    \n    /// Whether to use context modifiers\n    pub use_context: bool,\n    \n    /// How often to persist bandit state (in selections)\n    pub persist_frequency: u64,\n    \n    /// Path to persist state\n    pub persistence_path: Option\u003cPathBuf\u003e,\n}\n\nimpl Default for BanditConfig {\n    fn default() -\u003e Self {\n        Self {\n            exploration_factor: 0.1,\n            observation_decay: 0.99,\n            min_observations: 10,\n            use_context: true,\n            persist_frequency: 10,\n            persistence_path: None,\n        }\n    }\n}\n```\n\n## Bandit Implementation\n\n```rust\nimpl SignalBandit {\n    /// Create a new bandit with default arms\n    pub fn new() -\u003e Self {\n        let mut arms = HashMap::new();\n        \n        for signal_type in SignalType::all() {\n            arms.insert(signal_type, BanditArm::new(signal_type));\n        }\n        \n        Self {\n            arms,\n            prior: BetaDistribution::default(),\n            context_modifiers: HashMap::new(),\n            total_selections: 0,\n            config: BanditConfig::default(),\n        }\n    }\n    \n    /// Select signal weights using Thompson Sampling\n    pub fn select_weights(\u0026mut self, context: \u0026SuggestionContext) -\u003e SignalWeights {\n        let mut rng = rand::thread_rng();\n        let mut weights = HashMap::new();\n        \n        for (signal_type, arm) in \u0026self.arms {\n            // Sample from posterior Beta distribution\n            let alpha = self.prior.alpha + arm.successes as f64;\n            let beta = self.prior.beta + arm.failures as f64;\n            let sampled = BetaDistribution { alpha, beta }.sample(\u0026mut rng);\n            \n            // Apply context modifier if enabled\n            let modified = if self.config.use_context {\n                self.apply_context_modifier(sampled, *signal_type, context)\n            } else {\n                sampled\n            };\n            \n            weights.insert(*signal_type, modified);\n        }\n        \n        // Normalize weights to sum to 1\n        let total: f64 = weights.values().sum();\n        for weight in weights.values_mut() {\n            *weight /= total;\n        }\n        \n        self.total_selections += 1;\n        \n        SignalWeights { weights }\n    }\n    \n    /// Apply context-specific modifier to weight\n    fn apply_context_modifier(\n        \u0026self,\n        base_weight: f64,\n        signal_type: SignalType,\n        context: \u0026SuggestionContext,\n    ) -\u003e f64 {\n        let mut weight = base_weight;\n        \n        // Apply tech stack modifier\n        if let Some(stack) = \u0026context.tech_stack {\n            let key = ContextKey::TechStack(stack.clone());\n            if let Some(modifier) = self.context_modifiers.get(\u0026key) {\n                if let Some(bonus) = modifier.probability_bonus.get(\u0026signal_type) {\n                    weight += bonus;\n                }\n                if let Some(mult) = modifier.weight_multiplier.get(\u0026signal_type) {\n                    weight *= mult;\n                }\n            }\n        }\n        \n        // Apply time of day modifier\n        let time_key = ContextKey::TimeOfDay(context.time_of_day());\n        if let Some(modifier) = self.context_modifiers.get(\u0026time_key) {\n            if let Some(mult) = modifier.weight_multiplier.get(\u0026signal_type) {\n                weight *= mult;\n            }\n        }\n        \n        weight.clamp(0.0, 1.0)\n    }\n    \n    /// Update arm based on reward (user feedback)\n    pub fn update(\n        \u0026mut self,\n        signal_type: SignalType,\n        reward: Reward,\n        context: \u0026SuggestionContext,\n    ) {\n        let arm = self.arms.get_mut(\u0026signal_type)\n            .expect(\"Unknown signal type\");\n        \n        // Apply observation decay to existing counts\n        arm.successes = (arm.successes as f64 * self.config.observation_decay) as u64;\n        arm.failures = (arm.failures as f64 * self.config.observation_decay) as u64;\n        \n        // Update counts based on reward\n        match reward {\n            Reward::Success =\u003e arm.successes += 1,\n            Reward::Failure =\u003e arm.failures += 1,\n            Reward::Partial(p) =\u003e {\n                // Fractional reward (e.g., 0.5 for \"used but not immediately\")\n                arm.successes += (p * 100.0) as u64;\n                arm.failures += ((1.0 - p) * 100.0) as u64;\n            }\n        }\n        \n        // Update estimated probability\n        let total = arm.successes + arm.failures;\n        arm.estimated_prob = if total \u003e 0 {\n            arm.successes as f64 / total as f64\n        } else {\n            0.5\n        };\n        \n        arm.last_selected = Some(chrono::Utc::now());\n        \n        // Update context modifier\n        if self.config.use_context {\n            self.update_context_modifier(signal_type, reward, context);\n        }\n        \n        // Persist if needed\n        if self.total_selections % self.config.persist_frequency == 0 {\n            if let Some(path) = \u0026self.config.persistence_path {\n                let _ = self.save(path);\n            }\n        }\n    }\n    \n    /// Update context-specific modifier based on observation\n    fn update_context_modifier(\n        \u0026mut self,\n        signal_type: SignalType,\n        reward: Reward,\n        context: \u0026SuggestionContext,\n    ) {\n        // Update tech stack modifier\n        if let Some(stack) = \u0026context.tech_stack {\n            let key = ContextKey::TechStack(stack.clone());\n            let modifier = self.context_modifiers\n                .entry(key)\n                .or_insert_with(ContextModifier::default);\n            \n            modifier.observation_count += 1;\n            \n            // Adjust probability bonus based on reward\n            let bonus = modifier.probability_bonus\n                .entry(signal_type)\n                .or_insert(0.0);\n            \n            let reward_value = match reward {\n                Reward::Success =\u003e 0.01,\n                Reward::Failure =\u003e -0.01,\n                Reward::Partial(p) =\u003e (p - 0.5) * 0.02,\n            };\n            \n            *bonus = (*bonus + reward_value).clamp(-0.2, 0.2);\n        }\n    }\n    \n    /// Get current arm statistics for debugging/display\n    pub fn get_stats(\u0026self) -\u003e BanditStats {\n        let arm_stats: Vec\u003cArmStats\u003e = self.arms\n            .iter()\n            .map(|(signal_type, arm)| ArmStats {\n                signal_type: *signal_type,\n                successes: arm.successes,\n                failures: arm.failures,\n                estimated_prob: arm.estimated_prob,\n                total_pulls: arm.successes + arm.failures,\n            })\n            .collect();\n        \n        BanditStats {\n            total_selections: self.total_selections,\n            arm_stats,\n            context_modifier_count: self.context_modifiers.len(),\n        }\n    }\n}\n\nimpl BanditArm {\n    pub fn new(signal_type: SignalType) -\u003e Self {\n        Self {\n            signal_type,\n            successes: 0,\n            failures: 0,\n            estimated_prob: 0.5,\n            ucb: 1.0,\n            last_selected: None,\n            decay_factor: 0.99,\n        }\n    }\n}\n\n/// Reward signal from user interaction\n#[derive(Debug, Clone, Copy)]\npub enum Reward {\n    /// User accepted and used the suggestion\n    Success,\n    /// User rejected or ignored the suggestion\n    Failure,\n    /// Partial success (e.g., used later, used partially)\n    Partial(f64),\n}\n\nimpl SignalType {\n    pub fn all() -\u003e Vec\u003cSignalType\u003e {\n        vec![\n            SignalType::Bm25,\n            SignalType::Embedding,\n            SignalType::Trigger,\n            SignalType::Freshness,\n            SignalType::ProjectMatch,\n            SignalType::FileTypeMatch,\n            SignalType::CommandPattern,\n            SignalType::UserHistory,\n        ]\n    }\n}\n```\n\n### Signal Weights Output\n\n```rust\n/// Computed weights for each signal type\n#[derive(Debug, Clone)]\npub struct SignalWeights {\n    pub weights: HashMap\u003cSignalType, f64\u003e,\n}\n\nimpl SignalWeights {\n    /// Get weight for a specific signal type\n    pub fn get(\u0026self, signal_type: SignalType) -\u003e f64 {\n        *self.weights.get(\u0026signal_type).unwrap_or(\u00260.0)\n    }\n    \n    /// Compute weighted score from individual signal scores\n    pub fn compute_score(\u0026self, scores: \u0026SignalScores) -\u003e f64 {\n        let mut total = 0.0;\n        \n        for (signal_type, score) in \u0026scores.scores {\n            let weight = self.get(*signal_type);\n            total += weight * score;\n        }\n        \n        total\n    }\n    \n    /// Format weights for logging\n    pub fn format_for_log(\u0026self) -\u003e String {\n        let mut pairs: Vec\u003c_\u003e = self.weights.iter().collect();\n        pairs.sort_by(|a, b| b.1.partial_cmp(a.1).unwrap());\n        \n        pairs\n            .iter()\n            .map(|(t, w)| format!(\"{:?}={:.3}\", t, w))\n            .collect::\u003cVec\u003c_\u003e\u003e()\n            .join(\", \")\n    }\n}\n\n/// Individual signal scores for a suggestion\n#[derive(Debug, Clone)]\npub struct SignalScores {\n    pub scores: HashMap\u003cSignalType, f64\u003e,\n}\n```\n\n## Integration with Suggestion Engine\n\n```rust\n/// Suggestion engine with bandit-based weighting\npub struct BanditSuggestionEngine {\n    /// The signal bandit\n    bandit: SignalBandit,\n    \n    /// Individual signal scorers\n    scorers: HashMap\u003cSignalType, Box\u003cdyn SignalScorer\u003e\u003e,\n    \n    /// Logger\n    logger: Arc\u003cdyn SuggestionLogger\u003e,\n}\n\nimpl BanditSuggestionEngine {\n    /// Score a skill using bandit-selected weights\n    pub fn score_skill(\n        \u0026mut self,\n        skill: \u0026Skill,\n        context: \u0026SuggestionContext,\n    ) -\u003e ScoredSkill {\n        // Select weights from bandit\n        let weights = self.bandit.select_weights(context);\n        \n        self.logger.log_weights_selected(\u0026weights);\n        \n        // Compute individual signal scores\n        let mut scores = SignalScores { scores: HashMap::new() };\n        \n        for (signal_type, scorer) in \u0026self.scorers {\n            let score = scorer.score(skill, context);\n            scores.scores.insert(*signal_type, score);\n            \n            self.logger.log_signal_score(*signal_type, score);\n        }\n        \n        // Compute weighted total\n        let total_score = weights.compute_score(\u0026scores);\n        \n        self.logger.log_total_score(skill.id(), total_score);\n        \n        ScoredSkill {\n            skill: skill.clone(),\n            score: total_score,\n            signal_scores: scores,\n            weights_used: weights,\n        }\n    }\n    \n    /// Record feedback for learning\n    pub fn record_feedback(\n        \u0026mut self,\n        skill_id: \u0026str,\n        accepted: bool,\n        context: \u0026SuggestionContext,\n        signal_scores: \u0026SignalScores,\n    ) {\n        let reward = if accepted { Reward::Success } else { Reward::Failure };\n        \n        // Update bandit for each signal based on contribution\n        for (signal_type, score) in \u0026signal_scores.scores {\n            // Weight the reward by how much this signal contributed\n            if *score \u003e 0.5 {\n                // This signal was influential\n                self.bandit.update(*signal_type, reward, context);\n            }\n        }\n        \n        self.logger.log_feedback_recorded(skill_id, accepted);\n    }\n}\n\n/// Trait for individual signal scorers\npub trait SignalScorer: Send + Sync {\n    fn score(\u0026self, skill: \u0026Skill, context: \u0026SuggestionContext) -\u003e f64;\n}\n```\n\n## Persistence\n\n```rust\nimpl SignalBandit {\n    /// Save bandit state to disk\n    pub fn save(\u0026self, path: \u0026Path) -\u003e Result\u003c(), BanditError\u003e {\n        if let Some(parent) = path.parent() {\n            std::fs::create_dir_all(parent)?;\n        }\n        \n        let json = serde_json::to_string_pretty(self)?;\n        \n        // Atomic write\n        let temp_path = path.with_extension(\"tmp\");\n        std::fs::write(\u0026temp_path, \u0026json)?;\n        std::fs::rename(\u0026temp_path, path)?;\n        \n        Ok(())\n    }\n    \n    /// Load bandit state from disk\n    pub fn load(path: \u0026Path) -\u003e Result\u003cSelf, BanditError\u003e {\n        if !path.exists() {\n            return Ok(Self::new());\n        }\n        \n        let json = std::fs::read_to_string(path)?;\n        let bandit: Self = serde_json::from_str(\u0026json)?;\n        \n        Ok(bandit)\n    }\n}\n```\n\n## Tasks\n\n### Task 1: Implement Core Bandit Types\n- [ ] Create `src/suggestions/bandit/types.rs`\n- [ ] Implement SignalType enum with all signal types\n- [ ] Implement BanditArm with success/failure tracking\n- [ ] Implement BetaDistribution with sampling\n\n### Task 2: Implement SignalBandit\n- [ ] Create `src/suggestions/bandit/bandit.rs`\n- [ ] Implement Thompson Sampling selection\n- [ ] Implement observation decay\n- [ ] Implement arm update logic\n\n### Task 3: Implement Context Modifiers\n- [ ] Create `src/suggestions/bandit/context.rs`\n- [ ] Implement ContextKey types\n- [ ] Implement ContextModifier application\n- [ ] Implement context modifier learning\n\n### Task 4: Implement Signal Scorers\n- [ ] Create `src/suggestions/bandit/scorers.rs`\n- [ ] Implement BM25 scorer\n- [ ] Implement embedding scorer (with placeholder)\n- [ ] Implement trigger pattern scorer\n- [ ] Implement freshness scorer\n- [ ] Implement project match scorer\n\n### Task 5: Integrate with Suggestion Engine\n- [ ] Modify SuggestionEngine to use bandit\n- [ ] Wire up feedback collection\n- [ ] Add bandit state persistence\n- [ ] Add bandit stats to diagnostic output\n\n### Task 6: Add CLI Commands\n- [ ] Add `ms bandit stats` command\n- [ ] Add `ms bandit reset` command\n- [ ] Add `--no-bandit` flag for fixed weights\n- [ ] Add `--bandit-exploration` flag\n\n## Acceptance Criteria\n\n1. **Learning**: Bandit learns from user feedback over time\n2. **Exploration**: Initial period explores all signals fairly\n3. **Exploitation**: Converges to best signals for each context\n4. **Persistence**: State survives restarts\n5. **Context Sensitivity**: Different contexts produce different weights\n6. **Decay**: Old observations have less influence than recent ones\n7. **Diagnostics**: Stats available for debugging\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_beta_sampling_in_range() {\n        let beta = BetaDistribution { alpha: 1.0, beta: 1.0 };\n        let mut rng = rand::thread_rng();\n        \n        for _ in 0..1000 {\n            let sample = beta.sample(\u0026mut rng);\n            assert!(sample \u003e= 0.0 \u0026\u0026 sample \u003c= 1.0);\n        }\n    }\n    \n    #[test]\n    fn test_arm_update_increases_prob() {\n        let mut bandit = SignalBandit::new();\n        let context = SuggestionContext::default();\n        \n        // Update with many successes\n        for _ in 0..100 {\n            bandit.update(SignalType::Bm25, Reward::Success, \u0026context);\n        }\n        \n        let arm = bandit.arms.get(\u0026SignalType::Bm25).unwrap();\n        assert!(arm.estimated_prob \u003e 0.9);\n    }\n    \n    #[test]\n    fn test_weights_sum_to_one() {\n        let mut bandit = SignalBandit::new();\n        let context = SuggestionContext::default();\n        \n        let weights = bandit.select_weights(\u0026context);\n        let sum: f64 = weights.weights.values().sum();\n        \n        assert!((sum - 1.0).abs() \u003c 0.001);\n    }\n    \n    #[test]\n    fn test_context_modifier_application() {\n        // Test that context modifiers affect weights\n    }\n    \n    #[test]\n    fn test_observation_decay() {\n        // Test that old observations decay over time\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_bandit_learns_from_feedback() {\n    let mut engine = BanditSuggestionEngine::new();\n    let context = SuggestionContext::default();\n    \n    // Simulate many interactions where BM25 is good\n    for _ in 0..100 {\n        engine.record_feedback(\"skill-1\", true, \u0026context, \u0026bm25_high_scores());\n    }\n    \n    // BM25 weight should be higher now\n    let weights = engine.bandit.select_weights(\u0026context);\n    assert!(weights.get(SignalType::Bm25) \u003e 0.2);\n}\n\n#[tokio::test]\nasync fn test_bandit_persistence() {\n    let mut bandit = SignalBandit::new();\n    // Add some observations\n    // Save\n    // Load\n    // Verify state restored\n}\n```\n\n### Logging Requirements\n\n```rust\n// DEBUG level\nlog::debug!(\"Selecting weights with Thompson Sampling\");\nlog::debug!(\"Arm {} sampled: {}\", signal_type, sampled_value);\nlog::debug!(\"Context modifier applied: {:?}\", modifier);\n\n// INFO level\nlog::info!(\"Selected weights: {}\", weights.format_for_log());\nlog::info!(\"Updated arm {} with {:?}\", signal_type, reward);\n\n// WARN level\nlog::warn!(\"Bandit has few observations, weights may be unstable\");\n\n// ERROR level\nlog::error!(\"Failed to persist bandit state: {}\", error);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-o8o` (Context-Aware Suggestions) - Core suggestion infrastructure\n\n## Mathematical Background\n\n### Thompson Sampling\n\nFor each arm $i$ with $s_i$ successes and $f_i$ failures:\n\n1. Sample $\\theta_i \\sim \\text{Beta}(\\alpha + s_i, \\beta + f_i)$\n2. Select arm $i^* = \\arg\\max_i \\theta_i$\n\nThe Beta distribution naturally balances:\n- **Exploration**: Arms with few observations have high variance samples\n- **Exploitation**: Arms with many successes have high mean samples\n\n### Observation Decay\n\nTo adapt to changing preferences, we decay old observations:\n\n$$s_i(t+1) = \\gamma \\cdot s_i(t) + \\mathbb{1}[\\text{success}]$$\n$$f_i(t+1) = \\gamma \\cdot f_i(t) + \\mathbb{1}[\\text{failure}]$$\n\nWhere $\\gamma \\in (0, 1)$ is the decay factor (default 0.99).\n\n## References\n\n- Plan Section 7.2: Context-aware suggestions\n- Thompson Sampling: https://en.wikipedia.org/wiki/Thompson_sampling\n- Contextual Bandits: https://arxiv.org/abs/1003.0146","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:57:49.064226866-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:57:49.064226866-05:00","labels":["bandit","ml","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-q5x","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:00:23.20630478-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-qox","title":"Safety Invariant Layer","description":"# Safety Invariant Layer (DCG-Backed)\n\n## Overview\n\nHard‑enforce the global safety invariant: **no destructive filesystem or git operation executes without explicit, verbatim approval**. This is enforced at runtime via **DCG (Destructive Command Guard)** from `/data/projects/destructive_command_guard`, not via ad‑hoc regexes. This layer gates *all* command execution paths (CLI, skill scripts, automation) and records auditable safety events.\n\nThis bead must be fully self‑contained so future work never needs to consult the main plan or external docs.\n\n---\n\n## Why This Matters\n\n- Aligns with AGENTS.md Rule 1 and irreversible action constraints.\n- Prevents catastrophic operations from automation or malformed skills.\n- Provides consistent, explainable safety behavior across ms features (build, prune, sync, bundle, simulate).\n\n---\n\n## Scope\n\n**In scope:**\n- Any command execution initiated by ms (skill scripts, maintenance, sync, build helpers).\n- Classification into safety tiers (Safe/Caution/Danger/Critical).\n- Mandatory **verbatim approval** for destructive ops.\n- Tombstone deletes (never rm) in ms‑managed directories.\n- Auditable safety events stored in SQLite.\n\n**Out of scope:**\n- OS‑level sandboxing or kernel enforcement (external responsibility).\n- Arbitrary third‑party process supervision beyond ms command execution.\n\n---\n\n## Core Concepts \u0026 Data Model\n\n### DCG Wrapper\n\n```rust\npub struct DcgGuard {\n    pub dcg_bin: PathBuf,\n    pub packs: Vec\u003cString\u003e,\n}\n\npub struct DcgDecision {\n    pub allowed: bool,\n    pub reason: String,\n    pub remediation: Option\u003cString\u003e,\n    pub rule_id: Option\u003cString\u003e,\n    pub pack: Option\u003cString\u003e,\n    pub tier: SafetyTier,\n}\n```\n\n### Safety Event (Audit)\n\n```rust\npub struct CommandSafetyEvent {\n    pub session_id: Option\u003cString\u003e,\n    pub command: String,\n    pub dcg_version: Option\u003cString\u003e,\n    pub dcg_pack: Option\u003cString\u003e,\n    pub decision: DcgDecision,\n    pub created_at: DateTime\u003cUtc\u003e,\n}\n```\n\n### SQLite Tables\n\n- `command_safety_events` stores DCG decisions + context.\n- Use `ms doctor --check=safety` to surface failures.\n\n---\n\n## Behavioral Rules\n\n1. **Default deny** for destructive tiers unless exact approval is present.\n2. **Verbatim approval** required for `Dangerous` + `Critical` tiers.\n3. **Tombstone deletes** inside ms‑managed dirs (no actual delete).\n4. **Fail‑open only for observation** (log + warning) when DCG is unavailable.\n5. **Policy slices** must always be included in packs; packer fails closed if omitted.\n\n---\n\n## Implementation Tasks\n\n1. **DCG Integration**\n   - Implement `DcgGuard::evaluate_command` using `dcg explain` / scan mode.\n   - Map DCG decision to `SafetyTier` and ms policies.\n2. **Command Gate**\n   - Wrap all command execution paths with a `SafetyGate` that consults DCG.\n   - Return structured `approval_required` responses in robot mode.\n3. **Tombstone Deletes**\n   - Replace deletes with tombstone markers in `.ms/tombstones/`.\n   - Add `ms prune --approve` flow for explicit clean‑up.\n4. **Audit Logging**\n   - Persist `CommandSafetyEvent` in SQLite for every gated command.\n   - Expose via `ms doctor --check=safety` and `ms safety log`.\n5. **Config Wiring**\n   - `[safety] dcg_bin`, `dcg_packs`, `dcg_explain_mode`, `require_verbatim_approval`.\n6. **Policy Slice Enforcement**\n   - Mark critical policies as `Policy` slices with `MandatoryPredicate::Always`.\n   - Validate in packer before emitting any content.\n\n---\n\n## Testing Requirements\n\n### Unit Tests\n- Decision mapping (DCG output → SafetyTier → approval requirement).\n- Tombstone creation for delete operations.\n- Mandatory policy slices enforced in packer.\n\n### Integration Tests\n- Run a blocked command and assert `approval_required` in robot output.\n- Run allowed command and ensure it executes normally.\n- Verify `command_safety_events` rows are written.\n\n### E2E\n- Simulate `ms prune` without approval → blocked.\n- Provide exact approval string → allowed and logged.\n\n### Logging\n- **DEBUG**: DCG decision payload\n- **INFO**: approval required events\n- **WARN**: DCG unavailable fallback\n- **ERROR**: attempted destructive command without approval\n\n---\n\n## Acceptance Criteria\n\n- No destructive command executes without explicit approval.\n- DCG decisions logged for every executed command.\n- Tombstone deletes are used consistently in ms‑managed dirs.\n- Policy slices are mandatory and cannot be packed away.\n- Robot mode returns `approval_required` with exact approve hint.\n\n---\n\n## Dependencies\n\n- `meta_skill-vqr` Robot Mode Infrastructure (for structured approval output)\n- `meta_skill-qs1` SQLite Database Layer (for audit logging)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:55:50.573156935-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:44:30.771072081-05:00","labels":["destructive","invariants","phase-4","safety"],"dependencies":[{"issue_id":"meta_skill-qox","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:57:37.95673377-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-qox","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:44:39.595541554-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-qs1","title":"[P1] SQLite Database Layer","description":"## Overview\n\nImplement the SQLite database layer following xf patterns exactly. This provides the structured data persistence for fast queries, skill metadata, embeddings storage, and usage tracking. Combined with the Git archive layer (dual persistence), this forms the foundation for all data operations in ms.\n\n## Background \u0026 Rationale\n\n### Why SQLite\n\n1. **Zero Dependencies**: No external database server required\n2. **Battle-Tested**: Powers billions of devices, extremely reliable\n3. **WAL Mode**: Concurrent readers with single writer\n4. **FTS5**: Built-in full-text search with BM25 ranking\n5. **Local-First**: Works offline, syncs when available\n6. **Performance**: Sub-millisecond queries for most operations\n\n### Why WAL Mode\n\nWrite-Ahead Logging (WAL) mode provides:\n- **Concurrent Reads**: Multiple readers don't block each other\n- **Faster Writes**: Writes append to WAL, not main DB file\n- **Crash Safety**: WAL provides atomic commit guarantees\n- **Checkpoint Control**: Can control when WAL merges with main DB\n\n### PRAGMA Tuning Philosophy\n\nSQLite defaults are conservative. For a local CLI tool, we tune for:\n- **journal_mode=WAL**: Concurrent access\n- **synchronous=NORMAL**: Balance of safety and speed\n- **cache_size=-64000**: 64MB cache (negative = KB)\n- **mmap_size=268435456**: 256MB memory-mapped I/O\n- **temp_store=MEMORY**: Temp tables in RAM\n- **foreign_keys=ON**: Enforce referential integrity\n\n---\n\n## COMPLETE SQLite Schema (from Plan Section 3.2)\n\nThis is the authoritative schema from the big plan. All tables below are required for full ms functionality.\n\n```sql\n-- ============================================================================\n-- CORE SKILL REGISTRY\n-- ============================================================================\n\n-- Core skill registry\nCREATE TABLE skills (\n    id TEXT PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT NOT NULL,\n    version TEXT,\n    author TEXT,\n\n    -- Source tracking\n    source_path TEXT NOT NULL,\n    source_layer TEXT NOT NULL,  -- base | org | project | user\n    git_remote TEXT,\n    git_commit TEXT,\n    content_hash TEXT NOT NULL,\n\n    -- Content\n    body TEXT NOT NULL,\n    metadata_json TEXT NOT NULL,\n    assets_json TEXT NOT NULL,\n\n    -- Computed\n    token_count INTEGER NOT NULL,\n    quality_score REAL NOT NULL,\n\n    -- Timestamps\n    indexed_at TEXT NOT NULL,\n    modified_at TEXT NOT NULL,\n\n    -- Status\n    is_deprecated INTEGER NOT NULL DEFAULT 0,\n    deprecation_reason TEXT\n);\n\n-- Alternate names / legacy ids\nCREATE TABLE skill_aliases (\n    alias TEXT PRIMARY KEY,\n    skill_id TEXT NOT NULL,\n    alias_type TEXT NOT NULL, -- alias | deprecated\n    created_at TEXT NOT NULL,\n    FOREIGN KEY(skill_id) REFERENCES skills(id) ON DELETE CASCADE\n);\n\nCREATE INDEX idx_skill_aliases_skill ON skill_aliases(skill_id);\n\n-- ============================================================================\n-- FULL-TEXT SEARCH (FTS5)\n-- ============================================================================\n\nCREATE VIRTUAL TABLE skills_fts USING fts5(\n    name,\n    description,\n    body,\n    tags,\n    content='skills',\n    content_rowid='rowid'\n);\n\n-- Triggers to keep FTS in sync (INSERT, UPDATE, DELETE)\nCREATE TRIGGER skills_ai AFTER INSERT ON skills BEGIN\n    INSERT INTO skills_fts(rowid, name, description, body, tags)\n    VALUES (NEW.rowid, NEW.name, NEW.description, NEW.body,\n            (SELECT json_extract(NEW.metadata_json, '$.tags')));\nEND;\n\nCREATE TRIGGER skills_ad AFTER DELETE ON skills BEGIN\n    INSERT INTO skills_fts(skills_fts, rowid, name, description, body, tags)\n    VALUES ('delete', OLD.rowid, OLD.name, OLD.description, OLD.body,\n            (SELECT json_extract(OLD.metadata_json, '$.tags')));\nEND;\n\nCREATE TRIGGER skills_au AFTER UPDATE ON skills BEGIN\n    INSERT INTO skills_fts(skills_fts, rowid, name, description, body, tags)\n    VALUES ('delete', OLD.rowid, OLD.name, OLD.description, OLD.body,\n            (SELECT json_extract(OLD.metadata_json, '$.tags')));\n    INSERT INTO skills_fts(rowid, name, description, body, tags)\n    VALUES (NEW.rowid, NEW.name, NEW.description, NEW.body,\n            (SELECT json_extract(NEW.metadata_json, '$.tags')));\nEND;\n\n-- ============================================================================\n-- VECTOR EMBEDDINGS \u0026 SEARCH\n-- ============================================================================\n\n-- Vector embeddings storage\nCREATE TABLE skill_embeddings (\n    skill_id TEXT PRIMARY KEY REFERENCES skills(id),\n    embedding BLOB NOT NULL,  -- f16 quantized, 384 dimensions\n    created_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- PRECOMPILED RUNTIME CACHE\n-- ============================================================================\n\n-- Precompiled runtime skillpack cache\nCREATE TABLE skill_packs (\n    skill_id TEXT PRIMARY KEY REFERENCES skills(id),\n    pack_path TEXT NOT NULL,\n    spec_hash TEXT NOT NULL,\n    slices_hash TEXT NOT NULL,\n    embedding_hash TEXT NOT NULL,\n    predicate_index_hash TEXT NOT NULL,\n    generated_at TEXT NOT NULL\n);\n\n-- Pre-sliced content blocks for token packing\nCREATE TABLE skill_slices (\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    slices_json TEXT NOT NULL,  -- SkillSliceIndex\n    updated_at TEXT NOT NULL,\n    PRIMARY KEY (skill_id)\n);\n\n-- ============================================================================\n-- EVIDENCE \u0026 PROVENANCE\n-- ============================================================================\n\n-- Rule-level evidence and provenance\nCREATE TABLE skill_evidence (\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    rule_id TEXT NOT NULL,\n    evidence_json TEXT NOT NULL,   -- JSON array of EvidenceRef\n    coverage_json TEXT NOT NULL,   -- EvidenceCoverage snapshot\n    updated_at TEXT NOT NULL,\n    PRIMARY KEY (skill_id, rule_id)\n);\n\nCREATE INDEX idx_evidence_skill ON skill_evidence(skill_id);\n\n-- Rule strength calibration (0.0 - 1.0)\nCREATE TABLE skill_rules (\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    rule_id TEXT NOT NULL,\n    strength REAL NOT NULL DEFAULT 0.5,\n    updated_at TEXT NOT NULL,\n    PRIMARY KEY (skill_id, rule_id)\n);\n\n-- ============================================================================\n-- UNCERTAINTY \u0026 ACTIVE LEARNING\n-- ============================================================================\n\n-- Uncertainty queue for low-confidence generalizations\nCREATE TABLE uncertainty_queue (\n    id TEXT PRIMARY KEY,\n    pattern_json TEXT NOT NULL,     -- ExtractedPattern\n    reason TEXT NOT NULL,\n    confidence REAL NOT NULL,\n    suggested_queries TEXT NOT NULL, -- JSON array\n    auto_mine_attempts INTEGER NOT NULL DEFAULT 0,\n    last_mined_at TEXT,\n    status TEXT NOT NULL,            -- pending | resolved | discarded\n    created_at TEXT NOT NULL\n);\n\nCREATE INDEX idx_uncertainty_status ON uncertainty_queue(status);\n\n-- ============================================================================\n-- SAFETY \u0026 SECURITY\n-- ============================================================================\n\n-- Redaction reports for privacy and secret-scrubbing\nCREATE TABLE redaction_reports (\n    id INTEGER PRIMARY KEY,\n    session_id TEXT NOT NULL,\n    report_json TEXT NOT NULL,   -- RedactionReport\n    created_at TEXT NOT NULL\n);\n\nCREATE INDEX idx_redaction_session ON redaction_reports(session_id);\n\n-- Prompt injection reports for safety filtering\nCREATE TABLE injection_reports (\n    id INTEGER PRIMARY KEY,\n    session_id TEXT NOT NULL,\n    acip_version TEXT,\n    acip_mode TEXT,\n    acip_audit_mode INTEGER,\n    report_json TEXT NOT NULL,   -- InjectionReport\n    created_at TEXT NOT NULL\n);\n\nCREATE INDEX idx_injection_session ON injection_reports(session_id);\n\n-- Command safety events (DCG decisions + policy enforcement)\nCREATE TABLE command_safety_events (\n    id INTEGER PRIMARY KEY,\n    session_id TEXT,\n    command TEXT NOT NULL,\n    dcg_version TEXT,\n    dcg_pack TEXT,\n    decision_json TEXT NOT NULL,  -- DcgDecision\n    created_at TEXT NOT NULL\n);\n\nCREATE INDEX idx_command_safety_session ON command_safety_events(session_id);\n\n-- ============================================================================\n-- USAGE TRACKING \u0026 ANALYTICS\n-- ============================================================================\n\n-- Skill usage tracking\nCREATE TABLE skill_usage (\n    id INTEGER PRIMARY KEY,\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    project_path TEXT,\n    used_at TEXT NOT NULL,\n    disclosure_level INTEGER NOT NULL,\n    context_keywords TEXT,  -- JSON array\n    success_signal INTEGER,  -- 1 = worked well, 0 = didn't help, NULL = unknown\n    experiment_id TEXT,\n    variant_id TEXT\n);\n\n-- Skill usage events (full detail for effectiveness analysis)\nCREATE TABLE skill_usage_events (\n    id TEXT PRIMARY KEY,\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    session_id TEXT NOT NULL,\n    loaded_at TEXT NOT NULL,\n    disclosure_level TEXT NOT NULL,   -- JSON\n    discovery_method TEXT NOT NULL,   -- JSON\n    experiment_id TEXT,\n    variant_id TEXT,\n    outcome TEXT,                     -- JSON\n    feedback TEXT                     -- JSON\n);\n\n-- Per-rule outcomes for calibration\nCREATE TABLE rule_outcomes (\n    id TEXT PRIMARY KEY,\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    rule_id TEXT NOT NULL,\n    session_id TEXT NOT NULL,\n    followed INTEGER NOT NULL,\n    outcome TEXT NOT NULL,     -- JSON SessionOutcome\n    created_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- TOOL INTEGRATIONS\n-- ============================================================================\n\n-- UBS static analysis reports (quality gates)\nCREATE TABLE ubs_reports (\n    id INTEGER PRIMARY KEY,\n    project_path TEXT,\n    run_at TEXT NOT NULL,\n    exit_code INTEGER NOT NULL,\n    report_json TEXT NOT NULL      -- UbsReport\n);\n\nCREATE INDEX idx_ubs_project ON ubs_reports(project_path);\n\n-- CM (cass-memory) rule link registry\nCREATE TABLE cm_rule_links (\n    id TEXT PRIMARY KEY,\n    cm_rule_id TEXT NOT NULL,\n    ms_rule_id TEXT NOT NULL,\n    linkage_json TEXT NOT NULL,    -- CmRuleLink\n    updated_at TEXT NOT NULL\n);\n\nCREATE INDEX idx_cm_rule ON cm_rule_links(cm_rule_id);\n\n-- CM sync state (import/export checkpoints)\nCREATE TABLE cm_sync_state (\n    id INTEGER PRIMARY KEY,\n    cm_db_path TEXT,\n    last_imported_at TEXT,\n    last_exported_at TEXT,\n    status_json TEXT,              -- CmSyncStatus\n    updated_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- A/B EXPERIMENTS\n-- ============================================================================\n\n-- A/B experiments for skill variants\nCREATE TABLE skill_experiments (\n    id TEXT PRIMARY KEY,\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    scope TEXT NOT NULL DEFAULT 'skill', -- skill | slice\n    scope_id TEXT,                       -- slice_id if scope = slice\n    variants_json TEXT NOT NULL,      -- Vec\u003cExperimentVariant\u003e\n    allocation_json TEXT NOT NULL,    -- AllocationStrategy\n    status TEXT NOT NULL,\n    started_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- CONCURRENCY \u0026 COORDINATION\n-- ============================================================================\n\n-- Local reservation fallback (when Agent Mail is unavailable)\nCREATE TABLE skill_reservations (\n    id TEXT PRIMARY KEY,\n    path_pattern TEXT NOT NULL,\n    holder TEXT NOT NULL,\n    exclusive INTEGER NOT NULL,\n    expires_at TEXT NOT NULL,\n    created_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- SKILL RELATIONSHIPS\n-- ============================================================================\n\n-- Skill dependencies\nCREATE TABLE skill_dependencies (\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    depends_on TEXT NOT NULL REFERENCES skills(id),\n    PRIMARY KEY (skill_id, depends_on)\n);\n\n-- Capability index (for 'provides')\nCREATE TABLE skill_capabilities (\n    capability TEXT NOT NULL,\n    skill_id TEXT NOT NULL REFERENCES skills(id),\n    PRIMARY KEY (capability, skill_id)\n);\n\n-- ============================================================================\n-- BUILD SESSIONS (CASS INTEGRATION)\n-- ============================================================================\n\n-- Build sessions (CASS integration)\nCREATE TABLE build_sessions (\n    id TEXT PRIMARY KEY,\n    name TEXT NOT NULL,\n    status TEXT NOT NULL,  -- 'draft', 'refining', 'complete', 'published'\n\n    -- CASS queries that seeded this build\n    cass_queries TEXT NOT NULL,  -- JSON array\n\n    -- Extracted patterns\n    patterns_json TEXT NOT NULL,\n\n    -- Generated skill (in progress or complete)\n    draft_skill_json TEXT,\n\n    -- Deterministic source-of-truth\n    skill_spec_json TEXT,   -- SkillSpec (structured parts)\n\n    -- Iteration tracking\n    iteration_count INTEGER NOT NULL DEFAULT 0,\n    last_feedback TEXT,\n\n    -- Timestamps\n    created_at TEXT NOT NULL,\n    updated_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- CONFIGURATION \u0026 TRANSACTIONS\n-- ============================================================================\n\n-- Config store\nCREATE TABLE config (\n    key TEXT PRIMARY KEY,\n    value TEXT NOT NULL,\n    updated_at TEXT NOT NULL\n);\n\n-- Two-phase commit transactions\nCREATE TABLE tx_log (\n    id TEXT PRIMARY KEY,\n    entity_type TEXT NOT NULL,   -- skill | usage | config | build\n    entity_id TEXT NOT NULL,\n    phase TEXT NOT NULL,         -- prepare | commit | complete\n    payload_json TEXT NOT NULL,\n    created_at TEXT NOT NULL\n);\n\n-- CASS session fingerprints for incremental processing\nCREATE TABLE cass_fingerprints (\n    session_id TEXT PRIMARY KEY,\n    content_hash TEXT NOT NULL,\n    updated_at TEXT NOT NULL\n);\n\n-- ============================================================================\n-- INDEXES\n-- ============================================================================\n\nCREATE INDEX idx_skills_name ON skills(name);\nCREATE INDEX idx_skills_modified ON skills(modified_at);\nCREATE INDEX idx_skills_quality ON skills(quality_score DESC);\nCREATE INDEX idx_usage_skill ON skill_usage(skill_id);\nCREATE INDEX idx_usage_time ON skill_usage(used_at);\n```\n\n---\n\n## Key Data Structures (Rust Wrappers)\n\n```rust\nuse rusqlite::{Connection, params};\nuse std::path::{Path, PathBuf};\nuse chrono::{DateTime, Utc};\n\n/// Database connection wrapper with schema management\npub struct Database {\n    /// The underlying SQLite connection\n    conn: Connection,\n    /// Path to the database file\n    path: PathBuf,\n    /// Current schema version\n    schema_version: u32,\n}\n\nimpl Database {\n    /// Current schema version (bump on breaking changes)\n    pub const SCHEMA_VERSION: u32 = 1;\n    \n    /// Open or create database at path\n    pub fn open(path: impl AsRef\u003cPath\u003e) -\u003e Result\u003cSelf\u003e {\n        let path = path.as_ref().to_path_buf();\n        let conn = Connection::open(\u0026path)?;\n        \n        // Apply performance tuning\n        Self::apply_pragmas(\u0026conn)?;\n        \n        let mut db = Self {\n            conn,\n            path,\n            schema_version: 0,\n        };\n        \n        // Run migrations\n        db.migrate()?;\n        \n        Ok(db)\n    }\n    \n    /// Apply performance-tuned PRAGMAs\n    fn apply_pragmas(conn: \u0026Connection) -\u003e Result\u003c()\u003e {\n        conn.execute_batch(r#\"\n            PRAGMA journal_mode = WAL;\n            PRAGMA synchronous = NORMAL;\n            PRAGMA cache_size = -64000;\n            PRAGMA mmap_size = 268435456;\n            PRAGMA temp_store = MEMORY;\n            PRAGMA foreign_keys = ON;\n            PRAGMA auto_vacuum = INCREMENTAL;\n            PRAGMA page_size = 4096;\n        \"#)?;\n        Ok(())\n    }\n}\n```\n\n---\n\n## Tasks\n\n### Task 1: Database Initialization\n- [ ] Create `src/storage/sqlite.rs` module\n- [ ] Implement `Database::open()` with path handling\n- [ ] Create database directory if not exists\n- [ ] Handle database file permissions\n- [ ] Support both file and in-memory databases (for tests)\n\n### Task 2: PRAGMA Tuning\n- [ ] Enable WAL mode for concurrent access\n- [ ] Set synchronous=NORMAL for performance\n- [ ] Configure 64MB cache size\n- [ ] Enable 256MB memory-mapped I/O\n- [ ] Set temp_store to MEMORY\n- [ ] Enable foreign key constraints\n- [ ] Configure auto_vacuum=INCREMENTAL\n\n### Task 3: Migration System\n- [ ] Create _ms_migrations tracking table\n- [ ] Embed migrations at compile time (include_str!)\n- [ ] Run migrations on database open\n- [ ] Track schema version in Database struct\n- [ ] Log migration progress\n\n### Task 4: Core Tables Migration\n- [ ] Create skills table with all columns\n- [ ] Create skill_aliases table with foreign key\n- [ ] Create skill_embeddings table (BLOB storage)\n- [ ] Create skill_packs table (runtime cache)\n- [ ] Create skill_slices table (token packing)\n- [ ] Add all required indexes\n\n### Task 5: FTS5 Full-Text Search\n- [ ] Create skills_fts virtual table\n- [ ] Create insert trigger (skills_ai)\n- [ ] Create delete trigger (skills_ad)\n- [ ] Create update trigger (skills_au)\n- [ ] Test FTS5 queries with MATCH and bm25()\n\n### Task 6: Evidence \u0026 Provenance Tables\n- [ ] Create skill_evidence table\n- [ ] Create skill_rules table (strength calibration)\n- [ ] Create uncertainty_queue table\n- [ ] Add indexes for common queries\n\n### Task 7: Safety Tables\n- [ ] Create redaction_reports table\n- [ ] Create injection_reports table\n- [ ] Create command_safety_events table\n- [ ] Add session_id indexes\n\n### Task 8: Usage Tracking Tables\n- [ ] Create skill_usage table\n- [ ] Create skill_usage_events table\n- [ ] Create rule_outcomes table\n- [ ] Add composite indexes for analytics queries\n\n### Task 9: Integration Tables\n- [ ] Create ubs_reports table (UBS integration)\n- [ ] Create cm_rule_links table (CM integration)\n- [ ] Create cm_sync_state table\n- [ ] Create skill_experiments table (A/B testing)\n\n### Task 10: Coordination Tables\n- [ ] Create skill_reservations table\n- [ ] Create skill_dependencies table\n- [ ] Create skill_capabilities table\n- [ ] Ensure foreign key constraints\n\n### Task 11: Build Session Tables\n- [ ] Create build_sessions table\n- [ ] Create config table\n- [ ] Create tx_log table (two-phase commit)\n- [ ] Create cass_fingerprints table\n\n### Task 12: Query Methods\n- [ ] Implement `get_skill(id)` - single skill lookup\n- [ ] Implement `list_skills(filter)` - filtered listing\n- [ ] Implement `search_fts(query)` - full-text search with bm25()\n- [ ] Implement `upsert_skill(skill)` - insert or update\n- [ ] Implement `delete_skill(id)` - cascade removal\n- [ ] Implement `resolve_alias(alias)` - alias resolution\n\n---\n\n## Acceptance Criteria\n\n1. **Database Creates**: Database file created on first run\n2. **Migrations Run**: All migrations applied automatically with version tracking\n3. **WAL Mode**: `PRAGMA journal_mode` returns 'wal'\n4. **FTS5 Works**: Full-text queries return ranked results via bm25()\n5. **Triggers Fire**: FTS triggers sync on insert/update/delete\n6. **Foreign Keys**: Invalid references rejected with helpful errors\n7. **Concurrency**: Multiple readers work simultaneously (WAL mode)\n8. **Performance**: Simple queries complete in \u003c1ms, FTS in \u003c10ms\n9. **All Tables**: All 25+ tables from schema created successfully\n\n---\n\n## Testing Requirements\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n    \n    #[test]\n    fn test_database_creation() {\n        let dir = tempdir().unwrap();\n        let db_path = dir.path().join(\"test.db\");\n        \n        let db = Database::open(\u0026db_path).unwrap();\n        assert!(db_path.exists());\n        assert_eq!(db.schema_version, Database::SCHEMA_VERSION);\n    }\n    \n    #[test]\n    fn test_wal_mode_enabled() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        let mode: String = db.conn.query_row(\n            \"PRAGMA journal_mode\",\n            [],\n            |row| row.get(0),\n        ).unwrap();\n        \n        assert_eq!(mode.to_lowercase(), \"wal\");\n    }\n    \n    #[test]\n    fn test_all_tables_created() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        let tables = vec![\n            \"skills\", \"skill_aliases\", \"skill_embeddings\", \"skill_packs\",\n            \"skill_slices\", \"skill_evidence\", \"skill_rules\", \"uncertainty_queue\",\n            \"redaction_reports\", \"injection_reports\", \"command_safety_events\",\n            \"skill_usage\", \"skill_usage_events\", \"rule_outcomes\", \"ubs_reports\",\n            \"cm_rule_links\", \"cm_sync_state\", \"skill_experiments\",\n            \"skill_reservations\", \"skill_dependencies\", \"skill_capabilities\",\n            \"build_sessions\", \"config\", \"tx_log\", \"cass_fingerprints\",\n        ];\n        \n        for table in tables {\n            let exists: i32 = db.conn.query_row(\n                \"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name=?\",\n                [table],\n                |row| row.get(0),\n            ).unwrap();\n            assert_eq!(exists, 1, \"Table {} should exist\", table);\n        }\n    }\n    \n    #[test]\n    fn test_fts5_search() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        // Insert skill\n        db.conn.execute(\n            \"INSERT INTO skills (id, name, description, source_path, source_layer, \n             content_hash, body, metadata_json, assets_json, token_count, \n             quality_score, indexed_at, modified_at)\n             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'))\",\n            params![\n                \"git-commit\", \"Git Commit Patterns\", \"Best practices for commits\",\n                \"/skills/git\", \"base\", \"abc123\", \"Write good commit messages\",\n                r#\"{\"tags\": \"git,workflow\"}\"#, \"{}\", 500, 0.85\n            ],\n        ).unwrap();\n        \n        // Search\n        let id: String = db.conn.query_row(\n            \"SELECT id FROM skills_fts WHERE skills_fts MATCH ? ORDER BY bm25(skills_fts)\",\n            [\"commit\"],\n            |row| row.get(0),\n        ).unwrap();\n        \n        assert_eq!(id, \"git-commit\");\n    }\n}\n```\n\n---\n\n## Logging Requirements\n\nAll database operations must log:\n- **DEBUG**: SQL queries, parameter count, execution times\n- **INFO**: Database opened, migrations applied, schema version\n- **WARN**: Slow queries (\u003e100ms), constraint violations caught\n- **ERROR**: Database corruption, migration failures, unrecoverable errors\n\n---\n\n## References\n\n- **Plan Section 3.2**: SQLite Schema (authoritative source)\n- **Plan Section 3.7**: Two-Phase Commit for Dual Persistence\n- **xf implementation**: /data/projects/xf/src/db/mod.rs\n- **Depends on**: meta_skill-5s0 (Rust Project Scaffolding)\n- **Blocks**: meta_skill-14h (CLI Commands), meta_skill-ch6 (Hash Embeddings), meta_skill-fus (2PC)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:21:59.808662035-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:56:19.993024085-05:00","labels":["database","phase-1","sqlite"],"dependencies":[{"issue_id":"meta_skill-qs1","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.795926271-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-r6k","title":"[P2] Skill Alias System","description":"# Skill Alias System\n\n## Overview\n\nSupport alternate skill identifiers (legacy names, short aliases) to improve search and load resilience. Aliases must resolve deterministically and be searchable.\n\n---\n\n## Tasks\n\n1. Extend SkillSpec metadata to include aliases.\n2. Index aliases in search and lookup.\n3. Resolve conflicts with precedence rules.\n\n---\n\n## Testing Requirements\n\n- Unit tests for alias resolution + conflict handling.\n- Integration tests: search by alias.\n\n---\n\n## Acceptance Criteria\n\n- Aliases resolve to canonical skill.\n- Search + load work with aliases.\n\n---\n\n## Dependencies\n\n- `meta_skill-ik6` SkillSpec Data Model\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:23:04.728286559-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:00:52.390159309-05:00","labels":["aliases","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-r6k","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:23:13.569822748-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-r6k","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-14T00:01:02.673227968-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-red","title":"[Cross-Cutting] Performance Optimization","description":"# Performance Optimization (Cross‑Cutting)\n\n## Overview\n\nEnsure ms meets latency and throughput targets for indexing, search, packing, and mining. This bead defines performance budgets, profiling workflow, caching strategy, and regression detection.\n\n---\n\n## Targets (Non‑Negotiable)\n\n| Metric | Target |\n|---|---|\n| Indexing speed | ≥ 1000 skills/sec |\n| Search latency | \u003c 50ms p99 |\n| Idle memory | \u003c 100MB |\n| Binary size | \u003c 20MB stripped |\n| Build session start | \u003c 2s |\n\n---\n\n## Key Optimization Strategies\n\n- **SIMD / SoA layouts** for embedding operations.\n- **Parallel batch ops** (Rayon) for indexing + mining.\n- **LRU caches** for parsed sessions + rendered templates.\n- **Pre‑computed slices** and **skillpack binaries** for fast load.\n- **String interning** for tags, ids, and section headers.\n\n---\n\n## Tasks\n\n1. Define performance budgets per subsystem (search, pack, suggest, build).\n2. Add profiling build + flamegraph instructions.\n3. Add benchmark suite (Criterion) per hot path.\n4. Wire performance regression alerts in CI.\n5. Add memory usage sampling in `ms doctor --check=perf`.\n\n---\n\n## Testing \u0026 Benchmarking\n\n- Benchmark suite in `meta_skill-ftb` must include search/pack/suggest.\n- Add p50/p95/p99 latency assertions in tests.\n- Store benchmark baselines for regression detection.\n\n---\n\n## Acceptance Criteria\n\n- Benchmarks exist for all hot paths.\n- Performance stays within targets on CI runners.\n- Regression checks are enforced before release.\n\n---\n\n## Dependencies\n\n- `meta_skill-ftb` Benchmark Tests\n- `meta_skill-q3l` Doctor Command (perf checks)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:29:08.882398138-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:47:31.123372906-05:00","labels":["cross-cutting","optimization","performance"],"dependencies":[{"issue_id":"meta_skill-red","depends_on_id":"meta_skill-ftb","type":"blocks","created_at":"2026-01-13T23:47:39.914695949-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-red","depends_on_id":"meta_skill-q3l","type":"blocks","created_at":"2026-01-13T23:47:47.964979373-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-rvd","title":"[P6] Skill Effectiveness Tracking","description":"# Skill Effectiveness Tracking\n\nTrack whether skills actually help agents.\n\n## Tasks\n1. Record skill loads with context\n2. Track session outcomes\n3. Infer effectiveness from outcomes\n4. Update quality scores\n5. A/B experiment framework\n\n## Tracking Events (from Section 22)\n- skill_loaded: When skill loaded into context\n- session_completed: Session reached goal\n- session_failed: Session failed\n- explicit_feedback: User thumbs up/down\n\n## Inference Logic\n- Skill loaded + session success = positive signal\n- Skill loaded + session failure = negative signal\n- Weight by recency\n\n## A/B Experiments (from Section 22.4.1)\n- Create variants of same skill\n- Randomly assign to sessions\n- Compare outcomes\n- Promote winning variant\n\n## Storage\n```sql\nCREATE TABLE skill_usage (\n    id TEXT PRIMARY KEY,\n    skill_id TEXT,\n    session_id TEXT,\n    outcome TEXT,  -- success, failure, unknown\n    loaded_at TIMESTAMP,\n    feedback TEXT  -- positive, negative, null\n);\n```\n\n## Acceptance Criteria\n- Usage tracked automatically\n- Effectiveness scores computed\n- A/B framework functional","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:24.171197012-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:21.925060629-05:00","closed_at":"2026-01-13T23:42:21.925060629-05:00","close_reason":"Duplicate of meta_skill-iim (Skill Effectiveness Feedback Loop)","labels":["analytics","effectiveness","phase-6"],"dependencies":[{"issue_id":"meta_skill-rvd","depends_on_id":"meta_skill-7va","type":"blocks","created_at":"2026-01-13T22:28:37.033158307-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-rvd","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:28:37.060620812-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-sqh","title":"[P3] Disclosure Levels System","description":"# Disclosure Levels System\n\n## Overview\n\nProvide progressive disclosure levels for skills (minimal → complete) to balance token cost with usefulness. This governs what ms load returns when no explicit pack budget is specified.\n\n---\n\n## Tasks\n\n1. Define `DisclosureLevel` enum + token targets.\n2. Implement slice filtering per level.\n3. Add level recommendation heuristics from context.\n4. Support explicit level overrides in CLI.\n\n---\n\n## Testing Requirements\n\n- Unit tests for level selection and filtering.\n- Integration tests with real skills at each level.\n- Snapshot tests for level outputs.\n\n---\n\n## Acceptance Criteria\n\n- Level outputs stay within target token ranges.\n- Higher levels always superset lower levels.\n- CLI `--level` matches expected output.\n\n---\n\n## Dependencies\n\n- `meta_skill-ik6` SkillSpec Data Model","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:12.151427753-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:58:44.450277791-05:00","labels":["disclosure","phase-3","ux"],"dependencies":[{"issue_id":"meta_skill-sqh","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:24:25.816771637-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-swe","title":"[P5] Local Modification Safety","description":"# Local Modification Safety\n\n## Overview\n\nProtect local user modifications when installing or updating bundles. Ensure merges are safe, reversible, and explicit.\n\n---\n\n## Tasks\n\n1. Detect local modifications vs bundle content.\n2. Support conflict resolution strategies (prefer local/remote/merge).\n3. Write audit log for resolution decisions.\n4. Provide `ms bundle conflicts` command.\n\n---\n\n## Testing Requirements\n\n- Integration tests for conflict detection.\n- Merge strategy tests.\n- E2E: bundle update with local edits.\n\n---\n\n## Acceptance Criteria\n\n- No local modifications lost without explicit approval.\n- Conflicts surfaced clearly.\n- Deterministic merge outcomes.\n\n---\n\n## Dependencies\n\n- `meta_skill-6fi` Bundle Format and Manifest\n- `meta_skill-qox` Safety Invariant Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:05.252577245-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:07:17.656967779-05:00","labels":["bundles","phase-5","safety"],"dependencies":[{"issue_id":"meta_skill-swe","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.401857842-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-swe","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:27:15.428882372-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-swe","depends_on_id":"meta_skill-qox","type":"blocks","created_at":"2026-01-14T00:07:30.182586547-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-tdj","title":"[P3] Meta-Skills (Composed Bundles)","description":"# Meta-Skills (Composed Bundles)\n\nSkills that compose slices from multiple skills.\n\n## Tasks\n1. Define MetaSkill spec format\n2. Cross-skill slice references\n3. Deduplication of overlapping content\n4. Coherent ordering of composed slices\n5. CLI for meta-skill creation\n\n## Meta-Skill Spec (from Section 5.5)\n```yaml\nname: react-debugging\ntype: meta-skill\ncompose:\n  - skill: react/hooks\n    slices: [pitfall-1, pitfall-2]\n  - skill: typescript/strict\n    slices: [rule-1, rule-2]\n  - skill: chrome-devtools\n    slices: [command-inspect]\n```\n\n## Use Cases\n- Combine debugging skills for specific stack\n- Create role-specific skill bundles\n- Curate onboarding sets\n\n## Coherence\n- Slices ordered by type (rules → commands → examples)\n- Duplicates removed (same content from multiple sources)\n- Transitions smoothed\n\n## Acceptance Criteria\n- Meta-skills compose correctly\n- Duplicates deduplicated\n- Order is logical","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:24:18.563378802-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:41:43.399656607-05:00","closed_at":"2026-01-13T23:41:43.399656607-05:00","close_reason":"Duplicate of meta_skill-7ws (Meta-Skills)","labels":["composition","meta-skill","phase-3"],"dependencies":[{"issue_id":"meta_skill-tdj","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T22:24:26.061331463-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-tun","title":"Anti-Pattern Mining","description":"## Section Reference\nSection 5.14 - Anti-Pattern Extraction and Presentation\n\n## Overview\nExtract anti-patterns from failure signals, marked anti-pattern sessions, and explicit \"wrong\" fixes. Present these as \"Avoid / When NOT to use\" sections in generated skills.\n\n## Core Concept\n**Counterexamples are first-class patterns** with the same pipeline as positive patterns:\n- Extraction → Clustering → Synthesis → Packing\n\nEach anti-pattern MUST link to the positive rule it constrains. Anti-patterns without positive counterparts are orphaned and flagged for review.\n\n## Data Structures\n\n```rust\n/// A negative pattern extracted from failure evidence\nstruct AntiPattern {\n    id: AntiPatternId,\n    /// The positive pattern this anti-pattern constrains\n    constrains: PatternId,\n    /// Extracted from failure/rollback evidence\n    evidence: Vec\u003cAntiPatternEvidence\u003e,\n    /// Synthesized \"do not\" rule\n    rule: NegativeRule,\n    /// When this anti-pattern applies\n    trigger_conditions: Vec\u003cCondition\u003e,\n    /// What goes wrong when violated\n    failure_modes: Vec\u003cFailureMode\u003e,\n    /// Confidence based on evidence strength\n    confidence: f32,\n}\n\nstruct AntiPatternEvidence {\n    source: AntiPatternSource,\n    session_id: SessionId,\n    /// The specific failure or correction\n    incident: FailureIncident,\n    /// User-provided context if marked explicitly\n    user_annotation: Option\u003cString\u003e,\n}\n\nenum AntiPatternSource {\n    /// Session explicitly marked as anti-pattern example\n    MarkedAntiPattern { marker: String },\n    /// Detected from rollback or undo sequence\n    RollbackDetected { rollback_type: RollbackType },\n    /// Explicit \"wrong\" fix with correction\n    WrongFix { original: String, correction: String },\n    /// Failure signal in session\n    FailureSignal { signal_type: FailureSignalType },\n    /// Counter-example surfaced during uncertainty resolution\n    CounterExample { uncertainty_id: UncertaintyId },\n}\n\nenum RollbackType {\n    GitReset,\n    GitRevert,\n    FileRestore,\n    ManualUndo,\n    ExplicitCorrection,\n}\n\nenum FailureSignalType {\n    TestFailure,\n    BuildError,\n    RuntimeException,\n    UserRejection,\n    ExplicitNo,\n    Frustration,\n}\n\nstruct NegativeRule {\n    /// \"NEVER do X when Y\"\n    statement: String,\n    /// Formal predicate for rule matching\n    predicate: Predicate,\n    /// Severity if violated\n    severity: AntiPatternSeverity,\n}\n\nenum AntiPatternSeverity {\n    /// Suggestion to avoid\n    Advisory,\n    /// Strong recommendation against\n    Warning,\n    /// Must not do - blocks action\n    Blocking,\n}\n\nstruct FailureMode {\n    description: String,\n    observed_count: u32,\n    example_session: Option\u003cSessionId\u003e,\n}\n```\n\n## Extraction Pipeline\n\n### Phase 1: Signal Detection\n```rust\ntrait AntiPatternDetector {\n    /// Scan session for anti-pattern signals\n    fn detect_signals(\u0026self, session: \u0026Session) -\u003e Vec\u003cAntiPatternSignal\u003e;\n    \n    /// Check for explicit anti-pattern markers\n    fn check_markers(\u0026self, session: \u0026Session) -\u003e Option\u003cMarkedAntiPattern\u003e;\n    \n    /// Detect rollback/undo sequences\n    fn detect_rollbacks(\u0026self, session: \u0026Session) -\u003e Vec\u003cRollbackSequence\u003e;\n    \n    /// Find explicit corrections (\"No, do X instead\")\n    fn find_corrections(\u0026self, session: \u0026Session) -\u003e Vec\u003cCorrection\u003e;\n}\n\nstruct AntiPatternSignal {\n    signal_type: FailureSignalType,\n    location: MessageIndex,\n    context: ContextWindow,\n    /// What action preceded the failure\n    preceding_action: Option\u003cActionSummary\u003e,\n}\n```\n\n### Phase 2: Context Extraction\n```rust\n/// Extract the \"what went wrong\" context\nstruct AntiPatternContext {\n    /// The action that failed\n    failed_action: ActionDescription,\n    /// Why it failed (if determinable)\n    failure_reason: Option\u003cString\u003e,\n    /// What conditions made it wrong\n    conditions: Vec\u003cCondition\u003e,\n    /// The correction applied (if any)\n    correction: Option\u003cActionDescription\u003e,\n}\n\nfn extract_anti_pattern_context(\n    signal: \u0026AntiPatternSignal,\n    session: \u0026Session,\n) -\u003e Result\u003cAntiPatternContext, ExtractionError\u003e;\n```\n\n### Phase 3: Clustering\n```rust\n/// Cluster similar anti-patterns across sessions\nfn cluster_anti_patterns(\n    patterns: Vec\u003cAntiPatternContext\u003e,\n    similarity_threshold: f32,\n) -\u003e Vec\u003cAntiPatternCluster\u003e;\n\nstruct AntiPatternCluster {\n    id: ClusterId,\n    /// Representative pattern for this cluster\n    centroid: AntiPatternContext,\n    /// All patterns in cluster\n    members: Vec\u003cAntiPatternContext\u003e,\n    /// Derived conditions when this anti-pattern applies\n    synthesized_conditions: Vec\u003cCondition\u003e,\n}\n```\n\n### Phase 4: Synthesis\n```rust\n/// Synthesize cluster into formal anti-pattern\nfn synthesize_anti_pattern(\n    cluster: \u0026AntiPatternCluster,\n    positive_patterns: \u0026[Pattern],\n) -\u003e Result\u003cAntiPattern, SynthesisError\u003e;\n\n/// Link anti-pattern to the positive rule it constrains\nfn find_constrained_pattern(\n    anti: \u0026AntiPatternContext,\n    patterns: \u0026[Pattern],\n) -\u003e Option\u003cPatternId\u003e;\n```\n\n### Phase 5: Packing\n```rust\n/// Pack anti-patterns into skill output\nstruct AntiPatternSection {\n    header: String, // \"## Avoid / When NOT to use\"\n    patterns: Vec\u003cFormattedAntiPattern\u003e,\n}\n\nstruct FormattedAntiPattern {\n    /// \"NEVER X when Y\"\n    rule: String,\n    /// Why this is wrong\n    rationale: String,\n    /// What to do instead (link to positive pattern)\n    instead: String,\n    /// Example from evidence\n    example: Option\u003cString\u003e,\n}\n```\n\n## CLI Commands\n\n```bash\n# Extract anti-patterns from sessions\nms mine --anti-patterns\n\n# Mine specific failure sessions\nms mine --failures-only\n\n# Show anti-patterns for a skill\nms skill show \u003cskill-id\u003e --anti-patterns\n\n# List orphaned anti-patterns (no positive counterpart)\nms anti-patterns --orphaned\n\n# Link anti-pattern to positive rule manually\nms anti-patterns link \u003canti-id\u003e \u003cpattern-id\u003e\n\n# Mark session as anti-pattern example\nms session mark \u003csession-id\u003e --anti-pattern --note \"Shows wrong approach to X\"\n```\n\n## Output Format\n\nIn generated skills, anti-patterns appear as:\n\n```markdown\n## Avoid / When NOT to use\n\n### NEVER force-push to shared branches without coordination\n**Severity**: Blocking\n**Conditions**: Branch has upstream, other contributors active\n**Failure mode**: Overwrites others work, causes merge conflicts\n**Instead**: Use git push --force-with-lease or coordinate first\n**Evidence**: 3 sessions, 2 explicit corrections\n\n### AVOID using recursive delete in scripts without path validation\n**Severity**: Warning  \n**Conditions**: Path comes from variable, script runs unattended\n**Failure mode**: Variable expansion to root or home, catastrophic deletion\n**Instead**: Validate path exists and is expected location first\n**Evidence**: 1 rollback, 1 explicit wrong fix\n```\n\n## Integration Points\n\n- **Pattern Extraction Pipeline** (meta_skill-237): Anti-patterns use same extraction infrastructure\n- **Uncertainty Queue**: Counter-examples feed anti-pattern mining\n- **Skill Packer**: Include anti-pattern section in output\n- **Linter**: Check for anti-pattern violations in new patterns\n\n## Validation\n\n- Every anti-pattern MUST link to a positive pattern (or be flagged as orphaned)\n- Anti-patterns MUST have at least 2 evidence sources for Warning severity\n- Blocking severity requires explicit user confirmation\n- Evidence must be traceable to source sessions\n\n## Testing Requirements\n\n- Unit tests for each detector type (rollback, correction, marker)\n- Integration test: full pipeline from session to packed anti-pattern\n- Test orphaned anti-pattern detection\n- Test severity escalation based on evidence count","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:52:28.403401172-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:52:28.403401172-05:00","labels":["antipatterns","mining","phase-4"],"dependencies":[{"issue_id":"meta_skill-tun","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:57:35.568728177-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-tzu","title":"Agent Mail Integration","description":"# Agent Mail Integration\n\n**Phase 6 - Section 20**\n\nIntegrate with Agent Mail MCP server for multi-agent skill coordination. This enables agents to share patterns, coordinate skill generation, and request skills from other agents working on related projects.\n\n---\n\n## Overview\n\nWhen multiple agents work on related projects or within the same organization, they can benefit from coordination:\n\n1. **Pattern Sharing**: Share discovered patterns with other agents\n2. **Skill Requests**: Request skills on topics you need but don't have\n3. **Generation Coordination**: Avoid duplicate work when building skills\n4. **Knowledge Distribution**: Broadcast useful skills to interested agents\n\nAgent Mail provides a message-passing infrastructure between agents. This integration makes meta_skill a first-class participant in multi-agent workflows.\n\n---\n\n## Core Data Structures\n\n### Agent Mail Client\n\n```rust\nuse std::collections::HashMap;\n\n/// Client for Agent Mail MCP server communication\npub struct AgentMailClient {\n    /// Project identifier (for message routing)\n    pub project_key: String,\n    \n    /// This agent's name/identifier\n    pub agent_name: String,\n    \n    /// MCP server endpoint\n    pub mcp_endpoint: String,\n    \n    /// Connection state\n    state: ConnectionState,\n    \n    /// Message handlers\n    handlers: HashMap\u003cMessageType, Box\u003cdyn MessageHandler\u003e\u003e,\n    \n    /// Subscriptions\n    subscriptions: Vec\u003cSubscription\u003e,\n}\n\n#[derive(Debug, Clone)]\npub enum ConnectionState {\n    Disconnected,\n    Connecting,\n    Connected { session_id: String },\n    Reconnecting { attempts: u32 },\n    Failed { error: String },\n}\n\nimpl AgentMailClient {\n    /// Create a new Agent Mail client\n    pub fn new(project_key: \u0026str, agent_name: \u0026str, endpoint: \u0026str) -\u003e Self {\n        Self {\n            project_key: project_key.to_string(),\n            agent_name: agent_name.to_string(),\n            mcp_endpoint: endpoint.to_string(),\n            state: ConnectionState::Disconnected,\n            handlers: HashMap::new(),\n            subscriptions: Vec::new(),\n        }\n    }\n    \n    /// Connect to Agent Mail server\n    pub async fn connect(\u0026mut self) -\u003e Result\u003c(), AgentMailError\u003e {\n        self.state = ConnectionState::Connecting;\n        \n        // Initialize MCP connection\n        let mcp_client = McpClient::connect(\u0026self.mcp_endpoint).await?;\n        \n        // Register as agent\n        let session_id = mcp_client.call(\n            \"agent_mail/register\",\n            json!({\n                \"project_key\": self.project_key,\n                \"agent_name\": self.agent_name,\n                \"capabilities\": [\"skill_sharing\", \"pattern_discovery\", \"skill_requests\"]\n            })\n        ).await?;\n        \n        self.state = ConnectionState::Connected { session_id };\n        \n        // Subscribe to skill-related topics\n        self.subscribe_to_defaults().await?;\n        \n        Ok(())\n    }\n    \n    /// Subscribe to default skill topics\n    async fn subscribe_to_defaults(\u0026mut self) -\u003e Result\u003c(), AgentMailError\u003e {\n        let default_topics = vec![\n            \"skills/new\",\n            \"skills/requests\",\n            \"patterns/discovered\",\n            format!(\"projects/{}/skills\", self.project_key),\n        ];\n        \n        for topic in default_topics {\n            self.subscribe(\u0026topic).await?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Subscribe to a topic\n    pub async fn subscribe(\u0026mut self, topic: \u0026str) -\u003e Result\u003c(), AgentMailError\u003e {\n        if !matches!(self.state, ConnectionState::Connected { .. }) {\n            return Err(AgentMailError::NotConnected);\n        }\n        \n        let subscription = Subscription {\n            topic: topic.to_string(),\n            subscribed_at: Utc::now(),\n            message_count: 0,\n        };\n        \n        // Register subscription with server\n        self.call_mcp(\"agent_mail/subscribe\", json!({\n            \"topic\": topic,\n            \"agent\": self.agent_name\n        })).await?;\n        \n        self.subscriptions.push(subscription);\n        Ok(())\n    }\n    \n    /// Send a message to a topic\n    pub async fn publish(\u0026self, topic: \u0026str, message: Message) -\u003e Result\u003c(), AgentMailError\u003e {\n        if !matches!(self.state, ConnectionState::Connected { .. }) {\n            return Err(AgentMailError::NotConnected);\n        }\n        \n        self.call_mcp(\"agent_mail/publish\", json!({\n            \"topic\": topic,\n            \"message\": message,\n            \"sender\": self.agent_name\n        })).await?;\n        \n        Ok(())\n    }\n    \n    /// Send a direct message to another agent\n    pub async fn send_direct(\u0026self, recipient: \u0026str, message: Message) -\u003e Result\u003c(), AgentMailError\u003e {\n        self.call_mcp(\"agent_mail/send\", json!({\n            \"to\": recipient,\n            \"message\": message,\n            \"from\": self.agent_name\n        })).await?;\n        \n        Ok(())\n    }\n    \n    /// Check inbox for new messages\n    pub async fn check_inbox(\u0026self) -\u003e Result\u003cVec\u003cInboxMessage\u003e, AgentMailError\u003e {\n        let response = self.call_mcp(\"agent_mail/inbox\", json!({\n            \"agent\": self.agent_name,\n            \"limit\": 50\n        })).await?;\n        \n        let messages: Vec\u003cInboxMessage\u003e = serde_json::from_value(response)?;\n        Ok(messages)\n    }\n    \n    /// Acknowledge message receipt\n    pub async fn acknowledge(\u0026self, message_id: \u0026str) -\u003e Result\u003c(), AgentMailError\u003e {\n        self.call_mcp(\"agent_mail/ack\", json!({\n            \"message_id\": message_id,\n            \"agent\": self.agent_name\n        })).await?;\n        \n        Ok(())\n    }\n    \n    async fn call_mcp(\u0026self, method: \u0026str, params: serde_json::Value) -\u003e Result\u003cserde_json::Value, AgentMailError\u003e {\n        // MCP call implementation\n        let client = McpClient::connect(\u0026self.mcp_endpoint).await?;\n        let response = client.call(method, params).await?;\n        Ok(response)\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Message {\n    pub id: String,\n    pub message_type: MessageType,\n    pub content: MessageContent,\n    pub metadata: MessageMetadata,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, Hash, Eq, PartialEq)]\npub enum MessageType {\n    SkillShared,\n    PatternDiscovered,\n    SkillRequest,\n    SkillRequestResponse,\n    GenerationStarted,\n    GenerationCompleted,\n    Ping,\n    Pong,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum MessageContent {\n    Skill(SharedSkill),\n    Pattern(SharedPattern),\n    Request(SkillRequest),\n    Response(SkillRequestResponse),\n    Generation(GenerationNotification),\n    Status(AgentStatus),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MessageMetadata {\n    pub sender: String,\n    pub timestamp: DateTime\u003cUtc\u003e,\n    pub priority: Priority,\n    pub ttl_seconds: Option\u003cu64\u003e,\n    pub correlation_id: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct InboxMessage {\n    pub message: Message,\n    pub received_at: DateTime\u003cUtc\u003e,\n    pub read: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Subscription {\n    pub topic: String,\n    pub subscribed_at: DateTime\u003cUtc\u003e,\n    pub message_count: u64,\n}\n```\n\n### Skill Request System\n\n```rust\n/// A request for a skill on a topic (bounty system)\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillRequestBounty {\n    /// Unique request identifier\n    pub id: String,\n    \n    /// Topic being requested\n    pub topic: String,\n    \n    /// Detailed description of what's needed\n    pub description: String,\n    \n    /// Urgency level\n    pub urgency: SkillRequestUrgency,\n    \n    /// Context about why this skill is needed\n    pub context: RequestContext,\n    \n    /// Who created the request\n    pub requester: String,\n    \n    /// When the request was created\n    pub created_at: DateTime\u003cUtc\u003e,\n    \n    /// When the request expires\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// Current status\n    pub status: RequestStatus,\n    \n    /// Responses received\n    pub responses: Vec\u003cRequestResponse\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SkillRequestUrgency {\n    /// Nice to have, no rush\n    Low,\n    \n    /// Would help current work\n    Medium,\n    \n    /// Blocking current work\n    High,\n    \n    /// Critical blocker\n    Critical,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RequestContext {\n    /// What the requester is trying to accomplish\n    pub goal: String,\n    \n    /// Technologies involved\n    pub technologies: Vec\u003cString\u003e,\n    \n    /// Specific aspects needed\n    pub aspects: Vec\u003cString\u003e,\n    \n    /// Example use cases\n    pub use_cases: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RequestStatus {\n    /// Request is open\n    Open,\n    \n    /// Someone is working on it\n    InProgress { assignee: String },\n    \n    /// Skill has been provided\n    Fulfilled { skill_id: SkillId },\n    \n    /// Request expired\n    Expired,\n    \n    /// Request cancelled\n    Cancelled,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RequestResponse {\n    /// Who responded\n    pub responder: String,\n    \n    /// Response type\n    pub response_type: ResponseType,\n    \n    /// When response was sent\n    pub responded_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ResponseType {\n    /// Will work on this\n    WillFulfill { eta: Option\u003cDateTime\u003cUtc\u003e\u003e },\n    \n    /// Have an existing skill that might help\n    ExistingSkill { skill_id: SkillId, relevance: f64 },\n    \n    /// Created new skill\n    NewSkill { skill: SharedSkill },\n    \n    /// Can't help\n    CannotFulfill { reason: String },\n}\n\nimpl SkillRequestBounty {\n    /// Create a new skill request\n    pub fn new(topic: \u0026str, description: \u0026str, urgency: SkillRequestUrgency) -\u003e Self {\n        Self {\n            id: Uuid::new_v4().to_string(),\n            topic: topic.to_string(),\n            description: description.to_string(),\n            urgency,\n            context: RequestContext {\n                goal: String::new(),\n                technologies: Vec::new(),\n                aspects: Vec::new(),\n                use_cases: Vec::new(),\n            },\n            requester: String::new(),\n            created_at: Utc::now(),\n            expires_at: None,\n            status: RequestStatus::Open,\n            responses: Vec::new(),\n        }\n    }\n    \n    /// Set context for the request\n    pub fn with_context(mut self, context: RequestContext) -\u003e Self {\n        self.context = context;\n        self\n    }\n    \n    /// Set expiration\n    pub fn expires_in(mut self, duration: chrono::Duration) -\u003e Self {\n        self.expires_at = Some(Utc::now() + duration);\n        self\n    }\n}\n```\n\n### Pattern Sharing\n\n```rust\n/// Shares discovered patterns with other agents\npub struct PatternSharer {\n    /// Agent mail client\n    mail_client: AgentMailClient,\n    \n    /// Local patterns pending share\n    local_patterns: Vec\u003cExtractedPattern\u003e,\n    \n    /// Patterns received from others\n    received_patterns: Vec\u003cSharedPattern\u003e,\n    \n    /// Sharing policy\n    policy: SharingPolicy,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SharedPattern {\n    /// Unique pattern identifier\n    pub id: String,\n    \n    /// The pattern itself\n    pub pattern: ExtractedPattern,\n    \n    /// Who discovered it\n    pub discovered_by: String,\n    \n    /// Projects where it was observed\n    pub source_projects: Vec\u003cString\u003e,\n    \n    /// How many times it's been observed\n    pub observation_count: u32,\n    \n    /// Confidence score\n    pub confidence: f64,\n    \n    /// When it was shared\n    pub shared_at: DateTime\u003cUtc\u003e,\n    \n    /// How many agents have used it\n    pub adoption_count: u32,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SharingPolicy {\n    /// Minimum confidence to share\n    pub min_confidence: f64,\n    \n    /// Minimum observations before sharing\n    pub min_observations: u32,\n    \n    /// Whether to auto-share new patterns\n    pub auto_share: bool,\n    \n    /// Topics to share (empty = all)\n    pub share_topics: Vec\u003cString\u003e,\n    \n    /// Topics to exclude from sharing\n    pub exclude_topics: Vec\u003cString\u003e,\n}\n\nimpl PatternSharer {\n    pub fn new(mail_client: AgentMailClient) -\u003e Self {\n        Self {\n            mail_client,\n            local_patterns: Vec::new(),\n            received_patterns: Vec::new(),\n            policy: SharingPolicy::default(),\n        }\n    }\n    \n    /// Share a pattern with other agents\n    pub async fn share_pattern(\u0026mut self, pattern: ExtractedPattern) -\u003e Result\u003c(), AgentMailError\u003e {\n        // Check policy\n        if pattern.confidence \u003c self.policy.min_confidence {\n            return Ok(()); // Don't share low-confidence patterns\n        }\n        \n        let shared = SharedPattern {\n            id: Uuid::new_v4().to_string(),\n            pattern: pattern.clone(),\n            discovered_by: self.mail_client.agent_name.clone(),\n            source_projects: vec![self.mail_client.project_key.clone()],\n            observation_count: 1,\n            confidence: pattern.confidence,\n            shared_at: Utc::now(),\n            adoption_count: 0,\n        };\n        \n        let message = Message {\n            id: Uuid::new_v4().to_string(),\n            message_type: MessageType::PatternDiscovered,\n            content: MessageContent::Pattern(shared),\n            metadata: MessageMetadata {\n                sender: self.mail_client.agent_name.clone(),\n                timestamp: Utc::now(),\n                priority: Priority::Normal,\n                ttl_seconds: Some(86400 * 7), // 7 days\n                correlation_id: None,\n            },\n        };\n        \n        self.mail_client.publish(\"patterns/discovered\", message).await?;\n        \n        Ok(())\n    }\n    \n    /// Process received pattern\n    pub fn receive_pattern(\u0026mut self, pattern: SharedPattern) {\n        // Check if we already have this pattern\n        let existing = self.received_patterns.iter_mut()\n            .find(|p| p.pattern.signature() == pattern.pattern.signature());\n        \n        if let Some(existing) = existing {\n            // Merge observations\n            existing.observation_count += pattern.observation_count;\n            existing.confidence = (existing.confidence + pattern.confidence) / 2.0;\n            for project in pattern.source_projects {\n                if !existing.source_projects.contains(\u0026project) {\n                    existing.source_projects.push(project);\n                }\n            }\n        } else {\n            self.received_patterns.push(pattern);\n        }\n    }\n    \n    /// Get patterns relevant to a topic\n    pub fn get_relevant_patterns(\u0026self, topic: \u0026str) -\u003e Vec\u003c\u0026SharedPattern\u003e {\n        self.received_patterns\n            .iter()\n            .filter(|p| p.pattern.relates_to(topic))\n            .collect()\n    }\n}\n\nimpl Default for SharingPolicy {\n    fn default() -\u003e Self {\n        Self {\n            min_confidence: 0.7,\n            min_observations: 3,\n            auto_share: true,\n            share_topics: Vec::new(),\n            exclude_topics: Vec::new(),\n        }\n    }\n}\n```\n\n### Skill Sharing\n\n```rust\n/// Shared skill representation (for transmission)\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SharedSkill {\n    /// Skill identifier\n    pub id: SkillId,\n    \n    /// Skill name\n    pub name: String,\n    \n    /// Brief description\n    pub description: String,\n    \n    /// Skill content (serialized)\n    pub content: String,\n    \n    /// Format version\n    pub format_version: String,\n    \n    /// Who created/shared it\n    pub shared_by: String,\n    \n    /// When it was shared\n    pub shared_at: DateTime\u003cUtc\u003e,\n    \n    /// Effectiveness score if known\n    pub effectiveness_score: Option\u003cf64\u003e,\n    \n    /// Topics covered\n    pub topics: Vec\u003cString\u003e,\n    \n    /// Checksum for integrity\n    pub checksum: String,\n}\n\nimpl SharedSkill {\n    /// Create from a local skill\n    pub fn from_skill(skill: \u0026Skill, sharer: \u0026str) -\u003e Self {\n        let content = serde_json::to_string(skill).unwrap_or_default();\n        let checksum = Self::compute_checksum(\u0026content);\n        \n        Self {\n            id: skill.id.clone(),\n            name: skill.name.clone(),\n            description: skill.description.clone(),\n            content,\n            format_version: \"1.0\".to_string(),\n            shared_by: sharer.to_string(),\n            shared_at: Utc::now(),\n            effectiveness_score: skill.effectiveness_score,\n            topics: skill.topics.clone(),\n            checksum,\n        }\n    }\n    \n    /// Convert back to a Skill\n    pub fn to_skill(\u0026self) -\u003e Result\u003cSkill, serde_json::Error\u003e {\n        // Verify checksum\n        let computed = Self::compute_checksum(\u0026self.content);\n        if computed != self.checksum {\n            // Log warning but continue (could be version difference)\n            tracing::warn!(\"Checksum mismatch for shared skill {}\", self.id.0);\n        }\n        \n        serde_json::from_str(\u0026self.content)\n    }\n    \n    fn compute_checksum(content: \u0026str) -\u003e String {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        hasher.update(content.as_bytes());\n        format!(\"{:x}\", hasher.finalize())\n    }\n}\n\n/// Skill sharing coordinator\npub struct SkillShareCoordinator {\n    mail_client: AgentMailClient,\n    skill_registry: Arc\u003cSkillRegistry\u003e,\n    pending_shares: Vec\u003cSharedSkill\u003e,\n}\n\nimpl SkillShareCoordinator {\n    /// Share a skill with other agents\n    pub async fn share_skill(\u0026mut self, skill: \u0026Skill) -\u003e Result\u003c(), AgentMailError\u003e {\n        let shared = SharedSkill::from_skill(skill, \u0026self.mail_client.agent_name);\n        \n        let message = Message {\n            id: Uuid::new_v4().to_string(),\n            message_type: MessageType::SkillShared,\n            content: MessageContent::Skill(shared),\n            metadata: MessageMetadata {\n                sender: self.mail_client.agent_name.clone(),\n                timestamp: Utc::now(),\n                priority: Priority::Normal,\n                ttl_seconds: None, // Persistent\n                correlation_id: None,\n            },\n        };\n        \n        self.mail_client.publish(\"skills/new\", message).await?;\n        \n        Ok(())\n    }\n    \n    /// Import a received skill\n    pub fn import_skill(\u0026self, shared: \u0026SharedSkill) -\u003e Result\u003c(), ShareError\u003e {\n        let skill = shared.to_skill()?;\n        \n        // Check if we already have this skill\n        if self.skill_registry.exists(\u0026skill.id)? {\n            // Merge or skip based on effectiveness\n            let existing = self.skill_registry.get(\u0026skill.id)?;\n            if let (Some(new_score), Some(old_score)) = (skill.effectiveness_score, existing.effectiveness_score) {\n                if new_score \u003c= old_score {\n                    return Ok(()); // Keep existing, it's better\n                }\n            }\n        }\n        \n        // Import the skill\n        self.skill_registry.save(\u0026skill)?;\n        \n        tracing::info!(\"Imported skill {} from {}\", skill.name, shared.shared_by);\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## Reservation-Aware Editing\n\nWhen Agent Mail is unavailable, use a local reservation mechanism with compatible semantics to prevent conflicts during skill editing.\n\n```rust\n/// Local reservation mechanism (fallback when Agent Mail unavailable)\npub struct LocalReservationManager {\n    /// Path to reservation lock file\n    lock_dir: PathBuf,\n    \n    /// Active reservations\n    reservations: HashMap\u003cSkillId, Reservation\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Reservation {\n    /// Skill being reserved\n    pub skill_id: SkillId,\n    \n    /// Who holds the reservation\n    pub holder: String,\n    \n    /// When reservation was acquired\n    pub acquired_at: DateTime\u003cUtc\u003e,\n    \n    /// When reservation expires\n    pub expires_at: DateTime\u003cUtc\u003e,\n    \n    /// Purpose of reservation\n    pub purpose: ReservationPurpose,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ReservationPurpose {\n    Editing,\n    Generation,\n    Sync,\n}\n\nimpl LocalReservationManager {\n    /// Try to acquire a reservation\n    pub fn acquire(\u0026mut self, skill_id: \u0026SkillId, purpose: ReservationPurpose) -\u003e Result\u003cReservation, ReservationError\u003e {\n        // Check for existing reservation\n        if let Some(existing) = self.reservations.get(skill_id) {\n            if existing.expires_at \u003e Utc::now() {\n                return Err(ReservationError::AlreadyReserved {\n                    skill_id: skill_id.clone(),\n                    holder: existing.holder.clone(),\n                    expires_at: existing.expires_at,\n                });\n            }\n        }\n        \n        let reservation = Reservation {\n            skill_id: skill_id.clone(),\n            holder: self.get_local_identity(),\n            acquired_at: Utc::now(),\n            expires_at: Utc::now() + chrono::Duration::minutes(30),\n            purpose,\n        };\n        \n        // Write lock file\n        self.write_lock_file(\u0026reservation)?;\n        \n        self.reservations.insert(skill_id.clone(), reservation.clone());\n        \n        Ok(reservation)\n    }\n    \n    /// Release a reservation\n    pub fn release(\u0026mut self, skill_id: \u0026SkillId) -\u003e Result\u003c(), ReservationError\u003e {\n        if let Some(reservation) = self.reservations.remove(skill_id) {\n            if reservation.holder == self.get_local_identity() {\n                self.remove_lock_file(skill_id)?;\n            }\n        }\n        Ok(())\n    }\n    \n    /// Extend reservation\n    pub fn extend(\u0026mut self, skill_id: \u0026SkillId, duration: chrono::Duration) -\u003e Result\u003c(), ReservationError\u003e {\n        if let Some(reservation) = self.reservations.get_mut(skill_id) {\n            if reservation.holder != self.get_local_identity() {\n                return Err(ReservationError::NotOwner);\n            }\n            reservation.expires_at = Utc::now() + duration;\n            self.write_lock_file(reservation)?;\n        }\n        Ok(())\n    }\n    \n    fn write_lock_file(\u0026self, reservation: \u0026Reservation) -\u003e Result\u003c(), ReservationError\u003e {\n        let lock_path = self.lock_dir.join(format!(\"{}.lock\", reservation.skill_id.0));\n        let content = serde_json::to_string_pretty(reservation)?;\n        std::fs::write(\u0026lock_path, content)?;\n        Ok(())\n    }\n    \n    fn remove_lock_file(\u0026self, skill_id: \u0026SkillId) -\u003e Result\u003c(), ReservationError\u003e {\n        let lock_path = self.lock_dir.join(format!(\"{}.lock\", skill_id.0));\n        if lock_path.exists() {\n            std::fs::remove_file(\u0026lock_path)?;\n        }\n        Ok(())\n    }\n    \n    fn get_local_identity(\u0026self) -\u003e String {\n        hostname::get()\n            .map(|h| h.to_string_lossy().to_string())\n            .unwrap_or_else(|_| \"unknown\".to_string())\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ReservationError {\n    #[error(\"Skill {skill_id:?} already reserved by {holder} until {expires_at}\")]\n    AlreadyReserved {\n        skill_id: SkillId,\n        holder: String,\n        expires_at: DateTime\u003cUtc\u003e,\n    },\n    \n    #[error(\"Not the owner of this reservation\")]\n    NotOwner,\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms inbox`\n\n```\nCheck Agent Mail inbox\n\nUSAGE:\n    ms inbox [OPTIONS]\n\nOPTIONS:\n    --unread            Only show unread messages\n    --type \u003cTYPE\u003e       Filter by message type: skill, pattern, request\n    --since \u003cDATE\u003e      Messages since date\n    --limit \u003cN\u003e         Maximum messages to show [default: 20]\n    --ack \u003cID\u003e          Acknowledge a message\n    --ack-all           Acknowledge all messages\n\nOUTPUT EXAMPLE:\n    Inbox (5 unread, 23 total)\n    ==========================\n    \n    [NEW] skill-request from proj-analytics-agent (2 hours ago)\n          Topic: \"Time Series Analysis in Rust\"\n          Urgency: High\n          \n    [NEW] pattern from data-pipeline-agent (5 hours ago)\n          Pattern: \"Streaming CSV Processing\"\n          Confidence: 0.89\n          \n    [NEW] skill-shared from ml-team-agent (1 day ago)\n          Skill: \"python-pandas-optimization\"\n          Effectiveness: 0.85\n          \n    Actions:\n      ms inbox --ack msg-123    Acknowledge message\n      ms request respond 456    Respond to request\n```\n\n### `ms request`\n\n```\nRequest skills from other agents\n\nUSAGE:\n    ms request \u003cSUBCOMMAND\u003e\n\nSUBCOMMANDS:\n    create \u003cTOPIC\u003e      Create a new skill request\n    list                List open requests\n    respond \u003cID\u003e        Respond to a request\n    cancel \u003cID\u003e         Cancel your request\n    status \u003cID\u003e         Check request status\n\nEXAMPLES:\n    # Create a request\n    ms request create \"Kubernetes StatefulSets\" \\\n        --description \"Need guidance on StatefulSet patterns for databases\" \\\n        --urgency high \\\n        --technologies kubernetes,databases \\\n        --expires 7d\n    \n    # List open requests\n    ms request list --topic kubernetes\n    \n    # Respond to a request\n    ms request respond req-456 --skill k8s-statefulsets\n    ms request respond req-456 --will-create --eta \"2 hours\"\n\nOUTPUT (create):\n    Created skill request: req-789\n    Topic: Kubernetes StatefulSets\n    Urgency: High\n    Expires: 2024-01-22\n    \n    Published to: skills/requests\n    Subscribed agents: 12\n```\n\n### `ms subscribe`\n\n```\nSubscribe to skill topics\n\nUSAGE:\n    ms subscribe \u003cTOPIC\u003e\n    ms subscribe --list\n    ms subscribe --unsubscribe \u003cTOPIC\u003e\n\nTOPICS:\n    skills/new              All new skills\n    skills/\u003clanguage\u003e       Skills for a language\n    patterns/discovered     New patterns\n    projects/\u003ckey\u003e/skills   Skills for a project\n\nEXAMPLES:\n    ms subscribe skills/rust\n    ms subscribe patterns/discovered\n    ms subscribe projects/my-team/skills\n    \n    ms subscribe --list\n    \nOUTPUT (list):\n    Active Subscriptions\n    ====================\n    \n    skills/new              (subscribed 30 days ago, 45 messages)\n    skills/rust             (subscribed 7 days ago, 12 messages)\n    patterns/discovered     (subscribed 30 days ago, 23 messages)\n```\n\n### `ms build --broadcast-start`\n\n```\nBroadcast skill generation start to avoid duplicate work\n\nUSAGE:\n    ms build \u003cTOPIC\u003e --broadcast-start [OPTIONS]\n\nOPTIONS:\n    --broadcast-start       Announce generation start\n    --broadcast-complete    Announce generation complete (with skill)\n    --check-in-progress     Check if someone is already building this\n\nBEHAVIOR:\n    When --broadcast-start is used:\n    1. Publishes \"generation-started\" message to skills/generation topic\n    2. Other agents see this and can skip generating the same skill\n    3. On completion, publishes \"generation-completed\" with the skill\n    4. Other agents can import the skill instead of building\n\nEXAMPLE:\n    $ ms build \"Rust async patterns\" --broadcast-start\n    \n    Broadcasting generation start...\n    Checking for in-progress generation... none found\n    \n    Generating skill: rust-async-patterns\n    [=========\u003e          ] 45%\n    \n    Generation complete!\n    Broadcasting completion with skill...\n    \n    Skill published to: skills/new\n    Agents notified: 8\n```\n\n---\n\n## Connection Management\n\n```rust\n/// Manages Agent Mail connection lifecycle\npub struct AgentMailManager {\n    client: AgentMailClient,\n    config: AgentMailConfig,\n    reconnect_task: Option\u003ctokio::task::JoinHandle\u003c()\u003e\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AgentMailConfig {\n    /// MCP endpoint URL\n    pub endpoint: String,\n    \n    /// Project key\n    pub project_key: String,\n    \n    /// Agent name\n    pub agent_name: String,\n    \n    /// Auto-reconnect on disconnect\n    pub auto_reconnect: bool,\n    \n    /// Reconnect interval (seconds)\n    pub reconnect_interval_secs: u64,\n    \n    /// Maximum reconnect attempts\n    pub max_reconnect_attempts: u32,\n    \n    /// Whether to use local fallback\n    pub use_local_fallback: bool,\n}\n\nimpl AgentMailManager {\n    /// Initialize from configuration\n    pub fn from_config(config: AgentMailConfig) -\u003e Self {\n        let client = AgentMailClient::new(\n            \u0026config.project_key,\n            \u0026config.agent_name,\n            \u0026config.endpoint,\n        );\n        \n        Self {\n            client,\n            config,\n            reconnect_task: None,\n        }\n    }\n    \n    /// Start connection with auto-reconnect\n    pub async fn start(\u0026mut self) -\u003e Result\u003c(), AgentMailError\u003e {\n        match self.client.connect().await {\n            Ok(()) =\u003e {\n                tracing::info!(\"Connected to Agent Mail\");\n                Ok(())\n            }\n            Err(e) if self.config.use_local_fallback =\u003e {\n                tracing::warn!(\"Agent Mail unavailable, using local fallback: {}\", e);\n                Ok(())\n            }\n            Err(e) =\u003e Err(e),\n        }\n    }\n    \n    /// Check if connected (or using fallback)\n    pub fn is_available(\u0026self) -\u003e bool {\n        matches!(self.client.state, ConnectionState::Connected { .. })\n            || self.config.use_local_fallback\n    }\n    \n    /// Get client reference\n    pub fn client(\u0026self) -\u003e \u0026AgentMailClient {\n        \u0026self.client\n    }\n}\n\nimpl Default for AgentMailConfig {\n    fn default() -\u003e Self {\n        Self {\n            endpoint: \"http://localhost:3000/mcp\".to_string(),\n            project_key: \"default\".to_string(),\n            agent_name: hostname::get()\n                .map(|h| h.to_string_lossy().to_string())\n                .unwrap_or_else(|_| \"unknown-agent\".to_string()),\n            auto_reconnect: true,\n            reconnect_interval_secs: 30,\n            max_reconnect_attempts: 10,\n            use_local_fallback: true,\n        }\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum AgentMailError {\n    #[error(\"Not connected to Agent Mail\")]\n    NotConnected,\n    \n    #[error(\"Connection failed: {0}\")]\n    ConnectionFailed(String),\n    \n    #[error(\"MCP error: {0}\")]\n    McpError(String),\n    \n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n    \n    #[error(\"Message expired\")]\n    MessageExpired,\n    \n    #[error(\"Topic not found: {0}\")]\n    TopicNotFound(String),\n    \n    #[error(\"Permission denied: {0}\")]\n    PermissionDenied(String),\n    \n    #[error(\"Rate limited\")]\n    RateLimited,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ShareError {\n    #[error(\"Skill parsing error: {0}\")]\n    ParseError(#[from] serde_json::Error),\n    \n    #[error(\"Registry error: {0}\")]\n    RegistryError(String),\n    \n    #[error(\"Checksum mismatch\")]\n    ChecksumMismatch,\n}\n```\n\n---\n\n## Configuration\n\nConfiguration in `~/.config/meta_skill/agent_mail.toml`:\n\n```toml\n[connection]\nendpoint = \"http://agent-mail.internal:3000/mcp\"\nproject_key = \"my-project\"\nagent_name = \"dev-laptop\"\nauto_reconnect = true\nreconnect_interval_secs = 30\nmax_reconnect_attempts = 10\nuse_local_fallback = true\n\n[sharing]\nauto_share_skills = false\nauto_share_patterns = true\nmin_pattern_confidence = 0.7\nmin_pattern_observations = 3\n\n[subscriptions]\ndefault_topics = [\n    \"skills/new\",\n    \"skills/rust\",\n    \"patterns/discovered\"\n]\n\n[notifications]\non_skill_request = true\non_pattern_discovered = true\non_skill_shared = false  # Can be noisy\n```\n\n---\n\n## Dependencies\n\n- **MCP Server Mode** (meta_skill-ugf): MCP protocol support for Agent Mail communication\n- `tokio`: Async runtime\n- `serde`, `serde_json`: Message serialization\n- `chrono`: Timestamps\n- `uuid`: Message IDs\n- `sha2`: Checksums","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-13T23:00:03.594409474-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:07.69132757-05:00","labels":["agent-mail","coordination","phase-6"],"dependencies":[{"issue_id":"meta_skill-tzu","depends_on_id":"meta_skill-ugf","type":"blocks","created_at":"2026-01-13T23:04:15.23692171-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-tzu","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T23:43:23.919324587-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ugf","title":"[P6] MCP Server Mode","description":"# MCP Server Mode\n\n## Overview\n\nExpose ms as an MCP server so agents can call it as a tool instead of shelling out. This provides streaming responses, caching, and a stable API contract.\n\n---\n\n## Tasks\n\n1. Implement MCP server loop (stdio + optional TCP).\n2. Define tools: search, suggest, load, evidence, build status, pack.\n3. Maintain hot caches (SQLite, Tantivy, skillpack).\n4. Add auth + rate limits (local policy).\n\n---\n\n## Testing Requirements\n\n- Integration tests for tool schema compliance.\n- E2E tests with real MCP client.\n- Concurrency tests for parallel calls.\n\n---\n\n## Acceptance Criteria\n\n- MCP tool responses match JSON schema.\n- Hot caches reduce p99 latency.\n- Server handles concurrent requests safely.\n\n---\n\n## Dependencies\n\n- `meta_skill-0ki` ms search\n- `meta_skill-7va` ms load\n- `meta_skill-q3l` Doctor Command","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:22.510815207-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:09:05.300677486-05:00","labels":["integration","mcp","phase-6"],"dependencies":[{"issue_id":"meta_skill-ugf","depends_on_id":"meta_skill-0ki","type":"blocks","created_at":"2026-01-13T22:28:36.950862753-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ugf","depends_on_id":"meta_skill-7va","type":"blocks","created_at":"2026-01-13T22:28:36.978593985-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ugf","depends_on_id":"meta_skill-q3l","type":"blocks","created_at":"2026-01-14T00:09:34.814029458-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ujr","title":"Multi-Machine Synchronization","description":"# Multi-Machine Synchronization\n\n**Phase 5 - Section 8.5**\n\nSynchronize skills across multiple development machines. This feature enables developers to maintain consistent skill libraries across workstations, laptops, and cloud environments with robust conflict resolution.\n\n---\n\n## Overview\n\nMulti-machine sync solves the problem of skill fragmentation when developers work across multiple environments. Skills created on a laptop should be available on a workstation, and vice versa. The system tracks machine identity, maintains sync state, and resolves conflicts when the same skill is modified on different machines.\n\n---\n\n## Core Data Structures\n\n### Machine Identity\n\n```rust\nuse std::collections::HashMap;\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\n\n/// Unique identity for a development machine participating in sync\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MachineIdentity {\n    /// Globally unique machine identifier (generated on first sync setup)\n    pub machine_id: String,\n    \n    /// Human-readable machine name (e.g., \"work-laptop\", \"home-desktop\")\n    pub machine_name: String,\n    \n    /// Last sync timestamp per remote (remote_url -\u003e last_sync_time)\n    pub sync_timestamps: HashMap\u003cString, DateTime\u003cUtc\u003e\u003e,\n    \n    /// Machine-specific metadata\n    pub metadata: MachineMetadata,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MachineMetadata {\n    /// Operating system (linux, macos, windows)\n    pub os: String,\n    \n    /// Hostname at registration time\n    pub hostname: String,\n    \n    /// When this machine was first registered\n    pub registered_at: DateTime\u003cUtc\u003e,\n    \n    /// Optional user-provided description\n    pub description: Option\u003cString\u003e,\n}\n\nimpl MachineIdentity {\n    /// Generate a new machine identity\n    pub fn generate(machine_name: String) -\u003e Self {\n        Self {\n            machine_id: Uuid::new_v4().to_string(),\n            machine_name,\n            sync_timestamps: HashMap::new(),\n            metadata: MachineMetadata {\n                os: std::env::consts::OS.to_string(),\n                hostname: hostname::get()\n                    .map(|h| h.to_string_lossy().to_string())\n                    .unwrap_or_else(|_| \"unknown\".to_string()),\n                registered_at: Utc::now(),\n                description: None,\n            },\n        }\n    }\n    \n    /// Update sync timestamp for a remote\n    pub fn record_sync(\u0026mut self, remote_url: \u0026str) {\n        self.sync_timestamps.insert(remote_url.to_string(), Utc::now());\n    }\n    \n    /// Get last sync time for a remote\n    pub fn last_sync(\u0026self, remote_url: \u0026str) -\u003e Option\u003cDateTime\u003cUtc\u003e\u003e {\n        self.sync_timestamps.get(remote_url).copied()\n    }\n    \n    /// Path to machine identity file\n    pub fn identity_path() -\u003e PathBuf {\n        dirs::data_local_dir()\n            .unwrap_or_else(|| PathBuf::from(\".\"))\n            .join(\"meta_skill\")\n            .join(\"machine_identity.json\")\n    }\n    \n    /// Load or generate machine identity\n    pub fn load_or_generate() -\u003e Result\u003cSelf, SyncError\u003e {\n        let path = Self::identity_path();\n        if path.exists() {\n            let content = std::fs::read_to_string(\u0026path)?;\n            Ok(serde_json::from_str(\u0026content)?)\n        } else {\n            let identity = Self::generate(Self::default_machine_name());\n            identity.save()?;\n            Ok(identity)\n        }\n    }\n    \n    fn default_machine_name() -\u003e String {\n        hostname::get()\n            .map(|h| h.to_string_lossy().to_string())\n            .unwrap_or_else(|_| \"default-machine\".to_string())\n    }\n    \n    pub fn save(\u0026self) -\u003e Result\u003c(), SyncError\u003e {\n        let path = Self::identity_path();\n        if let Some(parent) = path.parent() {\n            std::fs::create_dir_all(parent)?;\n        }\n        let content = serde_json::to_string_pretty(self)?;\n        std::fs::write(\u0026path, content)?;\n        Ok(())\n    }\n}\n```\n\n### Sync State\n\n```rust\nuse std::collections::HashMap;\n\n/// Per-skill sync state\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSyncState {\n    /// Skill identifier\n    pub skill_id: SkillId,\n    \n    /// Local version hash (content-addressable)\n    pub local_hash: String,\n    \n    /// Known remote hashes (remote_url -\u003e hash)\n    pub remote_hashes: HashMap\u003cString, String\u003e,\n    \n    /// Sync status\n    pub status: SkillSyncStatus,\n    \n    /// Last modification time locally\n    pub local_modified: DateTime\u003cUtc\u003e,\n    \n    /// Machine that last modified this skill\n    pub last_modified_by: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum SkillSyncStatus {\n    /// Local matches all remotes\n    Synced,\n    \n    /// Local has changes not pushed\n    LocalAhead,\n    \n    /// Remote has changes not pulled\n    RemoteAhead,\n    \n    /// Both local and remote have diverged\n    Diverged,\n    \n    /// Skill only exists locally\n    LocalOnly,\n    \n    /// Skill only exists on remote\n    RemoteOnly,\n    \n    /// Conflict detected and unresolved\n    Conflict(ConflictInfo),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub struct ConflictInfo {\n    /// Machine IDs involved in conflict\n    pub machines: Vec\u003cString\u003e,\n    \n    /// When conflict was detected\n    pub detected_at: DateTime\u003cUtc\u003e,\n    \n    /// Brief description of conflict\n    pub description: String,\n}\n\n/// Global sync state for this machine\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SyncState {\n    /// Per-skill sync states\n    pub skill_states: HashMap\u003cSkillId, SkillSyncState\u003e,\n    \n    /// Configured remotes\n    pub remotes: Vec\u003cRemoteConfig\u003e,\n    \n    /// Last time a full sync was performed\n    pub last_full_sync: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// This machine's identity\n    pub machine: MachineIdentity,\n    \n    /// Pending operations (for resume after interruption)\n    pub pending_ops: Vec\u003cPendingOperation\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum PendingOperation {\n    Push { skill_id: SkillId, remote: String },\n    Pull { skill_id: SkillId, remote: String },\n    Resolve { skill_id: SkillId, strategy: ConflictStrategy },\n}\n\nimpl SyncState {\n    pub fn new(machine: MachineIdentity) -\u003e Self {\n        Self {\n            skill_states: HashMap::new(),\n            remotes: Vec::new(),\n            last_full_sync: None,\n            machine,\n            pending_ops: Vec::new(),\n        }\n    }\n    \n    /// Get skills that need pushing\n    pub fn needs_push(\u0026self) -\u003e Vec\u003c\u0026SkillSyncState\u003e {\n        self.skill_states\n            .values()\n            .filter(|s| matches!(s.status, SkillSyncStatus::LocalAhead | SkillSyncStatus::LocalOnly))\n            .collect()\n    }\n    \n    /// Get skills that need pulling\n    pub fn needs_pull(\u0026self) -\u003e Vec\u003c\u0026SkillSyncState\u003e {\n        self.skill_states\n            .values()\n            .filter(|s| matches!(s.status, SkillSyncStatus::RemoteAhead | SkillSyncStatus::RemoteOnly))\n            .collect()\n    }\n    \n    /// Get skills with conflicts\n    pub fn conflicts(\u0026self) -\u003e Vec\u003c\u0026SkillSyncState\u003e {\n        self.skill_states\n            .values()\n            .filter(|s| matches!(s.status, SkillSyncStatus::Conflict(_) | SkillSyncStatus::Diverged))\n            .collect()\n    }\n    \n    /// Calculate content hash for a skill\n    pub fn hash_skill(skill: \u0026Skill) -\u003e String {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        \n        // Hash skill content deterministically\n        hasher.update(skill.name.as_bytes());\n        hasher.update(skill.description.as_bytes());\n        \n        // Sort sections for deterministic hashing\n        let mut sections: Vec\u003c_\u003e = skill.sections.iter().collect();\n        sections.sort_by_key(|(name, _)| *name);\n        \n        for (name, section) in sections {\n            hasher.update(name.as_bytes());\n            hasher.update(section.content.as_bytes());\n        }\n        \n        format!(\"{:x}\", hasher.finalize())\n    }\n}\n```\n\n### Remote Configuration\n\n```rust\n/// Type of remote storage\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum RemoteType {\n    /// Git repository (GitHub, GitLab, etc.)\n    Git,\n    \n    /// Amazon S3 or compatible (MinIO, DigitalOcean Spaces)\n    S3,\n    \n    /// Custom HTTP API\n    Custom,\n}\n\n/// Configuration for a sync remote\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RemoteConfig {\n    /// Unique name for this remote (e.g., \"origin\", \"backup\")\n    pub name: String,\n    \n    /// Remote type\n    pub remote_type: RemoteType,\n    \n    /// Connection URL\n    pub url: String,\n    \n    /// Authentication configuration\n    pub auth: RemoteAuth,\n    \n    /// Whether this remote is enabled\n    pub enabled: bool,\n    \n    /// Push/pull settings\n    pub settings: RemoteSettings,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RemoteAuth {\n    /// No authentication (public remote)\n    None,\n    \n    /// SSH key authentication (for Git)\n    SshKey { key_path: PathBuf },\n    \n    /// Token-based authentication\n    Token { token_env_var: String },\n    \n    /// AWS credentials (for S3)\n    AwsCredentials {\n        access_key_env: String,\n        secret_key_env: String,\n        region: String,\n    },\n    \n    /// Basic HTTP authentication\n    Basic { username: String, password_env: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RemoteSettings {\n    /// Auto-push on skill save\n    pub auto_push: bool,\n    \n    /// Auto-pull on sync check\n    pub auto_pull: bool,\n    \n    /// Sync frequency for background sync (in minutes, 0 = disabled)\n    pub sync_interval_minutes: u32,\n    \n    /// Skills to exclude from this remote (glob patterns)\n    pub exclude_patterns: Vec\u003cString\u003e,\n    \n    /// Only sync skills matching these patterns (empty = all)\n    pub include_patterns: Vec\u003cString\u003e,\n}\n\nimpl RemoteConfig {\n    /// Create a Git remote configuration\n    pub fn git(name: \u0026str, url: \u0026str) -\u003e Self {\n        Self {\n            name: name.to_string(),\n            remote_type: RemoteType::Git,\n            url: url.to_string(),\n            auth: RemoteAuth::SshKey {\n                key_path: dirs::home_dir()\n                    .unwrap_or_default()\n                    .join(\".ssh\")\n                    .join(\"id_rsa\"),\n            },\n            enabled: true,\n            settings: RemoteSettings::default(),\n        }\n    }\n    \n    /// Create an S3 remote configuration\n    pub fn s3(name: \u0026str, bucket: \u0026str, region: \u0026str) -\u003e Self {\n        Self {\n            name: name.to_string(),\n            remote_type: RemoteType::S3,\n            url: format!(\"s3://{}\", bucket),\n            auth: RemoteAuth::AwsCredentials {\n                access_key_env: \"AWS_ACCESS_KEY_ID\".to_string(),\n                secret_key_env: \"AWS_SECRET_ACCESS_KEY\".to_string(),\n                region: region.to_string(),\n            },\n            enabled: true,\n            settings: RemoteSettings::default(),\n        }\n    }\n}\n\nimpl Default for RemoteSettings {\n    fn default() -\u003e Self {\n        Self {\n            auto_push: false,\n            auto_pull: true,\n            sync_interval_minutes: 0,\n            exclude_patterns: vec![],\n            include_patterns: vec![],\n        }\n    }\n}\n```\n\n### Conflict Resolution\n\n```rust\n/// Strategy for resolving sync conflicts\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum ConflictStrategy {\n    /// Always prefer local version\n    KeepLocal,\n    \n    /// Always prefer remote version\n    KeepRemote,\n    \n    /// Prefer version from specific machine\n    PreferMachine(String),\n    \n    /// Keep most recently modified\n    LatestWins,\n    \n    /// Merge changes (section-level)\n    Merge,\n    \n    /// Create both versions with suffixes\n    Fork,\n    \n    /// Prompt user interactively\n    Manual,\n}\n\n/// Conflict resolver with configurable strategies\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ConflictResolver {\n    /// Default strategy for all skills\n    pub default_strategy: ConflictStrategy,\n    \n    /// Per-skill strategy overrides\n    pub skill_strategies: HashMap\u003cSkillId, ConflictStrategy\u003e,\n    \n    /// Machine priority for PreferMachine strategy\n    pub machine_priority: Vec\u003cString\u003e,\n}\n\nimpl ConflictResolver {\n    pub fn new(default_strategy: ConflictStrategy) -\u003e Self {\n        Self {\n            default_strategy,\n            skill_strategies: HashMap::new(),\n            machine_priority: Vec::new(),\n        }\n    }\n    \n    /// Get strategy for a specific skill\n    pub fn strategy_for(\u0026self, skill_id: \u0026SkillId) -\u003e \u0026ConflictStrategy {\n        self.skill_strategies\n            .get(skill_id)\n            .unwrap_or(\u0026self.default_strategy)\n    }\n    \n    /// Resolve a conflict between local and remote versions\n    pub fn resolve(\n        \u0026self,\n        skill_id: \u0026SkillId,\n        local: \u0026Skill,\n        remote: \u0026Skill,\n        local_machine: \u0026str,\n        remote_machine: \u0026str,\n    ) -\u003e Result\u003cConflictResolution, SyncError\u003e {\n        let strategy = self.strategy_for(skill_id);\n        \n        match strategy {\n            ConflictStrategy::KeepLocal =\u003e Ok(ConflictResolution::UseLocal),\n            \n            ConflictStrategy::KeepRemote =\u003e Ok(ConflictResolution::UseRemote),\n            \n            ConflictStrategy::PreferMachine(machine_id) =\u003e {\n                if machine_id == local_machine {\n                    Ok(ConflictResolution::UseLocal)\n                } else if machine_id == remote_machine {\n                    Ok(ConflictResolution::UseRemote)\n                } else {\n                    // Fallback to latest wins\n                    self.resolve_by_timestamp(local, remote)\n                }\n            }\n            \n            ConflictStrategy::LatestWins =\u003e self.resolve_by_timestamp(local, remote),\n            \n            ConflictStrategy::Merge =\u003e self.merge_skills(local, remote),\n            \n            ConflictStrategy::Fork =\u003e Ok(ConflictResolution::Fork {\n                local_suffix: format!(\"-{}\", local_machine),\n                remote_suffix: format!(\"-{}\", remote_machine),\n            }),\n            \n            ConflictStrategy::Manual =\u003e Ok(ConflictResolution::RequiresManual),\n        }\n    }\n    \n    fn resolve_by_timestamp(\n        \u0026self,\n        local: \u0026Skill,\n        remote: \u0026Skill,\n    ) -\u003e Result\u003cConflictResolution, SyncError\u003e {\n        if local.modified_at \u003e= remote.modified_at {\n            Ok(ConflictResolution::UseLocal)\n        } else {\n            Ok(ConflictResolution::UseRemote)\n        }\n    }\n    \n    fn merge_skills(\u0026self, local: \u0026Skill, remote: \u0026Skill) -\u003e Result\u003cConflictResolution, SyncError\u003e {\n        let mut merged_sections = HashMap::new();\n        let mut conflicts = Vec::new();\n        \n        // Collect all section names\n        let all_sections: HashSet\u003c_\u003e = local.sections.keys()\n            .chain(remote.sections.keys())\n            .collect();\n        \n        for section_name in all_sections {\n            match (local.sections.get(section_name), remote.sections.get(section_name)) {\n                (Some(l), Some(r)) if l.content != r.content =\u003e {\n                    // Section differs - record conflict\n                    conflicts.push(SectionConflict {\n                        section: section_name.clone(),\n                        local_content: l.content.clone(),\n                        remote_content: r.content.clone(),\n                    });\n                }\n                (Some(l), Some(_)) =\u003e {\n                    // Same content\n                    merged_sections.insert(section_name.clone(), l.clone());\n                }\n                (Some(l), None) =\u003e {\n                    // Only in local\n                    merged_sections.insert(section_name.clone(), l.clone());\n                }\n                (None, Some(r)) =\u003e {\n                    // Only in remote\n                    merged_sections.insert(section_name.clone(), r.clone());\n                }\n                (None, None) =\u003e unreachable!(),\n            }\n        }\n        \n        if conflicts.is_empty() {\n            Ok(ConflictResolution::Merged { sections: merged_sections })\n        } else {\n            Ok(ConflictResolution::PartialMerge {\n                merged_sections,\n                conflicts,\n            })\n        }\n    }\n}\n\n#[derive(Debug)]\npub enum ConflictResolution {\n    UseLocal,\n    UseRemote,\n    Merged { sections: HashMap\u003cString, Section\u003e },\n    PartialMerge {\n        merged_sections: HashMap\u003cString, Section\u003e,\n        conflicts: Vec\u003cSectionConflict\u003e,\n    },\n    Fork { local_suffix: String, remote_suffix: String },\n    RequiresManual,\n}\n\n#[derive(Debug)]\npub struct SectionConflict {\n    pub section: String,\n    pub local_content: String,\n    pub remote_content: String,\n}\n```\n\n---\n\n## Sync Engine\n\n```rust\n/// Main synchronization engine\npub struct SyncEngine {\n    /// Current sync state\n    state: SyncState,\n    \n    /// Conflict resolver\n    resolver: ConflictResolver,\n    \n    /// Skill registry for local operations\n    registry: Arc\u003cSkillRegistry\u003e,\n    \n    /// Remote clients\n    clients: HashMap\u003cString, Box\u003cdyn RemoteClient\u003e\u003e,\n}\n\n#[async_trait]\npub trait RemoteClient: Send + Sync {\n    /// List all skills on remote\n    async fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cRemoteSkillInfo\u003e, SyncError\u003e;\n    \n    /// Get skill content by ID\n    async fn get_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cSkill\u003e, SyncError\u003e;\n    \n    /// Push skill to remote\n    async fn push_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c(), SyncError\u003e;\n    \n    /// Delete skill from remote\n    async fn delete_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003c(), SyncError\u003e;\n    \n    /// Get skill hash without downloading content\n    async fn get_hash(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cString\u003e, SyncError\u003e;\n}\n\n#[derive(Debug)]\npub struct RemoteSkillInfo {\n    pub id: SkillId,\n    pub hash: String,\n    pub modified_at: DateTime\u003cUtc\u003e,\n    pub modified_by: String,\n}\n\nimpl SyncEngine {\n    /// Perform full sync with all remotes\n    pub async fn sync_all(\u0026mut self) -\u003e Result\u003cSyncReport, SyncError\u003e {\n        let mut report = SyncReport::new();\n        \n        for remote in \u0026self.state.remotes.clone() {\n            if !remote.enabled {\n                continue;\n            }\n            \n            match self.sync_remote(\u0026remote.name).await {\n                Ok(remote_report) =\u003e report.merge(remote_report),\n                Err(e) =\u003e report.add_error(\u0026remote.name, e),\n            }\n        }\n        \n        self.state.last_full_sync = Some(Utc::now());\n        self.save_state()?;\n        \n        Ok(report)\n    }\n    \n    /// Sync with a specific remote\n    pub async fn sync_remote(\u0026mut self, remote_name: \u0026str) -\u003e Result\u003cSyncReport, SyncError\u003e {\n        let client = self.clients.get(remote_name)\n            .ok_or_else(|| SyncError::UnknownRemote(remote_name.to_string()))?;\n        \n        let mut report = SyncReport::new();\n        \n        // Get remote skill list\n        let remote_skills = client.list_skills().await?;\n        let remote_map: HashMap\u003c_, _\u003e = remote_skills.iter()\n            .map(|s| (s.id.clone(), s))\n            .collect();\n        \n        // Get local skills\n        let local_skills = self.registry.list_all()?;\n        \n        // Process each local skill\n        for skill in \u0026local_skills {\n            let local_hash = SyncState::hash_skill(skill);\n            \n            if let Some(remote_info) = remote_map.get(\u0026skill.id) {\n                if local_hash != remote_info.hash {\n                    // Diverged - need to resolve\n                    self.handle_divergence(skill, remote_info, client.as_ref(), \u0026mut report).await?;\n                } else {\n                    report.add_synced(\u0026skill.id);\n                }\n            } else {\n                // Local only - push\n                client.push_skill(skill).await?;\n                report.add_pushed(\u0026skill.id);\n            }\n        }\n        \n        // Process remote-only skills\n        let local_ids: HashSet\u003c_\u003e = local_skills.iter().map(|s| \u0026s.id).collect();\n        for remote_info in \u0026remote_skills {\n            if !local_ids.contains(\u0026remote_info.id) {\n                // Remote only - pull\n                if let Some(skill) = client.get_skill(\u0026remote_info.id).await? {\n                    self.registry.save(\u0026skill)?;\n                    report.add_pulled(\u0026remote_info.id);\n                }\n            }\n        }\n        \n        // Update sync timestamp\n        self.state.machine.record_sync(\u0026self.get_remote_url(remote_name)?);\n        \n        Ok(report)\n    }\n    \n    async fn handle_divergence(\n        \u0026mut self,\n        local: \u0026Skill,\n        remote_info: \u0026RemoteSkillInfo,\n        client: \u0026dyn RemoteClient,\n        report: \u0026mut SyncReport,\n    ) -\u003e Result\u003c(), SyncError\u003e {\n        let remote = client.get_skill(\u0026local.id).await?\n            .ok_or_else(|| SyncError::SkillNotFound(local.id.clone()))?;\n        \n        let resolution = self.resolver.resolve(\n            \u0026local.id,\n            local,\n            \u0026remote,\n            \u0026self.state.machine.machine_id,\n            \u0026remote_info.modified_by,\n        )?;\n        \n        match resolution {\n            ConflictResolution::UseLocal =\u003e {\n                client.push_skill(local).await?;\n                report.add_pushed(\u0026local.id);\n            }\n            ConflictResolution::UseRemote =\u003e {\n                self.registry.save(\u0026remote)?;\n                report.add_pulled(\u0026local.id);\n            }\n            ConflictResolution::Merged { sections } =\u003e {\n                let mut merged = local.clone();\n                merged.sections = sections;\n                merged.modified_at = Utc::now();\n                self.registry.save(\u0026merged)?;\n                client.push_skill(\u0026merged).await?;\n                report.add_merged(\u0026local.id);\n            }\n            ConflictResolution::PartialMerge { merged_sections, conflicts } =\u003e {\n                // Save what we can, record conflicts\n                let mut partial = local.clone();\n                partial.sections = merged_sections;\n                self.registry.save(\u0026partial)?;\n                \n                for conflict in conflicts {\n                    report.add_conflict(\u0026local.id, \u0026conflict.section);\n                }\n            }\n            ConflictResolution::Fork { local_suffix, remote_suffix } =\u003e {\n                // Create forked versions\n                let mut local_fork = local.clone();\n                local_fork.id = SkillId(format!(\"{}{}\", local.id.0, local_suffix));\n                \n                let mut remote_fork = remote.clone();\n                remote_fork.id = SkillId(format!(\"{}{}\", remote.id.0, remote_suffix));\n                \n                self.registry.save(\u0026local_fork)?;\n                self.registry.save(\u0026remote_fork)?;\n                client.push_skill(\u0026local_fork).await?;\n                \n                report.add_forked(\u0026local.id, \u0026local_fork.id, \u0026remote_fork.id);\n            }\n            ConflictResolution::RequiresManual =\u003e {\n                report.add_manual_required(\u0026local.id);\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n#[derive(Debug, Default)]\npub struct SyncReport {\n    pub synced: Vec\u003cSkillId\u003e,\n    pub pushed: Vec\u003cSkillId\u003e,\n    pub pulled: Vec\u003cSkillId\u003e,\n    pub merged: Vec\u003cSkillId\u003e,\n    pub conflicts: Vec\u003c(SkillId, String)\u003e,\n    pub forked: Vec\u003c(SkillId, SkillId, SkillId)\u003e,\n    pub manual_required: Vec\u003cSkillId\u003e,\n    pub errors: Vec\u003c(String, SyncError)\u003e,\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms sync`\n\n```\nSynchronize skills with remote storage\n\nUSAGE:\n    ms sync [OPTIONS] [REMOTE]\n\nARGS:\n    [REMOTE]    Specific remote to sync (default: all enabled remotes)\n\nOPTIONS:\n    --push-only         Only push local changes, don't pull\n    --pull-only         Only pull remote changes, don't push\n    --dry-run           Show what would be synced without making changes\n    --force             Force sync even with conflicts (uses default strategy)\n    -v, --verbose       Show detailed sync progress\n\nEXAMPLES:\n    ms sync                     # Sync with all remotes\n    ms sync origin              # Sync only with 'origin' remote\n    ms sync --pull-only         # Only download new/updated skills\n    ms sync --dry-run           # Preview sync operations\n```\n\n### `ms sync --status`\n\n```\nShow synchronization status\n\nUSAGE:\n    ms sync --status [OPTIONS]\n\nOPTIONS:\n    --remote \u003cNAME\u003e     Check status for specific remote\n    --skill \u003cSKILL\u003e     Check status for specific skill\n    --conflicts         Only show skills with conflicts\n    --pending           Only show skills with pending changes\n\nOUTPUT EXAMPLE:\n    Machine: work-laptop (abc123)\n    Last full sync: 2 hours ago\n\n    Remote: origin (git@github.com:user/skills.git)\n      Status: Connected\n      Skills synced: 42\n      Local ahead: 3\n      Remote ahead: 1\n      Conflicts: 0\n\n    Pending Changes:\n      rust-error-handling    [local ahead]   Modified 5 minutes ago\n      python-testing         [local ahead]   Modified 1 hour ago\n      go-concurrency         [local ahead]   Created today\n      \n    Available from remote:\n      java-spring-boot       [remote only]   By: home-desktop\n```\n\n### `ms remote add`\n\n```\nAdd a sync remote\n\nUSAGE:\n    ms remote add \u003cNAME\u003e \u003cURL\u003e [OPTIONS]\n\nARGS:\n    \u003cNAME\u003e    Name for this remote (e.g., 'origin', 'backup')\n    \u003cURL\u003e     Remote URL\n\nOPTIONS:\n    --type \u003cTYPE\u003e       Remote type: git, s3, custom [default: auto-detect]\n    --auth \u003cMETHOD\u003e     Authentication: ssh, token, aws, basic\n    --token-env \u003cVAR\u003e   Environment variable containing auth token\n    --auto-push         Enable automatic push on skill save\n    --auto-pull         Enable automatic pull on sync check\n    --interval \u003cMIN\u003e    Background sync interval in minutes\n\nEXAMPLES:\n    ms remote add origin git@github.com:user/skills.git\n    ms remote add backup s3://my-bucket/skills --auth aws\n    ms remote add work https://skills.company.com --auth token --token-env SKILLS_TOKEN\n\nOTHER SUBCOMMANDS:\n    ms remote list              List configured remotes\n    ms remote remove \u003cNAME\u003e     Remove a remote\n    ms remote set-url \u003cNAME\u003e    Update remote URL\n    ms remote enable \u003cNAME\u003e     Enable a disabled remote\n    ms remote disable \u003cNAME\u003e    Disable a remote\n```\n\n### `ms conflicts`\n\n```\nManage sync conflicts\n\nUSAGE:\n    ms conflicts \u003cSUBCOMMAND\u003e\n\nSUBCOMMANDS:\n    list                List all unresolved conflicts\n    show \u003cSKILL\u003e        Show detailed conflict for a skill\n    resolve \u003cSKILL\u003e     Resolve conflict for a skill\n    strategy            Configure conflict resolution strategies\n\nEXAMPLES:\n    ms conflicts list\n    ms conflicts show rust-error-handling\n    ms conflicts resolve rust-error-handling --keep-local\n    ms conflicts resolve rust-error-handling --keep-remote\n    ms conflicts resolve rust-error-handling --merge\n    ms conflicts resolve rust-error-handling --manual  # Open in editor\n    \n    # Set default strategy\n    ms conflicts strategy --default latest-wins\n    \n    # Set per-skill strategy\n    ms conflicts strategy rust-error-handling --prefer-machine work-laptop\n```\n\n---\n\n## Git Remote Implementation\n\n```rust\n/// Git-based remote client\npub struct GitRemoteClient {\n    /// Repository URL\n    url: String,\n    \n    /// Local clone path\n    local_path: PathBuf,\n    \n    /// Branch to sync\n    branch: String,\n    \n    /// Git authentication\n    auth: GitAuth,\n}\n\nimpl GitRemoteClient {\n    pub fn new(url: \u0026str, auth: RemoteAuth) -\u003e Result\u003cSelf, SyncError\u003e {\n        let local_path = dirs::cache_dir()\n            .unwrap_or_else(|| PathBuf::from(\".cache\"))\n            .join(\"meta_skill\")\n            .join(\"remotes\")\n            .join(Self::url_to_dirname(url));\n        \n        let git_auth = match auth {\n            RemoteAuth::SshKey { key_path } =\u003e GitAuth::Ssh { key_path },\n            RemoteAuth::Token { token_env_var } =\u003e {\n                let token = std::env::var(\u0026token_env_var)\n                    .map_err(|_| SyncError::MissingAuth(token_env_var))?;\n                GitAuth::Token(token)\n            }\n            _ =\u003e GitAuth::None,\n        };\n        \n        Ok(Self {\n            url: url.to_string(),\n            local_path,\n            branch: \"main\".to_string(),\n            auth: git_auth,\n        })\n    }\n    \n    /// Ensure local clone is up to date\n    async fn ensure_clone(\u0026self) -\u003e Result\u003c(), SyncError\u003e {\n        if self.local_path.exists() {\n            // Fetch latest\n            self.run_git(\u0026[\"fetch\", \"origin\", \u0026self.branch]).await?;\n            self.run_git(\u0026[\"reset\", \"--hard\", \u0026format!(\"origin/{}\", self.branch)]).await?;\n        } else {\n            // Clone fresh\n            std::fs::create_dir_all(\u0026self.local_path)?;\n            self.run_git(\u0026[\"clone\", \"--branch\", \u0026self.branch, \u0026self.url, \".\"]).await?;\n        }\n        Ok(())\n    }\n    \n    /// Commit and push changes\n    async fn commit_and_push(\u0026self, message: \u0026str) -\u003e Result\u003c(), SyncError\u003e {\n        self.run_git(\u0026[\"add\", \"-A\"]).await?;\n        self.run_git(\u0026[\"commit\", \"-m\", message]).await?;\n        self.run_git(\u0026[\"push\", \"origin\", \u0026self.branch]).await?;\n        Ok(())\n    }\n}\n\n#[async_trait]\nimpl RemoteClient for GitRemoteClient {\n    async fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cRemoteSkillInfo\u003e, SyncError\u003e {\n        self.ensure_clone().await?;\n        \n        let mut skills = Vec::new();\n        let skills_dir = self.local_path.join(\"skills\");\n        \n        if skills_dir.exists() {\n            for entry in std::fs::read_dir(\u0026skills_dir)? {\n                let entry = entry?;\n                if entry.path().is_dir() {\n                    if let Some(info) = self.read_skill_info(\u0026entry.path())? {\n                        skills.push(info);\n                    }\n                }\n            }\n        }\n        \n        Ok(skills)\n    }\n    \n    async fn get_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cSkill\u003e, SyncError\u003e {\n        let skill_path = self.local_path.join(\"skills\").join(\u0026id.0);\n        if !skill_path.exists() {\n            return Ok(None);\n        }\n        \n        // Parse skill from directory structure\n        let skill = self.parse_skill_directory(\u0026skill_path)?;\n        Ok(Some(skill))\n    }\n    \n    async fn push_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c(), SyncError\u003e {\n        self.ensure_clone().await?;\n        \n        let skill_path = self.local_path.join(\"skills\").join(\u0026skill.id.0);\n        std::fs::create_dir_all(\u0026skill_path)?;\n        \n        // Write skill to directory structure\n        self.write_skill_directory(\u0026skill_path, skill)?;\n        \n        // Commit and push\n        let message = format!(\"Update skill: {}\", skill.name);\n        self.commit_and_push(\u0026message).await?;\n        \n        Ok(())\n    }\n    \n    async fn delete_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003c(), SyncError\u003e {\n        self.ensure_clone().await?;\n        \n        let skill_path = self.local_path.join(\"skills\").join(\u0026id.0);\n        if skill_path.exists() {\n            std::fs::remove_dir_all(\u0026skill_path)?;\n            let message = format!(\"Delete skill: {}\", id.0);\n            self.commit_and_push(\u0026message).await?;\n        }\n        \n        Ok(())\n    }\n    \n    async fn get_hash(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cString\u003e, SyncError\u003e {\n        if let Some(skill) = self.get_skill(id).await? {\n            Ok(Some(SyncState::hash_skill(\u0026skill)))\n        } else {\n            Ok(None)\n        }\n    }\n}\n```\n\n---\n\n## S3 Remote Implementation\n\n```rust\nuse aws_sdk_s3::{Client as S3Client, Config};\n\n/// S3-based remote client\npub struct S3RemoteClient {\n    client: S3Client,\n    bucket: String,\n    prefix: String,\n}\n\nimpl S3RemoteClient {\n    pub async fn new(bucket: \u0026str, region: \u0026str, prefix: Option\u003c\u0026str\u003e) -\u003e Result\u003cSelf, SyncError\u003e {\n        let config = aws_config::from_env()\n            .region(aws_sdk_s3::config::Region::new(region.to_string()))\n            .load()\n            .await;\n        \n        let client = S3Client::new(\u0026config);\n        \n        Ok(Self {\n            client,\n            bucket: bucket.to_string(),\n            prefix: prefix.unwrap_or(\"skills\").to_string(),\n        })\n    }\n    \n    fn skill_key(\u0026self, id: \u0026SkillId) -\u003e String {\n        format!(\"{}/{}/skill.json\", self.prefix, id.0)\n    }\n}\n\n#[async_trait]\nimpl RemoteClient for S3RemoteClient {\n    async fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cRemoteSkillInfo\u003e, SyncError\u003e {\n        let mut skills = Vec::new();\n        let mut continuation_token = None;\n        \n        loop {\n            let mut request = self.client\n                .list_objects_v2()\n                .bucket(\u0026self.bucket)\n                .prefix(\u0026self.prefix)\n                .delimiter(\"/\");\n            \n            if let Some(token) = continuation_token {\n                request = request.continuation_token(token);\n            }\n            \n            let response = request.send().await?;\n            \n            if let Some(prefixes) = response.common_prefixes {\n                for prefix in prefixes {\n                    if let Some(p) = prefix.prefix {\n                        // Extract skill ID from prefix\n                        let skill_id = p.trim_end_matches('/').rsplit('/').next()\n                            .map(|s| SkillId(s.to_string()));\n                        \n                        if let Some(id) = skill_id {\n                            if let Ok(Some(hash)) = self.get_hash(\u0026id).await {\n                                skills.push(RemoteSkillInfo {\n                                    id,\n                                    hash,\n                                    modified_at: Utc::now(), // TODO: Get from metadata\n                                    modified_by: \"unknown\".to_string(),\n                                });\n                            }\n                        }\n                    }\n                }\n            }\n            \n            if response.is_truncated == Some(true) {\n                continuation_token = response.next_continuation_token;\n            } else {\n                break;\n            }\n        }\n        \n        Ok(skills)\n    }\n    \n    async fn get_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cSkill\u003e, SyncError\u003e {\n        let key = self.skill_key(id);\n        \n        match self.client.get_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .send()\n            .await\n        {\n            Ok(response) =\u003e {\n                let body = response.body.collect().await?;\n                let skill: Skill = serde_json::from_slice(\u0026body.into_bytes())?;\n                Ok(Some(skill))\n            }\n            Err(e) if e.is_not_found() =\u003e Ok(None),\n            Err(e) =\u003e Err(SyncError::S3Error(e.to_string())),\n        }\n    }\n    \n    async fn push_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c(), SyncError\u003e {\n        let key = self.skill_key(\u0026skill.id);\n        let body = serde_json::to_vec_pretty(skill)?;\n        \n        self.client.put_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .body(body.into())\n            .content_type(\"application/json\")\n            .send()\n            .await?;\n        \n        Ok(())\n    }\n    \n    async fn delete_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003c(), SyncError\u003e {\n        let key = self.skill_key(id);\n        \n        self.client.delete_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .send()\n            .await?;\n        \n        Ok(())\n    }\n    \n    async fn get_hash(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cString\u003e, SyncError\u003e {\n        // Use head_object to check if exists and get ETag\n        let key = self.skill_key(id);\n        \n        match self.client.head_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .send()\n            .await\n        {\n            Ok(response) =\u003e Ok(response.e_tag),\n            Err(e) if e.is_not_found() =\u003e Ok(None),\n            Err(e) =\u003e Err(SyncError::S3Error(e.to_string())),\n        }\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum SyncError {\n    #[error(\"Unknown remote: {0}\")]\n    UnknownRemote(String),\n    \n    #[error(\"Skill not found: {0:?}\")]\n    SkillNotFound(SkillId),\n    \n    #[error(\"Missing authentication: {0}\")]\n    MissingAuth(String),\n    \n    #[error(\"Git operation failed: {0}\")]\n    GitError(String),\n    \n    #[error(\"S3 operation failed: {0}\")]\n    S3Error(String),\n    \n    #[error(\"Network error: {0}\")]\n    NetworkError(String),\n    \n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Conflict requires manual resolution: {0:?}\")]\n    ManualResolutionRequired(SkillId),\n}\n```\n\n---\n\n## Configuration File\n\nMachine identity and sync configuration stored in `~/.config/meta_skill/sync.toml`:\n\n```toml\n[machine]\nid = \"abc123-def456\"\nname = \"work-laptop\"\ndescription = \"Primary development machine\"\n\n[sync]\ndefault_conflict_strategy = \"latest-wins\"\nauto_sync_interval_minutes = 30\n\n[[remotes]]\nname = \"origin\"\ntype = \"git\"\nurl = \"git@github.com:user/skills.git\"\nenabled = true\nauto_push = true\nauto_pull = true\n\n[[remotes]]\nname = \"backup\"\ntype = \"s3\"\nurl = \"s3://my-skills-bucket/skills\"\nregion = \"us-east-1\"\nenabled = true\nauto_push = true\nauto_pull = false\n\n[conflict_strategies]\n\"work-critical-skill\" = \"keep-local\"\n\"shared-team-skill\" = \"prefer-machine:team-server\"\n```\n\n---\n\n## Testing\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_machine_identity_generation() {\n        let identity = MachineIdentity::generate(\"test-machine\".to_string());\n        assert!(!identity.machine_id.is_empty());\n        assert_eq!(identity.machine_name, \"test-machine\");\n    }\n    \n    #[test]\n    fn test_skill_hashing_deterministic() {\n        let skill = Skill {\n            id: SkillId(\"test\".to_string()),\n            name: \"Test Skill\".to_string(),\n            sections: HashMap::from([\n                (\"overview\".to_string(), Section { content: \"content\".to_string() }),\n            ]),\n            ..Default::default()\n        };\n        \n        let hash1 = SyncState::hash_skill(\u0026skill);\n        let hash2 = SyncState::hash_skill(\u0026skill);\n        \n        assert_eq!(hash1, hash2);\n    }\n    \n    #[test]\n    fn test_conflict_resolution_latest_wins() {\n        let resolver = ConflictResolver::new(ConflictStrategy::LatestWins);\n        \n        let older = Skill {\n            id: SkillId(\"test\".to_string()),\n            modified_at: Utc::now() - chrono::Duration::hours(1),\n            ..Default::default()\n        };\n        \n        let newer = Skill {\n            id: SkillId(\"test\".to_string()),\n            modified_at: Utc::now(),\n            ..Default::default()\n        };\n        \n        let resolution = resolver.resolve(\n            \u0026older.id, \u0026older, \u0026newer, \"machine-a\", \"machine-b\"\n        ).unwrap();\n        \n        assert!(matches!(resolution, ConflictResolution::UseRemote));\n    }\n    \n    #[tokio::test]\n    async fn test_sync_engine_local_only_pushes() {\n        // Test that local-only skills get pushed to remote\n        let mut engine = create_test_engine();\n        let skill = create_test_skill(\"local-only\");\n        engine.registry.save(\u0026skill).unwrap();\n        \n        let report = engine.sync_all().await.unwrap();\n        \n        assert!(report.pushed.contains(\u0026skill.id));\n    }\n}\n```\n\n---\n\n## Dependencies\n\n- **Bundle Format and Manifest** (meta_skill-6fi): Sync uses bundle format for transferring skills\n- `serde`, `serde_json`: Serialization\n- `chrono`: Timestamps\n- `sha2`: Content hashing\n- `uuid`: Machine ID generation\n- `git2`: Git operations (optional)\n- `aws-sdk-s3`: S3 operations (optional)\n- `tokio`: Async runtime\n- `thiserror`: Error handling","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:54:00.445843757-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:01.68296559-05:00","labels":["multi-machine","phase-5","sync"],"dependencies":[{"issue_id":"meta_skill-ujr","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T23:04:13.477055066-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ujr","depends_on_id":"meta_skill-swe","type":"blocks","created_at":"2026-01-13T23:43:35.346232597-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-vc3","title":"[P4] Autonomous Build Mode","description":"# Autonomous Build Mode\n\n## Overview\n\nEnable long‑running, unattended skill generation with checkpointing and resumability. Autonomous mode is essentially guided mode without prompts.\n\n---\n\n## Tasks\n\n1. Implement `BuildSession` state machine.\n2. Persist checkpoints to disk (`.ms/build/checkpoints/`).\n3. Resume from last checkpoint (`ms build --resume`).\n4. Emit progress + quality gate logs.\n\n---\n\n## Testing Requirements\n\n- Unit tests for state transitions.\n- Integration tests: simulate crash + resume.\n- E2E: autonomous build completes with valid SkillSpec.\n\n---\n\n## Acceptance Criteria\n\n- Long‑running builds can resume after interruption.\n- Quality gates enforce minimum evidence thresholds.\n- Progress reporting is deterministic.\n\n---\n\n## Dependencies\n\n- `meta_skill-ztm` ms build Command\n- `meta_skill-llm` Session Quality Scoring","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:51.39259279-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:03:22.70056445-05:00","labels":["autonomous","checkpoints","phase-4"],"dependencies":[{"issue_id":"meta_skill-vc3","depends_on_id":"meta_skill-330","type":"blocks","created_at":"2026-01-13T22:26:13.178995276-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-vc3","depends_on_id":"meta_skill-llm","type":"blocks","created_at":"2026-01-13T22:26:13.209862769-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-vc3","depends_on_id":"meta_skill-ztm","type":"blocks","created_at":"2026-01-14T00:03:41.429198364-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-vqr","title":"[P1] Robot Mode Infrastructure","description":"## Robot Mode Output Specification (Full)\n\nRobot mode enables machine-readable JSON output for automation integration. All robot output goes to stdout (data only), while stderr handles diagnostics/logs.\n\n### Global Robot Flags\n\n```bash\n# Global robot flags (alternative to --robot on individual commands)\nms --robot-status              # Full registry status\nms --robot-health              # Health check summary\nms --robot-suggest             # Context-aware suggestions\nms --robot-search=\"query\"      # Search as JSON\nms --robot-build-status        # Active build sessions\nms --robot-cass-status         # CASS integration status\n\n# Per-command robot mode\nms list --robot\nms search \"query\" --robot\nms show skill-id --robot\nms load skill-id --robot\nms review skill-id --robot\nms suggest --robot\nms build --robot --status\nms stats --robot\nms doctor --robot\nms sync status --robot\n```\n\n### Output Schemas\n\n```rust\n/// Standard robot response wrapper\n#[derive(Serialize)]\npub struct RobotResponse\u003cT\u003e {\n    /// Operation status\n    pub status: RobotStatus,\n    /// Timestamp of response\n    pub timestamp: DateTime\u003cUtc\u003e,\n    /// ms version\n    pub version: String,\n    /// Response payload (varies by command)\n    pub data: T,\n    /// Optional warnings (non-fatal issues)\n    #[serde(skip_serializing_if = \"Vec::is_empty\")]\n    pub warnings: Vec\u003cString\u003e,\n}\n\n#[derive(Serialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum RobotStatus {\n    Ok,\n    Error { code: String, message: String },\n    Partial { completed: usize, failed: usize },\n}\n\n/// --robot-status response\n#[derive(Serialize)]\npub struct StatusResponse {\n    pub registry: RegistryStatus,\n    pub search_index: IndexStatus,\n    pub cass_integration: CassStatus,\n    pub active_builds: Vec\u003cBuildSessionSummary\u003e,\n    pub config: ConfigSummary,\n}\n\n#[derive(Serialize)]\npub struct RegistryStatus {\n    pub total_skills: usize,\n    pub indexed_skills: usize,\n    pub local_skills: usize,\n    pub upstream_skills: usize,\n    pub modified_skills: usize,\n    pub last_index_update: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\n/// --robot-suggest response\n#[derive(Serialize)]\npub struct SuggestResponse {\n    pub context: SuggestionContext,\n    pub suggestions: Vec\u003cSuggestionItem\u003e,\n    pub swarm_plan: Option\u003cSwarmPlan\u003e,\n    pub explain: Option\u003cSuggestionExplain\u003e,\n}\n\n#[derive(Serialize)]\npub struct SuggestionItem {\n    pub skill_id: String,\n    pub name: String,\n    pub score: f32,\n    pub reason: String,\n    pub disclosure_level: String,\n    pub token_estimate: usize,\n    pub pack_budget: Option\u003cusize\u003e,\n    pub packed_token_estimate: Option\u003cusize\u003e,\n    pub slice_count: Option\u003cusize\u003e,\n    pub dependencies: Vec\u003cString\u003e,\n    pub layer: Option\u003cString\u003e,\n    pub conflicts: Vec\u003cString\u003e,\n    pub requirements: Option\u003cRequirementStatus\u003e,\n    pub explanation: Option\u003cSuggestionExplanation\u003e,\n}\n\n#[derive(Serialize)]\npub struct SuggestionExplain {\n    pub enabled: bool,\n    pub signals: Vec\u003cSuggestionSignalExplain\u003e,\n}\n\n#[derive(Serialize)]\npub struct SuggestionExplanation {\n    pub matched_triggers: Vec\u003cString\u003e,\n    pub signal_scores: Vec\u003cSignalScore\u003e,\n    pub rrf_components: RrfBreakdown,\n}\n\n#[derive(Serialize)]\npub struct SuggestionSignalExplain {\n    pub signal_type: String,\n    pub value: String,\n    pub weight: f32,\n}\n\n#[derive(Serialize)]\npub struct SignalScore {\n    pub signal: String,\n    pub contribution: f32,\n}\n\n#[derive(Serialize)]\npub struct RrfBreakdown {\n    pub bm25_rank: Option\u003cusize\u003e,\n    pub vector_rank: Option\u003cusize\u003e,\n    pub rrf_score: f32,\n}\n\n/// --robot-build-status response\n#[derive(Serialize)]\npub struct BuildStatusResponse {\n    pub active_sessions: Vec\u003cBuildSessionDetail\u003e,\n    pub recent_completed: Vec\u003cBuildSessionSummary\u003e,\n    pub queued_patterns: usize,\n    pub queued_uncertainties: usize,\n}\n\n/// --robot requirements response\n#[derive(Serialize)]\npub struct RequirementsResponse {\n    pub skill_id: String,\n    pub requirements: SkillRequirements,\n    pub status: RequirementStatus,\n    pub environment: EnvironmentSnapshot,\n}\n\n#[derive(Serialize)]\npub struct BuildSessionDetail {\n    pub session_id: String,\n    pub skill_name: String,\n    pub state: BuildState,\n    pub iteration: usize,\n    pub patterns_used: usize,\n    pub patterns_available: usize,\n    pub started_at: DateTime\u003cUtc\u003e,\n    pub last_activity: DateTime\u003cUtc\u003e,\n    pub checkpoint_path: Option\u003cPathBuf\u003e,\n}\n```\n\n### Error Response Format\n\n```json\n{\n  \"status\": {\n    \"error\": {\n      \"code\": \"SKILL_NOT_FOUND\",\n      \"message\": \"Skill 'nonexistent' not found in registry\"\n    }\n  },\n  \"timestamp\": \"2026-01-13T15:30:00Z\",\n  \"version\": \"0.1.0\",\n  \"data\": null,\n  \"warnings\": []\n}\n```\n\n### Integration Examples\n\n```bash\n# NTM integration: spawn agent with skills\nskills=$(ms --robot-suggest | jq -r '.data.suggestions[].skill_id')\nfor skill in $skills; do\n  content=$(ms load \"$skill\" --robot --level=full | jq -r '.data.content')\n  # Inject into agent prompt\ndone\n\n# BV integration: find skills for current bead\nbead_type=$(bv show BD-123 --json | jq -r '.type')\nrelevant_skills=$(ms search \"$bead_type\" --robot | jq -r '.data.results[].skill_id')\n\n# Automated skill generation pipeline\nms build --robot --from-cass \"nextjs ui\" --auto --max-iterations 10 | \\\n  jq -r '.data.generated_skill_path'\n```\n\n### Convention Summary\n\n| Stream | Content |\n|--------|---------|\n| stdout | JSON data only (parseable) |\n| stderr | Diagnostics, logs, human-readable progress |\n| Exit 0 | Success |\n| Exit \u003e0 | Error (check status.error for details) |","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:05.914271765-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:06:10.063535138-05:00","labels":["api","phase-1","robot-mode"],"dependencies":[{"issue_id":"meta_skill-vqr","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.875278949-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-wnk","title":"Snapshot Tests","description":"## Overview\n\nImplement snapshot tests using the insta crate for output verification. This bead implements Section 18.5 of the Testing Strategy, ensuring all CLI outputs, disclosure levels, error messages, and diagnostic outputs remain consistent.\n\n## Requirements\n\n### 1. Snapshot Test Configuration\n\nAdd to `Cargo.toml`:\n```toml\n[dev-dependencies]\ninsta = { version = \"1.34\", features = [\"yaml\", \"json\", \"redactions\"] }\n```\n\nCreate `insta.yaml` in project root:\n```yaml\n# Insta configuration\nbehavior:\n  review: true\n  update_mode: new\n  \nsnapshot_path_template: \"{module}/{function}\"\n```\n\n### 2. CLI Output Format Snapshots\n\nCreate `tests/snapshots/cli_output.rs`:\n\n```rust\nuse insta::{assert_snapshot, assert_json_snapshot, with_settings};\nuse ms::cli::{OutputFormat, run_command};\n\n#[test]\nfn test_list_output_human() {\n    let output = run_command(\u0026[\"list\"], OutputFormat::Human);\n    \n    with_settings!({\n        description =\u003e \"Human-readable list output\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"list_human\", output);\n    });\n}\n\n#[test]\nfn test_list_output_robot_json() {\n    let output = run_command(\u0026[\"list\", \"--robot\"], OutputFormat::Json);\n    \n    with_settings!({\n        description =\u003e \"Robot-mode JSON list output\",\n        omit_expression =\u003e true,\n    }, {\n        assert_json_snapshot!(\"list_robot_json\", output);\n    });\n}\n\n#[test]\nfn test_search_output_human() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"search\", \"rust\"], OutputFormat::Human);\n    \n    with_settings!({\n        description =\u003e \"Human-readable search results\",\n        omit_expression =\u003e true,\n        redactions =\u003e redact_dynamic_fields(),\n    }, {\n        assert_snapshot!(\"search_human\", output);\n    });\n}\n\n#[test]\nfn test_search_output_robot_json() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"search\", \"rust\", \"--robot\"], OutputFormat::Json);\n    \n    with_settings!({\n        description =\u003e \"Robot-mode JSON search results\",\n        omit_expression =\u003e true,\n        redactions =\u003e redact_dynamic_fields(),\n    }, {\n        assert_json_snapshot!(\"search_robot_json\", output);\n    });\n}\n\n#[test]\nfn test_show_output_human() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"show\", \"test-skill\"], OutputFormat::Human);\n    \n    with_settings!({\n        description =\u003e \"Human-readable skill details\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"show_human\", output);\n    });\n}\n\n#[test]\nfn test_show_output_robot_json() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"show\", \"test-skill\", \"--robot\"], OutputFormat::Json);\n    \n    with_settings!({\n        description =\u003e \"Robot-mode JSON skill details\",\n        omit_expression =\u003e true,\n    }, {\n        assert_json_snapshot!(\"show_robot_json\", output);\n    });\n}\n\n/// Redact dynamic fields like timestamps and UUIDs\nfn redact_dynamic_fields() -\u003e Vec\u003c(\u0026'static str, \u0026'static str)\u003e {\n    vec![\n        (\".timestamp\", \"[TIMESTAMP]\"),\n        (\".id\", \"[UUID]\"),\n        (\".created_at\", \"[TIMESTAMP]\"),\n        (\".updated_at\", \"[TIMESTAMP]\"),\n    ]\n}\n```\n\n### 3. Disclosure Level Snapshots\n\nCreate `tests/snapshots/disclosure_levels.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::disclosure::{disclose, DisclosureLevel};\nuse ms::skill::SkillSpec;\n\nfn create_test_skill() -\u003e SkillSpec {\n    SkillSpec {\n        name: \"rust-error-handling\".to_string(),\n        description: \"Best practices for error handling in Rust\".to_string(),\n        tags: vec![\"rust\".to_string(), \"errors\".to_string(), \"best-practices\".to_string()],\n        content: r#\"\n# Error Handling in Rust\n\n## Overview\nThis skill covers comprehensive error handling patterns in Rust.\n\n## Key Patterns\n\n### Result Type\nUse Result\u003cT, E\u003e for recoverable errors.\n\n### The ? Operator\nPropagate errors elegantly with the ? operator.\n\n### Custom Error Types\nCreate domain-specific error types.\n\n## Examples\n```rust\nfn read_file(path: \u0026str) -\u003e Result\u003cString, std::io::Error\u003e {\n    std::fs::read_to_string(path)\n}\n```\n\n## Context\nThis skill is useful when building robust Rust applications.\n\n## Dependencies\n- rust-basics\n- rust-types\n\"#.to_string(),\n        dependencies: vec![\"rust-basics\".to_string()],\n        ..Default::default()\n    }\n}\n\n#[test]\nfn test_disclosure_minimal() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Minimal);\n    \n    with_settings!({\n        description =\u003e \"Minimal disclosure: name and brief description only\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_minimal\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_overview() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Overview);\n    \n    with_settings!({\n        description =\u003e \"Overview disclosure: name, description, tags, high-level structure\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_overview\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_standard() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Standard);\n    \n    with_settings!({\n        description =\u003e \"Standard disclosure: everything except implementation details\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_standard\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_full() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Full);\n    \n    with_settings!({\n        description =\u003e \"Full disclosure: complete content including code examples\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_full\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_complete() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Complete);\n    \n    with_settings!({\n        description =\u003e \"Complete disclosure: everything including metadata and provenance\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_complete\", output);\n    });\n}\n```\n\n### 4. Error Message Snapshots\n\nCreate `tests/snapshots/error_messages.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::errors::MsError;\n\n#[test]\nfn test_error_skill_not_found() {\n    let error = MsError::SkillNotFound {\n        name: \"nonexistent-skill\".to_string(),\n        suggestions: vec![\"similar-skill\".to_string(), \"other-skill\".to_string()],\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when skill is not found with suggestions\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_skill_not_found\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_skill_not_found_no_suggestions() {\n    let error = MsError::SkillNotFound {\n        name: \"xyz-unknown\".to_string(),\n        suggestions: vec![],\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when skill not found without suggestions\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_skill_not_found_no_suggestions\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_invalid_skill_name() {\n    let error = MsError::InvalidSkillName {\n        name: \"Invalid Skill Name!\".to_string(),\n        reason: \"Skill names must be lowercase with hyphens\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error for invalid skill name format\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_invalid_skill_name\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_config_not_found() {\n    let error = MsError::ConfigNotFound {\n        path: \"/home/user/.config/ms/config.toml\".to_string(),\n        hint: \"Run 'ms init' to create a default configuration\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when config file is missing\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_config_not_found\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_database_error() {\n    let error = MsError::DatabaseError {\n        operation: \"insert skill\".to_string(),\n        details: \"UNIQUE constraint failed: skills.name\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error for database operation failure\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_database_error\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_network_error() {\n    let error = MsError::NetworkError {\n        url: \"https://api.skills.example.com/v1/search\".to_string(),\n        reason: \"Connection timed out\".to_string(),\n        retry_hint: \"Check your internet connection and try again\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error for network operation failure\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_network_error\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_parse_error() {\n    let error = MsError::ParseError {\n        file: \"skills/my-skill/SKILL.md\".to_string(),\n        line: 15,\n        column: 8,\n        message: \"Unexpected token 'invalid'\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when parsing SKILL.md fails\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_parse_error\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_dependency_cycle() {\n    let error = MsError::DependencyCycle {\n        skill: \"skill-a\".to_string(),\n        cycle: vec![\"skill-a\".to_string(), \"skill-b\".to_string(), \"skill-c\".to_string(), \"skill-a\".to_string()],\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when dependency cycle is detected\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_dependency_cycle\", error.to_string());\n    });\n}\n```\n\n### 5. Diagnostic Output Snapshots\n\nCreate `tests/snapshots/diagnostics.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::diagnostics::{DiagnosticReport, HealthCheck, IndexStats};\n\n#[test]\nfn test_diagnostic_health_all_ok() {\n    let report = DiagnosticReport {\n        checks: vec![\n            HealthCheck { name: \"database\".to_string(), status: \"ok\".to_string(), details: None },\n            HealthCheck { name: \"index\".to_string(), status: \"ok\".to_string(), details: None },\n            HealthCheck { name: \"config\".to_string(), status: \"ok\".to_string(), details: None },\n        ],\n        warnings: vec![],\n        errors: vec![],\n    };\n    \n    with_settings!({\n        description =\u003e \"Health check output when all systems are OK\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_health_all_ok\", report.render());\n    });\n}\n\n#[test]\nfn test_diagnostic_health_with_warnings() {\n    let report = DiagnosticReport {\n        checks: vec![\n            HealthCheck { name: \"database\".to_string(), status: \"ok\".to_string(), details: None },\n            HealthCheck { name: \"index\".to_string(), status: \"warning\".to_string(), \n                         details: Some(\"Index is 3 days old, consider re-indexing\".to_string()) },\n            HealthCheck { name: \"config\".to_string(), status: \"ok\".to_string(), details: None },\n        ],\n        warnings: vec![\"Index may be stale\".to_string()],\n        errors: vec![],\n    };\n    \n    with_settings!({\n        description =\u003e \"Health check output with warnings\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_health_with_warnings\", report.render());\n    });\n}\n\n#[test]\nfn test_diagnostic_health_with_errors() {\n    let report = DiagnosticReport {\n        checks: vec![\n            HealthCheck { name: \"database\".to_string(), status: \"error\".to_string(), \n                         details: Some(\"Database file is corrupted\".to_string()) },\n            HealthCheck { name: \"index\".to_string(), status: \"error\".to_string(), \n                         details: Some(\"Index directory not found\".to_string()) },\n            HealthCheck { name: \"config\".to_string(), status: \"ok\".to_string(), details: None },\n        ],\n        warnings: vec![],\n        errors: vec![\n            \"Database integrity check failed\".to_string(),\n            \"Search functionality unavailable\".to_string(),\n        ],\n    };\n    \n    with_settings!({\n        description =\u003e \"Health check output with errors\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_health_with_errors\", report.render());\n    });\n}\n\n#[test]\nfn test_diagnostic_index_stats() {\n    let stats = IndexStats {\n        total_skills: 150,\n        indexed_skills: 148,\n        pending_skills: 2,\n        index_size_bytes: 1_234_567,\n        last_indexed: \"2024-01-15T10:30:00Z\".to_string(),\n        average_index_time_ms: 45,\n    };\n    \n    with_settings!({\n        description =\u003e \"Index statistics output\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_index_stats\", stats.render());\n    });\n}\n```\n\n### 6. SKILL.md Compilation Snapshots\n\nCreate `tests/snapshots/skill_compilation.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::compiler::{compile_skill, CompilationOptions};\n\n#[test]\nfn test_skill_compilation_minimal() {\n    let input = r#\"---\nname: minimal-skill\ndescription: A minimal skill for testing\n---\n# Minimal Skill\n\nJust a simple skill.\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for minimal SKILL.md\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_minimal\", output);\n    });\n}\n\n#[test]\nfn test_skill_compilation_with_code() {\n    let input = r#\"---\nname: code-skill\ndescription: Skill with code examples\ntags: [rust, examples]\n---\n# Code Examples\n\n## Rust Example\n```rust\nfn main() {\n    println!(\"Hello, world!\");\n}\n```\n\n## Python Example\n```python\nprint(\"Hello, world!\")\n```\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for skill with code blocks\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_with_code\", output);\n    });\n}\n\n#[test]\nfn test_skill_compilation_with_dependencies() {\n    let input = r#\"---\nname: dependent-skill\ndescription: Skill with dependencies\ndependencies:\n  - rust-basics\n  - error-handling\n---\n# Dependent Skill\n\nThis skill builds on rust-basics and error-handling.\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for skill with dependencies\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_with_dependencies\", output);\n    });\n}\n\n#[test]\nfn test_skill_compilation_full() {\n    let input = r#\"---\nname: complete-skill\ndescription: A complete skill with all sections\ntags: [complete, testing, example]\ndependencies:\n  - prerequisite-skill\ncontext:\n  when: Building production Rust applications\n  why: To ensure robust error handling\n---\n# Complete Skill\n\n## Overview\nThis is a comprehensive skill example.\n\n## Patterns\n\n### Pattern 1: Error Propagation\nUse the ? operator for clean error propagation.\n\n```rust\nfn read_config() -\u003e Result\u003cConfig, Error\u003e {\n    let content = std::fs::read_to_string(\"config.toml\")?;\n    let config: Config = toml::from_str(\u0026content)?;\n    Ok(config)\n}\n```\n\n### Pattern 2: Custom Error Types\nDefine domain-specific errors.\n\n## Examples\n\n### Example 1: Basic Usage\n```rust\nlet result = read_config();\nmatch result {\n    Ok(config) =\u003e println!(\"Loaded: {:?}\", config),\n    Err(e) =\u003e eprintln!(\"Error: {}\", e),\n}\n```\n\n## Caveats\n- Not suitable for performance-critical hot paths\n- Requires Rust 1.0 or later\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for complete SKILL.md\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_full\", output);\n    });\n}\n```\n\n### 7. Snapshot Test Organization\n\n```\ntests/\n├── snapshots/\n│   ├── mod.rs\n│   ├── cli_output.rs\n│   ├── disclosure_levels.rs\n│   ├── error_messages.rs\n│   ├── diagnostics.rs\n│   └── skill_compilation.rs\n└── snapshots/\n    └── cli_output/\n        ├── list_human.snap\n        ├── list_robot_json.snap\n        ├── search_human.snap\n        └── ...\n    └── disclosure_levels/\n        ├── disclosure_minimal.snap\n        ├── disclosure_overview.snap\n        └── ...\n    └── error_messages/\n        ├── error_skill_not_found.snap\n        └── ...\n```\n\n### 8. CI Integration\n\nAdd to CI pipeline:\n```yaml\nsnapshot-tests:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v4\n    \n    - name: Run snapshot tests\n      run: cargo test --test snapshots\n    \n    - name: Check for uncommitted snapshot changes\n      run: |\n        if [ -n \"$(git status --porcelain tests/snapshots/)\" ]; then\n          echo \"Snapshot files have changed! Review and commit the changes.\"\n          git diff tests/snapshots/\n          exit 1\n        fi\n```\n\n### 9. Snapshot Review Workflow\n\n```bash\n# Run tests and review new/changed snapshots\ncargo insta test\n\n# Review pending snapshots interactively\ncargo insta review\n\n# Accept all pending snapshots\ncargo insta accept\n\n# Reject all pending snapshots\ncargo insta reject\n```\n\n## Acceptance Criteria\n\n1. [ ] insta crate configured with YAML/JSON support\n2. [ ] CLI output snapshots for human and robot modes\n3. [ ] Disclosure level snapshots (minimal, overview, standard, full, complete)\n4. [ ] Error message snapshots for all error types\n5. [ ] Diagnostic output snapshots\n6. [ ] SKILL.md compilation snapshots\n7. [ ] Snapshot tests organized by category\n8. [ ] CI integration to detect uncommitted changes\n9. [ ] Documentation for snapshot review workflow\n10. [ ] Redactions configured for dynamic fields\n\n## Dependencies\n\n- meta_skill-vqr (Robot Mode Infrastructure) - provides output formatting","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:58:48.418427242-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.911205318-05:00","labels":["output","snapshots","testing"],"dependencies":[{"issue_id":"meta_skill-wnk","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:58:53.133050445-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-wnk","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.232202773-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-wwx","title":"Beads Viewer (bv) Integration for Skill Graph Analysis","description":"# Beads Viewer (bv) Integration for Skill Graph Analysis\n\n## Overview\n\nIntegrate beads_viewer (bv) as the graph analysis engine for ms skill dependency visualization, bottleneck detection, and execution planning. Rather than implementing graph algorithms from scratch, ms leverages bv's battle-tested, SIMD-optimized graph analysis with 9 pre-computed metrics.\n\n**Location**: `/data/projects/beads_viewer`\n**Documentation**: `/data/projects/beads_viewer/README.md`\n\n## Why bv (not custom implementation)\n\n| Aspect | Custom Implementation | bv Integration |\n|--------|----------------------|----------------|\n| **Maturity** | New, untested | Production-ready, well-tested |\n| **Performance** | Unknown | Two-phase async (instant + 500ms timeout) |\n| **Metrics** | Must build each one | 9 pre-computed (PageRank, betweenness, HITS, etc.) |\n| **AI Interface** | Must design | Robot protocol ready (--robot-* flags) |\n| **Caching** | Build from scratch | Hash-based caching built-in |\n| **Cycle Detection** | Implement Tarjan's | Tarjan's variant included |\n\n## Use Cases for ms\n\n### 1. Skill Dependency Graph Analysis\nSkills can depend on other skills (e.g., \"docker-compose skill\" depends on \"docker skill\"). bv can:\n- Identify \"keystone\" skills (high PageRank) that are foundational\n- Detect cycles in skill dependencies (invalid configurations)\n- Find bottleneck skills that block many others\n- Compute execution order via topological sort\n\n### 2. Skill Pack Optimization\nWhen packing skills for token-limited contexts:\n- Use PageRank to prioritize foundational skills\n- Use HITS to identify Hubs (composite skills) vs Authorities (utility skills)\n- Use critical path to ensure dependencies are included\n\n### 3. Skill Suggestion Prioritization\nWhen suggesting skills to users:\n- High PageRank skills are more universally useful\n- Low out-degree skills have fewer prerequisites\n- Eigenvector centrality identifies strategically important skills\n\n### 4. Skill Collection Health\nUse bv's --robot-label-health to assess skill collection health:\n- Staleness detection (skills that haven't been updated)\n- Blocked skill chains\n- Velocity scoring\n\n## Architecture\n\n```rust\n/// bv-based skill graph analyzer\nstruct BvSkillAnalyzer {\n    /// Path to bv binary\n    bv_path: PathBuf,\n    /// Temporary JSONL file for skill graph\n    temp_graph_path: PathBuf,\n    /// Cache for computed metrics\n    metrics_cache: HashMap\u003cString, SkillMetrics\u003e,\n}\n\nimpl BvSkillAnalyzer {\n    /// Convert skill collection to beads.jsonl format for bv analysis\n    fn skills_to_beads_jsonl(\u0026self, skills: \u0026[Skill]) -\u003e Result\u003cPathBuf\u003e {\n        // Each skill becomes a \"bead\" with:\n        // - id: skill hash\n        // - title: skill name\n        // - dependencies: skill prerequisites\n        // - labels: skill domains (e.g., docker, k8s, git)\n        // - priority: skill confidence score\n    }\n\n    /// Run bv --robot-insights on skill graph\n    fn analyze_insights(\u0026self) -\u003e Result\u003cBvInsights\u003e {\n        // Call: bv --robot-insights --path \u003ctemp_graph\u003e\n        // Parse JSON response\n    }\n\n    /// Run bv --robot-plan for skill execution order\n    fn compute_execution_plan(\u0026self) -\u003e Result\u003cVec\u003cSkillId\u003e\u003e {\n        // Call: bv --robot-plan\n        // Extract topological order\n    }\n\n    /// Run bv --robot-triage for prioritized skill suggestions\n    fn triage_skills(\u0026self) -\u003e Result\u003cSkillTriage\u003e {\n        // Call: bv --robot-triage\n        // Map recommendations to skills\n    }\n\n    /// Check for cycles in skill dependencies\n    fn detect_cycles(\u0026self) -\u003e Result\u003cVec\u003cVec\u003cSkillId\u003e\u003e\u003e {\n        // Extract cycles from insights\n        // These indicate invalid skill configurations\n    }\n\n    /// Get keystone skills (high PageRank)\n    fn keystone_skills(\u0026self, limit: usize) -\u003e Result\u003cVec\u003c(SkillId, f64)\u003e\u003e {\n        // High PageRank = foundational skills\n    }\n\n    /// Get bottleneck skills (high betweenness)\n    fn bottleneck_skills(\u0026self, limit: usize) -\u003e Result\u003cVec\u003c(SkillId, f64)\u003e\u003e {\n        // High betweenness = gateway skills\n    }\n}\n```\n\n## CLI Commands\n\n```bash\n# Analyze skill graph\nms graph insights          # Full graph analysis (PageRank, betweenness, cycles)\nms graph insights --json   # JSON output for programmatic use\n\n# Find keystone skills\nms graph keystones         # Top foundational skills\nms graph keystones --limit 10\n\n# Find bottleneck skills\nms graph bottlenecks       # Skills that gate many others\n\n# Check for cycles\nms graph cycles            # Detect circular dependencies\nms graph cycles --fix      # Suggest cycle resolution\n\n# Execution planning\nms graph plan              # Optimal skill loading order\nms graph plan --for \"deploy to k8s\"  # Plan for specific task\n\n# Health check\nms graph health            # Overall skill collection health\nms graph health --by-domain  # Per-domain health\n```\n\n## Implementation Details\n\n### Skill-to-Bead Mapping\n\n```rust\n/// Convert a Skill to bv-compatible bead format\nfn skill_to_bead(skill: \u0026Skill) -\u003e serde_json::Value {\n    json!({\n        \"id\": skill.hash(),\n        \"title\": skill.name(),\n        \"description\": skill.description(),\n        \"status\": if skill.is_active() { \"open\" } else { \"closed\" },\n        \"priority\": skill.confidence_to_priority(), // P0-P4\n        \"labels\": skill.domains(),\n        \"dependencies\": skill.prerequisites().map(|p| p.hash()).collect::\u003cVec\u003c_\u003e\u003e(),\n        \"created_at\": skill.created_at(),\n        \"updated_at\": skill.updated_at(),\n    })\n}\n\n/// Priority mapping from confidence\nfn confidence_to_priority(confidence: f64) -\u003e u8 {\n    match confidence {\n        c if c \u003e= 0.9 =\u003e 0,  // P0 - Critical (very high confidence)\n        c if c \u003e= 0.7 =\u003e 1,  // P1 - High\n        c if c \u003e= 0.5 =\u003e 2,  // P2 - Medium\n        c if c \u003e= 0.3 =\u003e 3,  // P3 - Low\n        _ =\u003e 4,              // P4 - Backlog\n    }\n}\n```\n\n### Robot Output Parsing\n\n```rust\n/// Parse bv --robot-insights output\n#[derive(Deserialize)]\nstruct BvInsights {\n    bottlenecks: Vec\u003cBvMetric\u003e,\n    keystones: Vec\u003cBvMetric\u003e,\n    influencers: Vec\u003cBvMetric\u003e,\n    hubs: Vec\u003cBvMetric\u003e,\n    authorities: Vec\u003cBvMetric\u003e,\n    cycles: Vec\u003cVec\u003cString\u003e\u003e,\n    cluster_density: f64,\n    status: BvStatus,\n    data_hash: String,\n}\n\n#[derive(Deserialize)]\nstruct BvMetric {\n    id: String,\n    value: f64,\n}\n\n#[derive(Deserialize)]\nstruct BvStatus {\n    page_rank: MetricStatus,\n    betweenness: MetricStatus,\n    hits: MetricStatus,\n    eigenvector: MetricStatus,\n    critical_path: MetricStatus,\n    cycles: MetricStatus,\n}\n\n#[derive(Deserialize)]\nenum MetricStatus {\n    Computed { elapsed_ms: u64 },\n    Approx { elapsed_ms: u64, sample_size: usize },\n    Timeout { elapsed_ms: u64 },\n    Skipped { reason: String },\n}\n```\n\n## Tasks\n\n1. [ ] Detect bv installation and version\n2. [ ] Implement skill-to-bead JSONL conversion\n3. [ ] Implement BvSkillAnalyzer wrapper\n4. [ ] Parse all --robot-* outputs (insights, plan, triage)\n5. [ ] Build ms graph CLI commands\n6. [ ] Integrate with skill packer for priority ordering\n7. [ ] Integrate with skill suggester for recommendations\n8. [ ] Add cycle detection with resolution suggestions\n9. [ ] Implement metrics caching (use bv's data_hash)\n10. [ ] Handle bv unavailable case (graceful degradation)\n\n## Testing Requirements\n\n- bv integration tests (JSON parsing, command invocation)\n- Skill-to-bead conversion accuracy\n- Cycle detection correctness\n- Metrics caching validity\n- Graceful degradation when bv unavailable\n- Performance benchmarks for large skill graphs\n\n## Acceptance Criteria\n\n- bv detected and integrated\n- Skill graphs converted to beads.jsonl format\n- All 9 metrics available for skill analysis\n- Cycles detected and reported with suggestions\n- ms graph CLI commands functional\n- Graceful fallback when bv not installed\n- Metrics cached for performance\n\n## Dependencies\n\n- Phase 4 foundation (skill storage, basic operations)\n- Skill dependency tracking must be in place\n- Optional: Skill packer integration for ordering optimization\n\n## References\n\n- bv repository: /data/projects/beads_viewer\n- bv README: /data/projects/beads_viewer/README.md\n- bv robot protocol documentation (in README)\n- Plan Section 5.x (graph analysis integration)\n\nLabels: [phase-4 integration graph-analysis bv skill-deps]","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:12:10.179611368-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:12:10.179611368-05:00","labels":["bv","graph-analysis","integration","phase-4"],"dependencies":[{"issue_id":"meta_skill-wwx","depends_on_id":"meta_skill-jka","type":"blocks","created_at":"2026-01-13T23:12:25.776356588-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-wwx","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:12:25.808335935-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-x7k","title":"Skill Tests (ms test)","description":"# Skill Tests (ms test)\n\n**Phase 6 - Section 18.9**\n\nSkills include executable tests to validate correctness. Tests are stored under `tests/` in each skill directory and run via `ms test`. This ensures skills remain accurate and functional as they evolve.\n\n---\n\n## Overview\n\nSkills can become outdated or contain errors. Skill tests provide:\n\n1. **Validation**: Verify skill content is accurate and commands work\n2. **Regression Prevention**: Catch breaks when skills are updated\n3. **Quality Assurance**: Ensure skills meet quality standards before publishing\n4. **CI Integration**: Run tests in continuous integration pipelines\n\n---\n\n## Test File Format\n\nTests are written in YAML for readability and stored in `\u003cskill\u003e/tests/`:\n\n### Basic Test Structure\n\n```yaml\n# \u003cskill\u003e/tests/basic_load.yaml\nname: \"Skill loads correctly\"\ndescription: \"Verify the skill can be loaded and parsed\"\nskill: rust-error-handling\n\nsetup:\n  # Optional setup steps\n  - mkdir: { path: \"/tmp/test-workspace\" }\n  - write_file: \n      path: \"/tmp/test-workspace/test.rs\"\n      content: |\n        fn main() {\n            println!(\"test\");\n        }\n\nsteps:\n  - load_skill:\n      level: standard\n      \n  - assert:\n      skill_loaded: true\n      sections_present:\n        - overview\n        - error-types\n        - best-practices\n\n  - run:\n      cmd: \"rustc --version\"\n      \n  - assert:\n      exit_code: 0\n      stdout_contains: \"rustc\"\n\ncleanup:\n  - remove: { path: \"/tmp/test-workspace\" }\n\ntimeout: 30s\ntags: [smoke, load]\n```\n\n### Test Schema\n\n```yaml\n# Test file schema\nname: string                    # Test name (required)\ndescription: string             # What this test validates (optional)\nskill: string                   # Skill ID to test (required)\n\nsetup: Step[]                   # Setup steps (optional)\nsteps: Step[]                   # Test steps (required)\ncleanup: Step[]                 # Cleanup steps (optional)\n\ntimeout: duration               # Test timeout (default: 60s)\ntags: string[]                  # Tags for filtering\nskip_if: Condition[]            # Conditions to skip test\nrequires: Requirement[]         # System requirements\n```\n\n### Step Types\n\n```yaml\n# Load a skill\n- load_skill:\n    level: minimal | standard | comprehensive | full\n    budget: 2000                # Optional token budget\n    context:                    # Optional suggestion context\n      file_types: [\".rs\"]\n      recent_errors: [\"E0382\"]\n\n# Run a command\n- run:\n    cmd: \"cargo build\"\n    cwd: \"/tmp/workspace\"       # Working directory\n    env:                        # Environment variables\n      RUST_BACKTRACE: \"1\"\n    stdin: \"input text\"         # Optional stdin\n    timeout: 10s                # Command timeout\n\n# Assert conditions\n- assert:\n    exit_code: 0\n    stdout_contains: \"Success\"\n    stdout_not_contains: \"error\"\n    stderr_empty: true\n    file_exists: \"/tmp/output.txt\"\n    file_contains:\n      path: \"/tmp/output.txt\"\n      text: \"expected content\"\n    skill_loaded: true\n    sections_present: [\"overview\", \"examples\"]\n    tokens_used_lt: 2000\n    retrieval_rank_le: 3\n\n# Write a file\n- write_file:\n    path: \"/tmp/test.rs\"\n    content: |\n      fn main() {}\n\n# Create directory\n- mkdir:\n    path: \"/tmp/workspace\"\n    parents: true               # Like mkdir -p\n\n# Remove file/directory\n- remove:\n    path: \"/tmp/workspace\"\n    recursive: true\n\n# Copy file\n- copy:\n    from: \"fixtures/input.rs\"\n    to: \"/tmp/workspace/input.rs\"\n\n# Sleep (for async operations)\n- sleep:\n    duration: 1s\n\n# Set variable for later use\n- set:\n    name: \"output_path\"\n    value: \"/tmp/result.txt\"\n\n# Use variable\n- run:\n    cmd: \"cat ${output_path}\"\n\n# Conditional execution\n- if:\n    condition:\n      platform: linux\n    then:\n      - run: { cmd: \"ls -la\" }\n    else:\n      - run: { cmd: \"dir\" }\n```\n\n---\n\n## Extended Test Types\n\n### Retrieval Tests\n\nTest that skills are retrieved correctly for given queries:\n\n```yaml\n# \u003cskill\u003e/tests/retrieval_test.yaml\nname: \"Retrieval for error handling query\"\ntype: retrieval\nskill: rust-error-handling\n\nquery: \"How do I handle errors in Rust?\"\ncontext:\n  file_types: [\".rs\"]\n  project_type: \"rust\"\n\nexpect:\n  - skill: rust-error-handling\n    rank_le: 2                  # Should be in top 2 results\n    sections_include:\n      - error-types\n      - best-practices\n      \n  - skill: rust-result-option   # Related skill should also appear\n    rank_le: 5\n```\n\n```rust\n/// Retrieval test definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RetrievalTest {\n    /// Test name\n    pub name: String,\n    \n    /// Skill being tested\n    pub skill: String,\n    \n    /// Search query\n    pub query: String,\n    \n    /// Suggestion context\n    pub context: SuggestionContext,\n    \n    /// Expected results\n    pub expect: Vec\u003cExpectedResult\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExpectedResult {\n    /// Expected skill in results\n    pub skill: String,\n    \n    /// Maximum acceptable rank (1 = top result)\n    pub rank_le: Option\u003cu32\u003e,\n    \n    /// Minimum acceptable rank\n    pub rank_ge: Option\u003cu32\u003e,\n    \n    /// Sections that should be included\n    pub sections_include: Vec\u003cString\u003e,\n    \n    /// Minimum relevance score\n    pub score_ge: Option\u003cf64\u003e,\n}\n\nimpl RetrievalTest {\n    /// Run the retrieval test\n    pub fn run(\u0026self, searcher: \u0026HybridSearcher) -\u003e Result\u003cRetrievalTestResult, TestError\u003e {\n        // Perform search\n        let results = searcher.search(\u0026self.query, 10)?;\n        \n        let mut passed = true;\n        let mut failures = Vec::new();\n        \n        for expected in \u0026self.expect {\n            // Find the skill in results\n            let position = results.iter().position(|r| r.skill.id.0 == expected.skill);\n            \n            match position {\n                Some(pos) =\u003e {\n                    let rank = pos + 1; // 1-indexed\n                    \n                    // Check rank constraints\n                    if let Some(max_rank) = expected.rank_le {\n                        if rank \u003e max_rank as usize {\n                            passed = false;\n                            failures.push(format!(\n                                \"Skill '{}' at rank {} but expected \u003c= {}\",\n                                expected.skill, rank, max_rank\n                            ));\n                        }\n                    }\n                    \n                    if let Some(min_rank) = expected.rank_ge {\n                        if rank \u003c min_rank as usize {\n                            passed = false;\n                            failures.push(format!(\n                                \"Skill '{}' at rank {} but expected \u003e= {}\",\n                                expected.skill, rank, min_rank\n                            ));\n                        }\n                    }\n                    \n                    // Check sections\n                    let result = \u0026results[pos];\n                    for section in \u0026expected.sections_include {\n                        if !result.skill.sections.contains_key(section) {\n                            passed = false;\n                            failures.push(format!(\n                                \"Skill '{}' missing expected section '{}'\",\n                                expected.skill, section\n                            ));\n                        }\n                    }\n                }\n                None =\u003e {\n                    passed = false;\n                    failures.push(format!(\n                        \"Skill '{}' not found in top 10 results\",\n                        expected.skill\n                    ));\n                }\n            }\n        }\n        \n        Ok(RetrievalTestResult {\n            test_name: self.name.clone(),\n            passed,\n            failures,\n            actual_results: results.iter()\n                .map(|r| (r.skill.id.0.clone(), r.score))\n                .collect(),\n        })\n    }\n}\n```\n\n### Packing Tests\n\nTest that skills pack efficiently within token budgets:\n\n```yaml\n# \u003cskill\u003e/tests/packing_test.yaml\nname: \"Efficient packing under budget\"\ntype: packing\nskill: rust-error-handling\n\nbudget: 2000\ncontract:\n  must_include:\n    - overview\n    - error-types/result\n  should_include:\n    - best-practices\n  nice_to_have:\n    - examples\n\nexpect:\n  tokens_used_le: 1800          # Should use less than budget\n  must_sections_present: true   # All must_include sections present\n  should_sections_percent_ge: 80  # At least 80% of should_include\n```\n\n```rust\n/// Packing test definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackingTest {\n    /// Test name\n    pub name: String,\n    \n    /// Skill being tested\n    pub skill: String,\n    \n    /// Token budget\n    pub budget: usize,\n    \n    /// Pack contract\n    pub contract: PackContract,\n    \n    /// Expected outcomes\n    pub expect: PackExpectation,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackContract {\n    /// Sections that MUST be included\n    pub must_include: Vec\u003cString\u003e,\n    \n    /// Sections that SHOULD be included if space allows\n    pub should_include: Vec\u003cString\u003e,\n    \n    /// Sections that are nice to have\n    pub nice_to_have: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackExpectation {\n    /// Maximum tokens used\n    pub tokens_used_le: Option\u003cusize\u003e,\n    \n    /// Minimum tokens used (test isn't leaving budget on table)\n    pub tokens_used_ge: Option\u003cusize\u003e,\n    \n    /// All must_include sections present\n    pub must_sections_present: bool,\n    \n    /// Minimum percentage of should_include sections\n    pub should_sections_percent_ge: Option\u003cf64\u003e,\n    \n    /// Content quality score\n    pub quality_score_ge: Option\u003cf64\u003e,\n}\n\nimpl PackingTest {\n    /// Run the packing test\n    pub fn run(\u0026self, packer: \u0026SkillPacker, skill: \u0026Skill) -\u003e Result\u003cPackingTestResult, TestError\u003e {\n        // Pack the skill\n        let packed = packer.pack(skill, self.budget, \u0026self.contract)?;\n        \n        let mut passed = true;\n        let mut failures = Vec::new();\n        \n        // Check token budget\n        if let Some(max_tokens) = self.expect.tokens_used_le {\n            if packed.tokens_used \u003e max_tokens {\n                passed = false;\n                failures.push(format!(\n                    \"Used {} tokens but expected \u003c= {}\",\n                    packed.tokens_used, max_tokens\n                ));\n            }\n        }\n        \n        if let Some(min_tokens) = self.expect.tokens_used_ge {\n            if packed.tokens_used \u003c min_tokens {\n                passed = false;\n                failures.push(format!(\n                    \"Used {} tokens but expected \u003e= {} (underutilizing budget)\",\n                    packed.tokens_used, min_tokens\n                ));\n            }\n        }\n        \n        // Check must_include sections\n        if self.expect.must_sections_present {\n            for section in \u0026self.contract.must_include {\n                if !packed.sections_included.contains(section) {\n                    passed = false;\n                    failures.push(format!(\n                        \"Must-include section '{}' not present\",\n                        section\n                    ));\n                }\n            }\n        }\n        \n        // Check should_include percentage\n        if let Some(min_percent) = self.expect.should_sections_percent_ge {\n            let included_count = self.contract.should_include.iter()\n                .filter(|s| packed.sections_included.contains(*s))\n                .count();\n            let percent = (included_count as f64 / self.contract.should_include.len() as f64) * 100.0;\n            \n            if percent \u003c min_percent {\n                passed = false;\n                failures.push(format!(\n                    \"Only {:.1}% of should_include sections present, expected \u003e= {:.1}%\",\n                    percent, min_percent\n                ));\n            }\n        }\n        \n        Ok(PackingTestResult {\n            test_name: self.name.clone(),\n            passed,\n            failures,\n            tokens_used: packed.tokens_used,\n            sections_included: packed.sections_included,\n        })\n    }\n}\n```\n\n---\n\n## Core Data Structures\n\n### Skill Test Harness\n\n```rust\nuse std::path::PathBuf;\nuse std::collections::HashMap;\nuse std::time::{Duration, Instant};\n\n/// Test execution harness\npub struct SkillTestHarness {\n    /// Skill registry for loading skills\n    skill_registry: Registry,\n    \n    /// Temporary workspace for test execution\n    temp_workspace: PathBuf,\n    \n    /// Environment variables for tests\n    env: HashMap\u003cString, String\u003e,\n    \n    /// Test timeout\n    default_timeout: Duration,\n    \n    /// Searcher for retrieval tests\n    searcher: Option\u003cHybridSearcher\u003e,\n    \n    /// Packer for packing tests\n    packer: Option\u003cSkillPacker\u003e,\n}\n\nimpl SkillTestHarness {\n    pub fn new(skill_registry: Registry) -\u003e Result\u003cSelf, TestError\u003e {\n        let temp_workspace = tempfile::tempdir()?.into_path();\n        \n        Ok(Self {\n            skill_registry,\n            temp_workspace,\n            env: HashMap::new(),\n            default_timeout: Duration::from_secs(60),\n            searcher: None,\n            packer: None,\n        })\n    }\n    \n    /// Run all tests for a skill\n    pub fn run_skill_tests(\u0026self, skill_id: \u0026str) -\u003e Result\u003cSkillTestReport, TestError\u003e {\n        let skill = self.skill_registry.get(\u0026SkillId(skill_id.to_string()))?;\n        let test_dir = skill.path.join(\"tests\");\n        \n        if !test_dir.exists() {\n            return Ok(SkillTestReport {\n                skill_id: skill_id.to_string(),\n                tests_run: 0,\n                passed: 0,\n                failed: 0,\n                skipped: 0,\n                results: Vec::new(),\n                duration: Duration::ZERO,\n            });\n        }\n        \n        let mut report = SkillTestReport::new(skill_id);\n        let start = Instant::now();\n        \n        // Find all test files\n        for entry in std::fs::read_dir(\u0026test_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            \n            if path.extension().map(|e| e == \"yaml\" || e == \"yml\").unwrap_or(false) {\n                let result = self.run_test_file(\u0026path)?;\n                report.add_result(result);\n            }\n        }\n        \n        report.duration = start.elapsed();\n        Ok(report)\n    }\n    \n    /// Run a single test file\n    pub fn run_test_file(\u0026self, path: \u0026Path) -\u003e Result\u003cTestResult, TestError\u003e {\n        let content = std::fs::read_to_string(path)?;\n        let test: TestDefinition = serde_yaml::from_str(\u0026content)?;\n        \n        // Check skip conditions\n        if self.should_skip(\u0026test) {\n            return Ok(TestResult {\n                name: test.name,\n                status: TestStatus::Skipped,\n                duration: Duration::ZERO,\n                output: None,\n                failures: Vec::new(),\n            });\n        }\n        \n        // Check requirements\n        if let Some(missing) = self.check_requirements(\u0026test) {\n            return Ok(TestResult {\n                name: test.name,\n                status: TestStatus::Skipped,\n                duration: Duration::ZERO,\n                output: Some(format!(\"Missing requirement: {}\", missing)),\n                failures: Vec::new(),\n            });\n        }\n        \n        // Dispatch based on test type\n        match test.test_type.as_deref() {\n            Some(\"retrieval\") =\u003e self.run_retrieval_test(\u0026test),\n            Some(\"packing\") =\u003e self.run_packing_test(\u0026test),\n            _ =\u003e self.run_standard_test(\u0026test),\n        }\n    }\n    \n    /// Run a standard test\n    fn run_standard_test(\u0026self, test: \u0026TestDefinition) -\u003e Result\u003cTestResult, TestError\u003e {\n        let start = Instant::now();\n        let mut context = TestContext::new(\u0026self.temp_workspace, \u0026self.env);\n        let mut failures = Vec::new();\n        \n        // Run setup\n        if let Some(setup) = \u0026test.setup {\n            for step in setup {\n                if let Err(e) = self.execute_step(step, \u0026mut context) {\n                    return Ok(TestResult {\n                        name: test.name.clone(),\n                        status: TestStatus::Failed,\n                        duration: start.elapsed(),\n                        output: Some(format!(\"Setup failed: {}\", e)),\n                        failures: vec![format!(\"Setup: {}\", e)],\n                    });\n                }\n            }\n        }\n        \n        // Run test steps\n        for step in \u0026test.steps {\n            match self.execute_step(step, \u0026mut context) {\n                Ok(()) =\u003e {}\n                Err(e) =\u003e {\n                    failures.push(e.to_string());\n                }\n            }\n        }\n        \n        // Run cleanup (always, even if test failed)\n        if let Some(cleanup) = \u0026test.cleanup {\n            for step in cleanup {\n                let _ = self.execute_step(step, \u0026mut context);\n            }\n        }\n        \n        let status = if failures.is_empty() {\n            TestStatus::Passed\n        } else {\n            TestStatus::Failed\n        };\n        \n        Ok(TestResult {\n            name: test.name.clone(),\n            status,\n            duration: start.elapsed(),\n            output: context.last_output.clone(),\n            failures,\n        })\n    }\n    \n    /// Execute a single test step\n    fn execute_step(\u0026self, step: \u0026TestStep, context: \u0026mut TestContext) -\u003e Result\u003c(), TestError\u003e {\n        match step {\n            TestStep::LoadSkill { level, budget, .. } =\u003e {\n                let skill = self.skill_registry.get(\u0026SkillId(context.skill_id.clone()))?;\n                context.loaded_skill = Some(skill);\n                context.skill_loaded = true;\n                Ok(())\n            }\n            \n            TestStep::Run { cmd, cwd, env, timeout, .. } =\u003e {\n                let working_dir = cwd.as_ref()\n                    .map(PathBuf::from)\n                    .unwrap_or_else(|| context.workspace.clone());\n                \n                let timeout = timeout.unwrap_or(Duration::from_secs(30));\n                \n                let mut command = std::process::Command::new(\"sh\");\n                command.arg(\"-c\").arg(cmd);\n                command.current_dir(\u0026working_dir);\n                \n                // Set environment\n                for (k, v) in \u0026context.env {\n                    command.env(k, v);\n                }\n                if let Some(env) = env {\n                    for (k, v) in env {\n                        command.env(k, v);\n                    }\n                }\n                \n                let output = command.output()?;\n                \n                context.last_exit_code = Some(output.status.code().unwrap_or(-1));\n                context.last_stdout = Some(String::from_utf8_lossy(\u0026output.stdout).to_string());\n                context.last_stderr = Some(String::from_utf8_lossy(\u0026output.stderr).to_string());\n                context.last_output = context.last_stdout.clone();\n                \n                Ok(())\n            }\n            \n            TestStep::Assert(assertions) =\u003e {\n                self.check_assertions(assertions, context)\n            }\n            \n            TestStep::WriteFile { path, content } =\u003e {\n                let path = self.expand_path(path, context);\n                if let Some(parent) = path.parent() {\n                    std::fs::create_dir_all(parent)?;\n                }\n                std::fs::write(\u0026path, content)?;\n                Ok(())\n            }\n            \n            TestStep::Mkdir { path, parents } =\u003e {\n                let path = self.expand_path(path, context);\n                if *parents {\n                    std::fs::create_dir_all(\u0026path)?;\n                } else {\n                    std::fs::create_dir(\u0026path)?;\n                }\n                Ok(())\n            }\n            \n            TestStep::Remove { path, recursive } =\u003e {\n                let path = self.expand_path(path, context);\n                if path.is_dir() \u0026\u0026 *recursive {\n                    std::fs::remove_dir_all(\u0026path)?;\n                } else if path.is_dir() {\n                    std::fs::remove_dir(\u0026path)?;\n                } else {\n                    std::fs::remove_file(\u0026path)?;\n                }\n                Ok(())\n            }\n            \n            TestStep::Copy { from, to } =\u003e {\n                let from_path = self.expand_path(from, context);\n                let to_path = self.expand_path(to, context);\n                std::fs::copy(\u0026from_path, \u0026to_path)?;\n                Ok(())\n            }\n            \n            TestStep::Sleep { duration } =\u003e {\n                std::thread::sleep(*duration);\n                Ok(())\n            }\n            \n            TestStep::Set { name, value } =\u003e {\n                context.variables.insert(name.clone(), value.clone());\n                Ok(())\n            }\n            \n            TestStep::If { condition, then_steps, else_steps } =\u003e {\n                if self.evaluate_condition(condition, context) {\n                    for step in then_steps {\n                        self.execute_step(step, context)?;\n                    }\n                } else if let Some(else_steps) = else_steps {\n                    for step in else_steps {\n                        self.execute_step(step, context)?;\n                    }\n                }\n                Ok(())\n            }\n        }\n    }\n    \n    /// Check assertions\n    fn check_assertions(\u0026self, assertions: \u0026Assertions, context: \u0026TestContext) -\u003e Result\u003c(), TestError\u003e {\n        if let Some(expected_code) = assertions.exit_code {\n            if let Some(actual_code) = context.last_exit_code {\n                if actual_code != expected_code {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"Exit code: expected {}, got {}\",\n                        expected_code, actual_code\n                    )));\n                }\n            }\n        }\n        \n        if let Some(pattern) = \u0026assertions.stdout_contains {\n            if let Some(stdout) = \u0026context.last_stdout {\n                if !stdout.contains(pattern) {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"stdout does not contain '{}'\",\n                        pattern\n                    )));\n                }\n            }\n        }\n        \n        if let Some(pattern) = \u0026assertions.stdout_not_contains {\n            if let Some(stdout) = \u0026context.last_stdout {\n                if stdout.contains(pattern) {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"stdout contains '{}' but should not\",\n                        pattern\n                    )));\n                }\n            }\n        }\n        \n        if assertions.stderr_empty == Some(true) {\n            if let Some(stderr) = \u0026context.last_stderr {\n                if !stderr.trim().is_empty() {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"stderr not empty: {}\",\n                        stderr\n                    )));\n                }\n            }\n        }\n        \n        if let Some(path) = \u0026assertions.file_exists {\n            let path = self.expand_path(path, context);\n            if !path.exists() {\n                return Err(TestError::AssertionFailed(format!(\n                    \"File does not exist: {}\",\n                    path.display()\n                )));\n            }\n        }\n        \n        if assertions.skill_loaded == Some(true) \u0026\u0026 !context.skill_loaded {\n            return Err(TestError::AssertionFailed(\n                \"Skill not loaded\".to_string()\n            ));\n        }\n        \n        if let Some(sections) = \u0026assertions.sections_present {\n            if let Some(skill) = \u0026context.loaded_skill {\n                for section in sections {\n                    if !skill.sections.contains_key(section) {\n                        return Err(TestError::AssertionFailed(format!(\n                            \"Section '{}' not present in skill\",\n                            section\n                        )));\n                    }\n                }\n            }\n        }\n        \n        if let Some(max_tokens) = assertions.tokens_used_lt {\n            if let Some(tokens) = context.tokens_used {\n                if tokens \u003e= max_tokens {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"Tokens used ({}) \u003e= limit ({})\",\n                        tokens, max_tokens\n                    )));\n                }\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n/// Test execution context\npub struct TestContext {\n    pub workspace: PathBuf,\n    pub skill_id: String,\n    pub env: HashMap\u003cString, String\u003e,\n    pub variables: HashMap\u003cString, String\u003e,\n    pub loaded_skill: Option\u003cSkill\u003e,\n    pub skill_loaded: bool,\n    pub tokens_used: Option\u003cusize\u003e,\n    pub last_exit_code: Option\u003ci32\u003e,\n    pub last_stdout: Option\u003cString\u003e,\n    pub last_stderr: Option\u003cString\u003e,\n    pub last_output: Option\u003cString\u003e,\n}\n\n/// Test result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TestResult {\n    pub name: String,\n    pub status: TestStatus,\n    pub duration: Duration,\n    pub output: Option\u003cString\u003e,\n    pub failures: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum TestStatus {\n    Passed,\n    Failed,\n    Skipped,\n    Timeout,\n}\n\n/// Aggregate report for a skill's tests\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillTestReport {\n    pub skill_id: String,\n    pub tests_run: usize,\n    pub passed: usize,\n    pub failed: usize,\n    pub skipped: usize,\n    pub results: Vec\u003cTestResult\u003e,\n    pub duration: Duration,\n}\n\nimpl SkillTestReport {\n    pub fn new(skill_id: \u0026str) -\u003e Self {\n        Self {\n            skill_id: skill_id.to_string(),\n            tests_run: 0,\n            passed: 0,\n            failed: 0,\n            skipped: 0,\n            results: Vec::new(),\n            duration: Duration::ZERO,\n        }\n    }\n    \n    pub fn add_result(\u0026mut self, result: TestResult) {\n        self.tests_run += 1;\n        match result.status {\n            TestStatus::Passed =\u003e self.passed += 1,\n            TestStatus::Failed =\u003e self.failed += 1,\n            TestStatus::Skipped =\u003e self.skipped += 1,\n            TestStatus::Timeout =\u003e self.failed += 1,\n        }\n        self.results.push(result);\n    }\n    \n    pub fn success(\u0026self) -\u003e bool {\n        self.failed == 0\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms test \u003cskill\u003e`\n\n```\nRun tests for a skill\n\nUSAGE:\n    ms test \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --test \u003cNAME\u003e       Run specific test by name\n    --tags \u003cTAGS\u003e       Only run tests with these tags\n    --exclude-tags \u003cT\u003e  Skip tests with these tags\n    --timeout \u003cSECS\u003e    Override default timeout\n    -v, --verbose       Show detailed output\n    --fail-fast         Stop on first failure\n\nOUTPUT EXAMPLE:\n    Running tests for: rust-error-handling\n    \n    tests/basic_load.yaml\n      [PASS] Skill loads correctly (0.12s)\n      \n    tests/commands.yaml\n      [PASS] rustc available (0.08s)\n      [PASS] cargo build works (1.23s)\n      [FAIL] clippy check (0.45s)\n            Assertion failed: exit_code expected 0, got 1\n            \n    tests/retrieval.yaml\n      [PASS] Error handling query (0.34s)\n      [SKIP] Advanced query (missing: rust-nightly)\n\n    Results: 4 passed, 1 failed, 1 skipped (2.22s)\n```\n\n### `ms test --all`\n\n```\nRun tests for all skills\n\nUSAGE:\n    ms test --all [OPTIONS]\n\nOPTIONS:\n    --parallel \u003cN\u003e      Run tests in parallel [default: 4]\n    --tags \u003cTAGS\u003e       Only run tests with these tags\n    --type \u003cTYPE\u003e       Only run tests of type: standard, retrieval, packing\n    --fail-fast         Stop on first failure\n    --report \u003cFILE\u003e     Write report to file\n\nOUTPUT EXAMPLE:\n    Running tests for all skills...\n    \n    rust-error-handling        [4/5 passed]  FAIL\n    rust-async                 [3/3 passed]  PASS\n    python-testing             [6/6 passed]  PASS\n    go-concurrency             [2/2 passed]  PASS\n    typescript-types           [5/5 passed]  PASS\n    \n    Summary: 20/21 tests passed across 5 skills\n    Failed: rust-error-handling/tests/commands.yaml:clippy check\n```\n\n### `ms test --type retrieval`\n\n```\nRun retrieval tests\n\nUSAGE:\n    ms test --type retrieval [OPTIONS]\n\nOPTIONS:\n    --skill \u003cSKILL\u003e     Test specific skill\n    --query \u003cQUERY\u003e     Test with specific query\n    --show-results      Show actual search results\n\nOUTPUT EXAMPLE:\n    Running retrieval tests...\n    \n    rust-error-handling\n      Query: \"How do I handle errors in Rust?\"\n      Expected: rust-error-handling at rank \u003c= 2\n      Actual: rank 1, score 0.92\n      [PASS]\n      \n      Query: \"Result vs Option in Rust\"\n      Expected: rust-error-handling at rank \u003c= 3\n      Actual: rank 4, score 0.71\n      [FAIL] Expected rank \u003c= 3, got 4\n    \n    Results: 1 passed, 1 failed\n```\n\n### `ms test --ci --junit`\n\n```\nRun tests in CI mode with JUnit output\n\nUSAGE:\n    ms test --ci [OPTIONS]\n\nOPTIONS:\n    --junit \u003cFILE\u003e      Write JUnit XML report\n    --html \u003cFILE\u003e       Write HTML report\n    --coverage          Include coverage information\n    --strict            Fail on any warnings\n    --timeout \u003cSECS\u003e    CI timeout [default: 300]\n\nEXAMPLE:\n    ms test --all --ci --junit test-results.xml\n\n    # In CI pipeline:\n    - name: Run skill tests\n      run: ms test --all --ci --junit results.xml\n      \n    - name: Upload test results\n      uses: actions/upload-artifact@v3\n      with:\n        name: test-results\n        path: results.xml\n```\n\n---\n\n## JUnit XML Output\n\n```rust\nimpl SkillTestReport {\n    /// Generate JUnit XML format\n    pub fn to_junit_xml(\u0026self) -\u003e String {\n        let mut xml = String::from(r#\"\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\"#);\n        xml.push_str(\"\\n\u003ctestsuites\u003e\\n\");\n        \n        xml.push_str(\u0026format!(\n            r#\"  \u003ctestsuite name=\"{}\" tests=\"{}\" failures=\"{}\" skipped=\"{}\" time=\"{:.3}\"\u003e\"#,\n            self.skill_id,\n            self.tests_run,\n            self.failed,\n            self.skipped,\n            self.duration.as_secs_f64()\n        ));\n        xml.push('\\n');\n        \n        for result in \u0026self.results {\n            xml.push_str(\u0026format!(\n                r#\"    \u003ctestcase name=\"{}\" time=\"{:.3}\"\u003e\"#,\n                result.name,\n                result.duration.as_secs_f64()\n            ));\n            \n            match result.status {\n                TestStatus::Failed | TestStatus::Timeout =\u003e {\n                    xml.push_str(\"\\n      \u003cfailure message=\\\"Test failed\\\"\u003e\");\n                    for failure in \u0026result.failures {\n                        xml.push_str(\u0026format!(\"\\n        {}\", failure));\n                    }\n                    xml.push_str(\"\\n      \u003c/failure\u003e\\n    \");\n                }\n                TestStatus::Skipped =\u003e {\n                    xml.push_str(\"\\n      \u003cskipped/\u003e\\n    \");\n                }\n                TestStatus::Passed =\u003e {}\n            }\n            \n            xml.push_str(\"\u003c/testcase\u003e\\n\");\n        }\n        \n        xml.push_str(\"  \u003c/testsuite\u003e\\n\");\n        xml.push_str(\"\u003c/testsuites\u003e\\n\");\n        \n        xml\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum TestError {\n    #[error(\"Test file not found: {0}\")]\n    FileNotFound(PathBuf),\n    \n    #[error(\"Test parse error: {0}\")]\n    ParseError(#[from] serde_yaml::Error),\n    \n    #[error(\"Assertion failed: {0}\")]\n    AssertionFailed(String),\n    \n    #[error(\"Command failed: {0}\")]\n    CommandFailed(String),\n    \n    #[error(\"Timeout after {0:?}\")]\n    Timeout(Duration),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Skill not found: {0}\")]\n    SkillNotFound(String),\n    \n    #[error(\"Missing requirement: {0}\")]\n    MissingRequirement(String),\n}\n```\n\n---\n\n## Dependencies\n\n- **Testing Strategy** (meta_skill-9ok): Overall testing approach and patterns\n- `serde`, `serde_yaml`: Test file parsing\n- `tempfile`: Temporary workspaces\n- `chrono`: Duration handling\n- Command execution utilities","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T23:01:54.751632719-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:29:59.365537105-05:00","labels":["phase-6","skill-tests","testing","validation"],"dependencies":[{"issue_id":"meta_skill-x7k","depends_on_id":"meta_skill-9ok","type":"blocks","created_at":"2026-01-13T23:04:15.813119863-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-y73","title":"Phase 3: Disclosure \u0026 Suggestions","description":"# Epic: Phase 3 Disclosure \u0026 Suggestions\n\n## Goal\n\nDeliver progressive disclosure, token packing, context‑aware suggestions, and meta‑skill composition for efficient skill consumption.\n\n---\n\n## Scope\n\n- Disclosure levels + micro‑slicing\n- Constrained packer\n- Context‑aware suggestions + bandit\n- Cooldowns + fingerprints\n- Meta‑skills (composed bundles)\n- Conditional predicates + overlays\n\n---\n\n## Acceptance Criteria\n\n- `ms load` supports levels and token packing.\n- Suggestions are relevant and non‑spammy.\n- Meta‑skills load as a single unit.\n\n---\n\n## Child Beads\n\n- `meta_skill-sqh` Disclosure Levels\n- `meta_skill-0an` Micro‑Slicing Engine\n- `meta_skill-9ik` Token Packer\n- `meta_skill-o8o` Context‑Aware Suggestions\n- `meta_skill-q5x` Suggestion Bandit\n- `meta_skill-8df` Context Fingerprints \u0026 Cooldowns\n- `meta_skill-7ws` Meta‑Skills\n- `meta_skill-1jl` Conditional Predicates\n- `meta_skill-cn4` Block‑Level Overlays","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:53.633796121-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:13:45.757137804-05:00","dependencies":[{"issue_id":"meta_skill-y73","depends_on_id":"meta_skill-4ih","type":"blocks","created_at":"2026-01-13T22:21:01.852123545-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-yu1","title":"Phase 5: Bundles \u0026 Distribution","description":"# Phase 5: Bundles \u0026 Distribution\n\nSkill sharing, bundling, and multi-machine synchronization.\n\n## Core Components\n1. **Bundle format** - YAML manifest with skills + metadata\n2. **GitHub integration** - Publish/install from GitHub releases\n3. **Local modification safety** - Preserve customizations on update\n4. **Sync engine** - Three-way merge with conflict resolution\n5. **Multi-machine sync** - Machine identity, sync state tracking\n6. **One-URL sharing** - Share entire skill set via single URL\n7. **Backup system** - Automatic backups with retention\n\n## Key Design Decisions\n- Three-tier storage: upstream, local mods, merged\n- Conflict resolution is proposal-first\n- Patches stored, not full copies\n- SHA256 verification on install\n\n## Success Criteria\n- `ms bundle create my-skills` packages skills\n- `ms bundle publish --repo user/skills` pushes to GitHub\n- `ms bundle install user/skills` works\n- Local modifications survive updates\n- Conflict resolution surfaces differences clearly","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-13T22:20:55.238029107-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:55.238029107-05:00","dependencies":[{"issue_id":"meta_skill-yu1","depends_on_id":"meta_skill-4ki","type":"blocks","created_at":"2026-01-13T22:21:01.903929388-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-z2r","title":"CASS Mining: Performance Profiling Patterns","description":"Deep dive into CASS sessions about perf record, jemalloc allocation profiling, cargo bench profiling, RUSTFLAGS for frame pointers. Extract actionable skill patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:14.537502827-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:18:35.692964305-05:00","closed_at":"2026-01-13T18:18:35.692964305-05:00","close_reason":"Section 30 (Performance Profiling Patterns) added to plan. Covers: methodology (hot path analysis, inefficiency patterns), SIMD/vectorization, Criterion benchmarks, profiling builds, I/O optimization, caching, and parallelism patterns.","labels":["cass-mining"]}
{"id":"meta_skill-z3c","title":"Skill Pruning \u0026 Evolution","description":"# Skill Pruning \u0026 Evolution\n\n## Section Reference\nSection 7.5 - Skill Pruning \u0026 Evolution\n\n## Overview\n\nAs the skill registry grows, ms must keep skills lean and current without destructive deletions. Pruning is **proposal-first**: identify candidates, suggest merges or deprecations, and require explicit confirmation before applying changes.\n\n## Why Pruning Matters\n\nWithout active pruning:\n- Registry becomes cluttered with stale/unused skills\n- Duplicate skills confuse users and agents\n- Quality degrades as outdated skills persist\n- Search results become noisy\n\nWith proposal-first pruning:\n- Users maintain control over deletions\n- Valuable skills aren't accidentally removed\n- Evolution happens through merge/deprecate, not delete\n- Full audit trail of changes\n\n## Pruning Signals\n\n### Low Usage\n```rust\nstruct UsageSignal {\n    skill_id: String,\n    uses_last_30_days: u32,\n    threshold: u32,  // e.g., \u003c5 uses\n}\n```\n\n### Low Quality Score\n```rust\nstruct QualitySignal {\n    skill_id: String,\n    quality_score: f32,\n    threshold: f32,  // e.g., \u003c0.3\n}\n```\n\n### High Similarity\n```rust\nstruct SimilaritySignal {\n    skill_a: String,\n    skill_b: String,\n    similarity: f32,\n    threshold: f32,  // e.g., \u003e= 0.8\n}\n```\n\n### Toolchain Mismatch\n```rust\nstruct ToolchainSignal {\n    skill_id: String,\n    expected_tools: Vec\u003cString\u003e,\n    missing_tools: Vec\u003cString\u003e,\n}\n```\n\n## Pruning Actions (Non-Destructive)\n\n### Propose Merge\nCombine two similar skills into one:\n```rust\nstruct MergeProposal {\n    source_skills: Vec\u003cString\u003e,\n    target_name: String,\n    auto_draft: SkillSpec,\n    rationale: String,\n}\n```\n\n### Propose Deprecate\nMark as deprecated with replacement alias:\n```rust\nstruct DeprecateProposal {\n    skill_id: String,\n    replacement_id: Option\u003cString\u003e,\n    rationale: String,\n}\n```\n\n### Propose Split\nBreak overly broad skill into focused children:\n```rust\nstruct SplitProposal {\n    source_skill: String,\n    children: Vec\u003cSkillSpec\u003e,\n    rationale: String,\n}\n```\n\n## CLI Interface\n\n```bash\n# Analyze registry for pruning candidates\nms prune --analyze\n\n# Show detailed proposals\nms prune --proposals\n\n# Interactive review of proposals\nms prune --review\n\n# Apply specific proposal\nms prune --apply merge:rust-errors-v1,rust-errors-v2\n\n# Dry-run mode\nms prune --apply merge:a,b --dry-run\n\n# Generate beads for review\nms prune --emit-beads\n```\n\n## Robot Mode Output\n\n```json\n{\n  \"status\": \"proposals_ready\",\n  \"proposals\": [\n    {\n      \"type\": \"merge\",\n      \"sources\": [\"rust-errors-v1\", \"rust-errors-v2\"],\n      \"target\": \"rust-error-handling\",\n      \"rationale\": \"High similarity (0.92), low individual usage\",\n      \"draft_path\": \".ms/proposals/merge-001.yaml\"\n    },\n    {\n      \"type\": \"deprecate\",\n      \"skill\": \"old-testing-guide\",\n      \"replacement\": \"modern-testing\",\n      \"rationale\": \"No usage in 60 days, superseded\"\n    }\n  ],\n  \"stats\": {\n    \"total_skills\": 150,\n    \"candidates\": 12,\n    \"merge_proposals\": 3,\n    \"deprecate_proposals\": 7,\n    \"split_proposals\": 2\n  }\n}\n```\n\n## Beads Integration\n\nWhen --emit-beads is used, pruning creates beads for tracking:\n```\nms-prune-001: Merge rust-errors-v1 + v2 [P2]\nms-prune-002: Deprecate old-testing-guide [P3]\n...\n```\n\n## Acceptance Criteria\n\n1. [ ] Usage tracking for pruning signals\n2. [ ] Quality score integration\n3. [ ] Similarity detection (via embeddings)\n4. [ ] Toolchain mismatch detection\n5. [ ] Merge proposal generation with auto-draft\n6. [ ] Deprecate proposal with alias\n7. [ ] Split proposal with child drafts\n8. [ ] CLI: ms prune --analyze\n9. [ ] Interactive review mode\n10. [ ] Dry-run support\n11. [ ] Beads emission for tracking\n12. [ ] Robot mode JSON output\n\n## Dependencies\n\n- Depends on: meta_skill-e5e (Skill Quality Scoring)\n- Depends on: meta_skill-r6k (Skill Alias System)\n- Depends on: meta_skill-ch6 (Hash Embeddings for similarity)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:33:24.691038351-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:33:24.691038351-05:00","labels":["evolution","maintenance","phase-3"],"dependencies":[{"issue_id":"meta_skill-z3c","depends_on_id":"meta_skill-e5e","type":"blocks","created_at":"2026-01-13T23:33:31.76006781-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-z3c","depends_on_id":"meta_skill-r6k","type":"blocks","created_at":"2026-01-13T23:33:31.792780008-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-z3c","depends_on_id":"meta_skill-ch6","type":"blocks","created_at":"2026-01-13T23:33:31.822151681-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-z49","title":"[P4] Session Marking System","description":"# Session Marking System\n\n## Overview\n\nAllow users/agents to mark CASS sessions as exemplary, anti‑pattern, or low‑quality. Markings influence mining weights, exclusion, and evaluation.\n\n---\n\n## Tasks\n\n1. Define marking schema (tags + rationale).\n2. Store markings in SQLite.\n3. Surface marks in mining filters (`--marked`, `--exclude-marked`).\n4. Provide CLI commands: `ms session mark`, `ms session list`.\n\n---\n\n## Testing Requirements\n\n- Unit tests for mark persistence.\n- Integration tests: marks influence extraction.\n\n---\n\n## Acceptance Criteria\n\n- Marked sessions are weighted correctly.\n- CLI can add/remove/list marks.\n\n---\n\n## Dependencies\n\n- `meta_skill-hhu` CASS Client Integration\n- `meta_skill-qs1` SQLite Database Layer","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:47.552709662-05:00","created_by":"ubuntu","updated_at":"2026-01-14T00:05:19.946470343-05:00","labels":["curation","marking","phase-4"],"dependencies":[{"issue_id":"meta_skill-z49","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T22:26:13.020300992-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-z49","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-14T00:05:51.619939392-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-zno","title":"[P5] Multi-Machine Sync","description":"# Multi-Machine Sync\n\nSynchronize skills across multiple machines.\n\n## Tasks\n1. Machine identity (unique ID per installation)\n2. Sync state tracking\n3. Conflict detection across machines\n4. Push/pull operations\n5. Optional Git-based sync backend\n\n## Machine Identity\n- Generate UUID on first run\n- Store in .ms/machine_id\n- Include in sync metadata\n\n## Sync State\n```sql\nCREATE TABLE sync_state (\n    skill_id TEXT PRIMARY KEY,\n    local_version TEXT,\n    remote_version TEXT,\n    last_synced TIMESTAMP,\n    machine_id TEXT\n);\n```\n\n## Sync Protocol\n1. `ms sync pull` - Fetch changes from remote\n2. `ms sync push` - Push local changes to remote\n3. `ms sync status` - Show pending changes\n4. Auto-sync on bundle operations\n\n## Git Backend (Optional)\n- Store skills in Git repo\n- Sync via git pull/push\n- Leverage Git merge for conflicts\n\n## Acceptance Criteria\n- Sync works across machines\n- Conflicts detected and resolved\n- Git backend optional but supported","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:27:05.947965319-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:14.612186991-05:00","closed_at":"2026-01-13T23:42:14.612186991-05:00","close_reason":"Duplicate of meta_skill-ujr (Multi-Machine Synchronization)","labels":["multi-machine","phase-5","sync"],"dependencies":[{"issue_id":"meta_skill-zno","depends_on_id":"meta_skill-swe","type":"blocks","created_at":"2026-01-13T22:27:15.45696713-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ztm","title":"[P4] ms build Command","description":"# ms build Command\n\n## Overview\n\nGenerate a new skill from CASS sessions. This command orchestrates the entire mining pipeline: fetch sessions → redact/injection filter → extract patterns → generalize → synthesize SkillSpec → compile SKILL.md.\n\n---\n\n## Core Flags\n\n- `--topic` / `--from-cass` for session targeting.\n- `--with-cm` for CM seeding.\n- `--guided` for Brenner Method / checkpoint flow.\n- `--no-injection-filter` (explicit override).\n\n---\n\n## Tasks\n\n1. Orchestrate pipeline stages with checkpoints.\n2. Persist intermediate artifacts for recovery.\n3. Emit robot JSON output for automation.\n4. Integrate ACIP + redaction filters by default.\n\n---\n\n## Testing Requirements\n\n- Integration test: full build on fixture sessions.\n- Recovery test: resume from checkpoint.\n- E2E: CLI build → output skill passes `ms lint` / `ms test`.\n\n---\n\n## Acceptance Criteria\n\n- Build produces valid SkillSpec + compiled SKILL.md.\n- Failure states are resumable.\n- Audit logs include evidence and safety filters.\n\n---\n\n## Dependencies\n\n- `meta_skill-237` Pattern Extraction\n- `meta_skill-9r9` Specific‑to‑General\n- `meta_skill-ans` Redaction Pipeline\n- `meta_skill-fma` Prompt Injection Defense\n- `meta_skill-obj` Brenner Method (guided mode)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:53.762279786-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:53:37.246635323-05:00","labels":["build","cli","phase-4"],"dependencies":[{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-ans","type":"blocks","created_at":"2026-01-13T22:26:13.261778288-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-1p7","type":"blocks","created_at":"2026-01-13T22:26:13.288116355-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T23:54:03.158731324-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-9r9","type":"blocks","created_at":"2026-01-13T23:54:10.950907731-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-obj","type":"blocks","created_at":"2026-01-13T23:54:20.291930102-05:00","created_by":"ubuntu"}]}
